{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../corpus/hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy(convs):\n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in convs:\n",
    "        for i, ut in enumerate( conv ):\n",
    "            if ut.is_utt_level_error():\n",
    "                continue\n",
    "            X.append(ut.utt)\n",
    "            if ut.is_exist_type():\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_Xy(convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_func_ = lambda L: [\"FOS\",  *L, \"EOS\"]\n",
    "def fill_SYMBOL_(L):\n",
    "    return list(map(filler_func_, L))\n",
    "\n",
    "def sentence2formated(sen):\n",
    "    return sum( fill_SYMBOL_( sentence2normalize_noun(sen) ), [] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_vocab_dict(text):\n",
    "    vocab_dict = dict()\n",
    "    # doc = nlp(text)\n",
    "    print(\"analyzed vocab text\")\n",
    "    vocab_dict[\"[PAD]\"] = 0\n",
    "    for key in tqdm(sentence2formated(text)):\n",
    "        # key = token.orth_\n",
    "        if key not in vocab_dict:\n",
    "            vocab_dict[key] = len(vocab_dict)\n",
    "\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzed vocab text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54396/54396 [00:00<00:00, 4372848.31it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = make_vocab_dict(X)\n",
    "vocab_size = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/response2/vocab_dict_noun.pickle\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"../X_y_data/response2/\"\n",
    "vocab_name = \"vocab_dict_noun.pickle\"\n",
    "vocabM = DataManager(vocab_path)\n",
    "vocabM.save_data(vocab_name, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/response2/vocab_dict_noun.pickle\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"../X_y_data/response2/\"\n",
    "vocab_name = \"vocab_dict_noun.pickle\"\n",
    "vocabM = DataManager(vocab_path)\n",
    "vocab_dict = vocabM.load_data(vocab_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2ids(sentence:str, vocab_dict:dict):\n",
    "    doc = sentence2formated(sentence)\n",
    "    ids = np.zeros(len(doc))\n",
    "    for i, key in enumerate(doc):\n",
    "        # key = token.orth_\n",
    "        if key in vocab_dict:\n",
    "            ids[i] = vocab_dict[key]\n",
    "        else:\n",
    "            ids[i] = vocab_dict[\"[UNK]\"]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "def padding_vector(Xseq):\n",
    "    Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "    Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "    Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "    return Xseq\n",
    "\n",
    "\n",
    "def make_X(utt_list:list, vocab_dict:dict):\n",
    "    utt_id_list = []\n",
    "    for utt in tqdm( utt_list) :\n",
    "        utt_id = sentence2ids(utt, vocab_dict)\n",
    "        utt_id_list.append(utt_id)\n",
    "\n",
    "    utt_id_pad = padding_vector(utt_id_list)\n",
    "    upl = len(utt_id_pad[0])\n",
    "    # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "    # print(usr_pad_len, sys_pad_len)\n",
    "    X = torch.zeros( (len(utt_list), upl) )\n",
    "    for i, u in enumerate(utt_id_pad):\n",
    "        X[i, :upl] = u\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3814/3814 [01:18<00:00, 48.47it/s]\n"
     ]
    }
   ],
   "source": [
    "X_= make_X(X,  vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, vocab_dict):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim , tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.sizea() = (batch_size × len(sentence) × embedding_dim)\n",
    "\n",
    "        # x : [seq]\n",
    "        # usr_ = x[:, :upl]\n",
    "        # sys_ = x[:, upl:upl+spl]\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # emb2 = self.word_embeddings(sys_)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        # _, lstm2_out = self.lstm1(emb2)\n",
    "        # print(hidden_layer)\n",
    "        # bilstm_out = torch.cat([lstm_out[0][0], lstm_out[0][1]], dim=1)\n",
    "        \n",
    "        # usr_vec = ( lstm1_out[0][0] + lstm1_out[0][1] )\n",
    "        # sys_vec = ( lstm2_out[0][0] + lstm2_out[0][1] )/2\n",
    "\n",
    "        # print(usr_vec.shape, sys_vec.shape)\n",
    "        # print(torch.cat([ usr_vec, sys_vec], dim=1).shape)\n",
    "        tag_space = self.hidden2tag(torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 ))\n",
    "        \n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        # y = self.hidden2tag(bilstm_out)\n",
    "        y =self.softmax(tag_space)\n",
    "        return y\n",
    "    \n",
    "    def last_context(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # print(emb1.shape)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        context = torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 )\n",
    "        return context\n",
    "    \n",
    "    def text2context(self, text):\n",
    "        if isinstance(text, str):\n",
    "            utt_id = self._sentence2ids(text, self.vocab_dict)\n",
    "            utt_id_tensor = torch.tensor( [utt_id] , device='cuda:0', dtype=torch.int)\n",
    "            # utt_id_tensor = torch.tensor( [utt_id] , device='cpu', dtype=torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        if isinstance(text, list):\n",
    "            X = self._make_X(text, self.vocab_dict)\n",
    "            utt_id_tensor = X.to(torch.int).cuda()\n",
    "            # utt_id_tensor = X.to(torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def _sentence2ids(self, sentence:str, vocab_dict:dict):\n",
    "        doc = self._sentence2formated(sentence)\n",
    "        ids = np.zeros(len(doc))\n",
    "        for i, key in enumerate(doc):\n",
    "            # key = token.orth_\n",
    "            if key in vocab_dict:\n",
    "                ids[i] = vocab_dict[key]\n",
    "            else:\n",
    "                ids[i] = vocab_dict[\"[UNK]\"]\n",
    "        return ids\n",
    "    \n",
    "    def _sentence2formated(self, sen):\n",
    "        return sum( fill_SYMBOL_ONE( sentence2normalize_noun(sen) ), [] )\n",
    "    \n",
    "    def _padding_vector(self, Xseq):\n",
    "        Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "        Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "        Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "        return Xseq\n",
    "\n",
    "\n",
    "    def _make_X(self, utt_list:list, vocab_dict:dict):\n",
    "        utt_id_list = []\n",
    "        for utt in tqdm( utt_list) :\n",
    "            utt_id = self._sentence2ids(utt, vocab_dict)\n",
    "            utt_id_list.append(utt_id)\n",
    "\n",
    "        utt_id_pad = self._padding_vector(utt_id_list)\n",
    "        upl = len(utt_id_pad[0])\n",
    "        # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "        # print(usr_pad_len, sys_pad_len)\n",
    "        X = torch.zeros( (len(utt_list), upl) )\n",
    "        for i, u in enumerate(utt_id_pad):\n",
    "            X[i, :upl] = u\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 300\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, vocab_dict)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 51.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"あああ\", \"いいい\"]\n",
    "model.text2context(text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 1.329736495101315\n",
      "epoch 100 \t loss 1.1677136448615784\n",
      "epoch 150 \t loss 1.174605282547418\n",
      "epoch 200 \t loss 1.11150508863102\n",
      "epoch 250 \t loss 1.1212896907391041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-dbbc99e0c7c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mall_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        X_t_tensor = data[0].to(torch.int).cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0', dtype=torch.int)\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[828  27]\n",
      " [ 25 265]]\n",
      "accuracy =  0.9545851528384279\n",
      "precision =  0.9075342465753424\n",
      "recall =  0.9137931034482759\n",
      "f1 score =  0.9106529209621994\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/response2/forward_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response2/\"\n",
    "model_name = \"forward_v2.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
