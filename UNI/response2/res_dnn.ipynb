{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy(convs):\n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in convs:\n",
    "        for i, ut in enumerate( conv ):\n",
    "            # if ut.is_utt_level_error():\n",
    "            #     continue\n",
    "            X.append(ut.utt)\n",
    "            if ut.is_exist_type():\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_Xy(convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_func_ = lambda L: [\"FOS\",  *L, \"EOS\"]\n",
    "def fill_SYMBOL_(L):\n",
    "    return list(map(filler_func_, L))\n",
    "\n",
    "def sentence2formated(sen):\n",
    "    return sum( fill_SYMBOL_( sentence2normalize_noun(sen) ), [] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_vocab_dict(text):\n",
    "    vocab_dict = dict()\n",
    "    # doc = nlp(text)\n",
    "    print(\"analyzed vocab text\")\n",
    "    vocab_dict[\"[PAD]\"] = 0\n",
    "    for key in tqdm(sentence2formated(text)):\n",
    "        # key = token.orth_\n",
    "        if key not in vocab_dict:\n",
    "            vocab_dict[key] = len(vocab_dict)\n",
    "\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzed vocab text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68953/68953 [00:00<00:00, 4003126.04it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = make_vocab_dict(X)\n",
    "vocab_size = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/response2/vocab_dict_noun2.pickle\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"../X_y_data/response2/\"\n",
    "vocab_name = \"vocab_dict_noun2.pickle\"\n",
    "vocabM = DataManager(vocab_path)\n",
    "vocabM.save_data(vocab_name, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/response2/vocab_dict_noun2.pickle\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"../X_y_data/response2/\"\n",
    "vocab_name = \"vocab_dict_noun2.pickle\"\n",
    "vocabM = DataManager(vocab_path)\n",
    "vocab_dict = vocabM.load_data(vocab_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2ids(sentence:str, vocab_dict:dict):\n",
    "    doc = sentence2formated(sentence)\n",
    "    ids = np.zeros(len(doc))\n",
    "    for i, key in enumerate(doc):\n",
    "        # key = token.orth_\n",
    "        if key in vocab_dict:\n",
    "            ids[i] = vocab_dict[key]\n",
    "        else:\n",
    "            ids[i] = vocab_dict[\"[UNK]\"]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "def padding_vector(Xseq):\n",
    "    Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "    Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "    Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "    return Xseq\n",
    "\n",
    "\n",
    "def make_X(utt_list:list, vocab_dict:dict):\n",
    "    utt_id_list = []\n",
    "    for utt in tqdm( utt_list) :\n",
    "        utt_id = sentence2ids(utt, vocab_dict)\n",
    "        utt_id_list.append(utt_id)\n",
    "\n",
    "    utt_id_pad = padding_vector(utt_id_list)\n",
    "    upl = len(utt_id_pad[0])\n",
    "    # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "    # print(usr_pad_len, sys_pad_len)\n",
    "    X = torch.zeros( (len(utt_list), upl) )\n",
    "    for i, u in enumerate(utt_id_pad):\n",
    "        X[i, :upl] = u\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4200/4200 [01:45<00:00, 39.98it/s]\n"
     ]
    }
   ],
   "source": [
    "X_= make_X(X,  vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, vocab_dict):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim , tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.sizea() = (batch_size × len(sentence) × embedding_dim)\n",
    "\n",
    "        # x : [seq]\n",
    "        # usr_ = x[:, :upl]\n",
    "        # sys_ = x[:, upl:upl+spl]\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # emb2 = self.word_embeddings(sys_)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        # _, lstm2_out = self.lstm1(emb2)\n",
    "        # print(hidden_layer)\n",
    "        # bilstm_out = torch.cat([lstm_out[0][0], lstm_out[0][1]], dim=1)\n",
    "        \n",
    "        # usr_vec = ( lstm1_out[0][0] + lstm1_out[0][1] )\n",
    "        # sys_vec = ( lstm2_out[0][0] + lstm2_out[0][1] )/2\n",
    "\n",
    "        # print(usr_vec.shape, sys_vec.shape)\n",
    "        # print(torch.cat([ usr_vec, sys_vec], dim=1).shape)\n",
    "        tag_space = self.hidden2tag(torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 ))\n",
    "        \n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        # y = self.hidden2tag(bilstm_out)\n",
    "        y =self.softmax(tag_space)\n",
    "        return y\n",
    "    \n",
    "    def last_context(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # print(emb1.shape)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        context = torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 )\n",
    "        return context\n",
    "    \n",
    "    def text2context(self, text):\n",
    "        if isinstance(text, str):\n",
    "            utt_id = self._sentence2ids(text, self.vocab_dict)\n",
    "            utt_id_tensor = torch.tensor( [utt_id] , device='cuda:0', dtype=torch.int)\n",
    "            # utt_id_tensor = torch.tensor( [utt_id] , device='cpu', dtype=torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        if isinstance(text, list):\n",
    "            X = self._make_X(text, self.vocab_dict)\n",
    "            utt_id_tensor = X.to(torch.int).cuda()\n",
    "            # utt_id_tensor = X.to(torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def _sentence2ids(self, sentence:str, vocab_dict:dict):\n",
    "        doc = self._sentence2formated(sentence)\n",
    "        ids = np.zeros(len(doc))\n",
    "        for i, key in enumerate(doc):\n",
    "            # key = token.orth_\n",
    "            if key in vocab_dict:\n",
    "                ids[i] = vocab_dict[key]\n",
    "            else:\n",
    "                ids[i] = vocab_dict[\"[UNK]\"]\n",
    "        return ids\n",
    "    \n",
    "    def _sentence2formated(self, sen):\n",
    "        return sum( fill_SYMBOL_ONE( sentence2normalize_noun(sen) ), [] )\n",
    "    \n",
    "    def _padding_vector(self, Xseq):\n",
    "        Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "        Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "        Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "        return Xseq\n",
    "\n",
    "\n",
    "    def _make_X(self, utt_list:list, vocab_dict:dict):\n",
    "        utt_id_list = []\n",
    "        for utt in tqdm( utt_list) :\n",
    "            utt_id = self._sentence2ids(utt, vocab_dict)\n",
    "            utt_id_list.append(utt_id)\n",
    "\n",
    "        utt_id_pad = self._padding_vector(utt_id_list)\n",
    "        upl = len(utt_id_pad[0])\n",
    "        # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "        # print(usr_pad_len, sys_pad_len)\n",
    "        X = torch.zeros( (len(utt_list), upl) )\n",
    "        for i, u in enumerate(utt_id_pad):\n",
    "            X[i, :upl] = u\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 300\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, vocab_dict)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 34.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"あああ\", \"いいい\"]\n",
    "model.text2context(text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 1.189253134532919\n",
      "epoch 100 \t loss 1.0918073914926936\n",
      "epoch 150 \t loss 1.0422957030357338\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dbbc99e0c7c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mall_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        X_t_tensor = data[0].to(torch.int).cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0', dtype=torch.int)\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[932  35]\n",
      " [ 33 260]]\n",
      "accuracy =  0.946031746031746\n",
      "precision =  0.8813559322033898\n",
      "recall =  0.8873720136518771\n",
      "f1 score =  0.8843537414965986\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/response2/forward_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response2/\"\n",
    "model_name = \"forward_v3.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
