{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_plain(text):\n",
    "    text_ = neologdn.normalize(text)\n",
    "    text_ = re.sub(r'\\(.*\\)', \"\", text_)\n",
    "    text_ = re.sub(r'\\d+', \"0\", text_)\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, TOKENS=[\"[PAD]\", \"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\", \"[NONE]\"]):\n",
    "        self.index2item = []\n",
    "        self.item2index = {}\n",
    "\n",
    "        for sp_token in TOKENS:\n",
    "            self.add_item(sp_token)\n",
    "\n",
    "    # 単語数\n",
    "    def __len__(self):\n",
    "        return len(self.item2index)\n",
    "\n",
    "    # 単語が含まれているか\n",
    "    def __contains__(self, item):\n",
    "        return item in self.item2index.keys()\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return str(self.item2index)\n",
    "\n",
    "    # 単語の追加\n",
    "    def add_item(self, item):\n",
    "        # もう登録されてたら登録しないよ\n",
    "        if item in self.item2index:\n",
    "            return\n",
    "        index = len(self.item2index)\n",
    "        self.index2item.append(item)\n",
    "        self.item2index[item] = index\n",
    "    \n",
    "    def add_items(self, items:list):\n",
    "        for item in items:\n",
    "            self.add_item(item)\n",
    "\n",
    "    # 単語の取得\n",
    "    def get_item(self, index):\n",
    "        if len(self.index2item) <= index:\n",
    "            return \"[UNK]\"\n",
    "        return self.index2item[index]\n",
    "\n",
    "    # 単語をidへ\n",
    "    def get_index(self, item):\n",
    "        if item not in self.item2index:\n",
    "            return self.item2index[\"[UNK]\"]\n",
    "        return self.item2index[item]\n",
    "\n",
    "    # def save_vocab(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class LMN5(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LMN5, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        nn.init.normal_(self.embed.weight, std=0.01)\n",
    "        self.linear.weight = self.embed.weight\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb_out = self.embed(x)\n",
    "        # y =self.softmax(tag_space)\n",
    "        out, hc = self.lstm1(emb_out)\n",
    "        y = self.linear(out)\n",
    "        # print(y.shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/CF/ffn_CF1.pickle\n",
      "success load : ../models/CF/vocab_CF1.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/CF/\"\n",
    "model_name = \"ffn_CF1.pickle\"\n",
    "vocab_name = \"vocab_CF1.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "model = modelM.load_data(model_name)\n",
    "vocab = modelM.load_data(vocab_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_analyzer = Analyzer(Tokenizer())\n",
    "\n",
    "\n",
    "\n",
    "def sentence2normalize_noun_mecab(sentences):\n",
    "    normalize_sen = []\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]\n",
    "    for sen in sentences:\n",
    "        df = mecab_analyzer.analyze_with_dataframe(sen)\n",
    "        words = []\n",
    "        if df is None:\n",
    "            continue\n",
    "        for txt, pos in zip(df.surface, df.part_of_speech):\n",
    "            pos_split = pos.split(\"-\")\n",
    "            # print(pos_split)\n",
    "            if pos_split[0]==\"名詞\" :\n",
    "                if pos_split[1] == \"固有名詞\" :\n",
    "                    words.append(pos)\n",
    "                elif pos_split[1] == \"数詞\":\n",
    "                    words.append(\"0\")\n",
    "                else:\n",
    "                    words.append(txt)\n",
    "            else:\n",
    "                words.append(txt)\n",
    "        normalize_sen.append(words)\n",
    "    return normalize_sen\n",
    "\n",
    "def sentence2gram(sentence, vocab, N=5, is_id=True):\n",
    "    normalized = sentence2normalize_noun_mecab(sentence)\n",
    "    if len(normalized)==0:\n",
    "        print(\"sentence was empty\")\n",
    "        return []\n",
    "    normalized = [\"FOS\"] + normalized[0] + [\"EOS\"]\n",
    "    # padding\n",
    "    if len(normalized) < N:\n",
    "        normalized += [\"[PAD]\"]*(N-len(normalized)) \n",
    "    # id化を同時に行う場合\n",
    "    if is_id:\n",
    "        normalized = [ vocab.get_index(c) for c in normalized]\n",
    "    ngram_text = []\n",
    "    for i in range(len(normalized)-N+1):\n",
    "            # print(L[i:i+N])\n",
    "        ngram_text.append(normalized[i:i+N])\n",
    "    return ngram_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y_seq(A, vocab, N=5, is_id=True):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for utt in tqdm(A):\n",
    "        # ngram にした結果がかえってくる\n",
    "        ngram_text = sentence2gram(utt, vocab, N, is_id)\n",
    "        X.extend(ngram_text[:-1])\n",
    "        Y.extend(ngram_text[1:])\n",
    "    return X, Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)\n",
    "\n",
    "error = \"Semantic error\"\n",
    "# errors = ['Grammatical error', \"Uninterpretable\"]\n",
    "sys_utt = []\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "            # if not ut.utt[-1] in [\"？\", \"！\", \"。\", \"!\"]:\n",
    "            #     sys_utt.append( clean_text( ut.utt+\"。\" ))\n",
    "            #     # sys_utt.append(ut.utt)\n",
    "            # else:   \n",
    "            sys_utt.append( clean_text(ut.utt))\n",
    "            if ut.is_error_included(error):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1386/1386 [00:03<00:00, 410.61it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Y = make_X_y_seq(sys_utt, vocab, N=N, is_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "def sentence2score(sentence, vocab, N):\n",
    "    ngram_text = sentence2gram(sentence, vocab, N=N, is_id=True)\n",
    "    X = torch.tensor(ngram_text)\n",
    "        # print(X.shape, utt)\n",
    "    y_pred = model(X.to(torch.int))\n",
    "    max_ppl = 0\n",
    "    with torch.no_grad():\n",
    "        for x, yp in zip(X, y_pred):\n",
    "            ppl = 0\n",
    "            # print(x)\n",
    "            for i, (x_, yp_) in enumerate( zip(x[:-1], softmax(yp)) ):\n",
    "                    # print(torch.sum(yp_))\n",
    "                ppl += np.log2(yp_[x[i+1]])\n",
    "            ppl = ppl/N\n",
    "            ppl = float(np.power(2, ppl))\n",
    "                # print(float(ppl))\n",
    "            if ppl > max_ppl:\n",
    "                max_ppl = ppl\n",
    "    return max_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1386/1386 [00:14<00:00, 97.76it/s] \n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "\n",
    "ppl_list = []\n",
    "N=5\n",
    "for utt in tqdm(sys_utt):\n",
    "    # ngram_text = sentence2gram(utt, vocab, N=3, is_id=True)\n",
    "        \n",
    "    ppl = sentence2score(utt, vocab, N=N)\n",
    "    ppl_list.append(ppl)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = list(map(float, ppl_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list_sort = sorted(ppl_list, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999960184033294"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl_list_sort[len(ppl_list_sort)//5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元気ですかは元気です\n",
      "好きだを見ますよねー\n",
      "病院は治療を受けましょう\n",
      "好きだは好きですか。お寿司は縁側が好きですね\n",
      "時期から資格を取りますねぇ\n",
      "手を貯金に出しますねぇ\n",
      "所で、テレビでテレビあるって言ってましたが、テレビは民主党支持が多いですね\n",
      "旬ですねぇ。自分もオリンピック書いたし。\n",
      "confusion matrix = \n",
      " [[686 692]\n",
      " [  7   1]]\n",
      "accuracy =  0.49567099567099565\n",
      "precision =  0.001443001443001443\n",
      "recall =  0.125\n",
      "f1 score =  0.0028530670470756064\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "border = 1.5912122790274516e-05\n",
    "border= ppl_list_sort[len(ppl_list_sort)//2]\n",
    "epoch = 1\n",
    "# y_pred = np.zeros(len(y)) + 1\n",
    "y_pred = np.zeros(len(y))\n",
    "max_precision = 0\n",
    "\n",
    "for e in range(epoch):\n",
    "    # y_pred = np.zeros(len(y))\n",
    "    y_pred = np.zeros(len(y))\n",
    "    for i, score_ in enumerate(ppl_list):\n",
    "        # border 未満をエラーでとする\n",
    "        # print(score_)\n",
    "        if score_ > border :\n",
    "            y_pred[i] = 1\n",
    "            # print(sys_utt[i])\n",
    "        # precision = metrics.precision_score(y, y_pred)\n",
    "        if y[i] == 1:\n",
    "            print(sys_utt[i])\n",
    "    # print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "    # print(border + 0.01*e)\n",
    "    # print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "    # print()\n",
    "\n",
    "\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "def sentence2score_list(sentence, vocab, N):\n",
    "    ngram_text = sentence2gram(sentence, vocab, N=N, is_id=True)\n",
    "    X = torch.tensor(ngram_text)\n",
    "        # print(X.shape, utt)\n",
    "    y_pred = model(X.to(torch.int))\n",
    "    ppl_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, yp in zip(X, y_pred):\n",
    "            ppl = 0\n",
    "            # print(x)\n",
    "            for i, (x_, yp_) in enumerate( zip(x[:-1], softmax(yp)) ):\n",
    "                    # print(torch.sum(yp_))\n",
    "                ppl += np.log2(yp_[x[i+1]])\n",
    "                # ppl += np.log2(yp_[x_])\n",
    "            ppl = ppl/N\n",
    "            ppl = float(np.power(2, ppl))\n",
    "                # print(float(ppl))\n",
    "            ppl_list.append(ppl)\n",
    "    return ppl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOS', '時期', 'から', '資格'] 0.9994229054231399\n",
      "['時期', 'から', '資格', 'を'] 0.9997054971739678\n",
      "['から', '資格', 'を', '取り'] 0.9998664142064736\n",
      "['資格', 'を', '取り', 'ます'] 0.9999749057296895\n",
      "['を', '取り', 'ます', 'ねぇ'] 0.9984511960257818\n",
      "['取り', 'ます', 'ねぇ', 'EOS'] 0.525261368575887\n"
     ]
    }
   ],
   "source": [
    "sentence = \"時期から資格を取りますねぇ\"\n",
    "# sentence = \"お金は大切です\"\n",
    "n = 4\n",
    "ngrams = sentence2gram(sentence, vocab, n, is_id=False)\n",
    "scores = sentence2score_list(sentence, vocab, n)\n",
    "\n",
    "for n, s in zip(ngrams, scores):\n",
    "    print(n, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FOS', '同じ', '趣味', 'の', '白土'] 0.02145287021279901\n",
      "['同じ', '趣味', 'の', '白土', '人'] 0.015255695587417241\n",
      "['趣味', 'の', '白土', '人', 'と'] 0.02595353271797157\n",
      "['の', '白土', '人', 'と', '出会える'] 0.04081718965134213\n",
      "['白土', '人', 'と', '出会える', 'の'] 0.9992572020508157\n",
      "['人', 'と', '出会える', 'の', 'も'] 0.9999455066192333\n",
      "['と', '出会える', 'の', 'も', '楽しみ'] 0.9999031523085676\n",
      "['出会える', 'の', 'も', '楽しみ', 'に'] 0.9999811408291085\n",
      "['の', 'も', '楽しみ', 'に', '0'] 0.9999762769695517\n",
      "['も', '楽しみ', 'に', '0', 'つ'] 0.9999684792676979\n",
      "['楽しみ', 'に', '0', 'つ', 'です'] 0.9999605166236292\n",
      "['に', '0', 'つ', 'です', 'EOS'] 0.4227626730114587\n"
     ]
    }
   ],
   "source": [
    "sentence = \"同じ趣味の白土人と出会えるのも楽しみに1つです\"\n",
    "# sentence = \"お金は大切です\"\n",
    "ngrams = sentence2gram(sentence, vocab, N, is_id=False)\n",
    "scores = sentence2score_list(sentence, vocab, N)\n",
    "for n, s in zip(ngrams, scores):\n",
    "    print(n, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 熱中症に気をつけか?\n",
    "    - 8.845499791513074e-06\n",
    "- お金は大きとか\n",
    "    - 7.319023919936928e-08\n",
    "- はんばんこ\n",
    "    - 7.965650687703318e-09\n",
    "- 4.300948350625364e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['集中', '熱', '演劇', '芝居', '疼く', 'エンターテインメント', '歌', 'わかっ', '大き', 'ビックリ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.item2index.keys())[1960:1970]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM ベースの手法\n",
    "    ボーダー設定なし\n",
    "    ```\n",
    "    confusion matrix = \n",
    "    [[1243  136]\n",
    "    [   5    2]]\n",
    "    accuracy =  0.8982683982683982\n",
    "    precision =  0.014492753623188406\n",
    "    recall =  0.2857142857142857\n",
    "    f1 score =  0.02758620689655172\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
