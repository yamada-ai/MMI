{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5016/5016 [00:00<00:00, 1260779.58it/s]\n",
      "100%|██████████| 19999/19999 [00:00<00:00, 1554750.25it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../corpus/NTT/persona.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    convs = json.load(f)\n",
    "all_utt = []\n",
    "for did in tqdm( convs[\"convs\"] ) :\n",
    "    dids = list( did.keys() )[0]\n",
    "    all_utt += did[dids]\n",
    "\n",
    "with open(\"../../corpus/NTT/empathetic.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    convs = json.load(f)\n",
    "for did in tqdm( convs[\"convs\"] ) :\n",
    "    dids = list( did.keys() )[0]\n",
    "    all_utt += did[dids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_set = set(\"は　が　を　に　で\")\n",
    "case_set = set(\"は　が　を　に\")\n",
    "# case_set = set(\"が　を　に\")\n",
    "# def set_pair()\n",
    "import copy\n",
    "suit_base =  {\n",
    "    \"が\": \"[NONE]\",\n",
    "    \"を\": \"[NONE]\",\n",
    "    \"に\": \"[NONE]\",\n",
    "    \"V\" : \"\"\n",
    "}\n",
    "\n",
    "# def extract_RDF_triple(text, clean_=True):\n",
    "def predicate_argument_structure_analysis(text, clean_=True):\n",
    "    if clean_:\n",
    "        text = clean_text(text)\n",
    "    doc = nlp(text)\n",
    "    triple_list = []\n",
    "    suit_list = []\n",
    "    for i, token in enumerate( doc ):\n",
    "        if token.pos_ in [\"VERB\", \"ADJ\"]:\n",
    "            # 受動表現の可能性があるものは回避\n",
    "            if i<=len(doc)-2 and doc[i+1].pos_ == \"AUX\" and doc[i+1].lemma_ in [\"れる\", \"られる\"]:\n",
    "                # print(doc)\n",
    "                continue\n",
    "            suit = copy.deepcopy(suit_base)\n",
    "            suit[\"V\"] = token.lemma_\n",
    "            is_exist_case = False\n",
    "            for c in token.children:\n",
    "                if c.dep_ in [\"nsubj\", \"obj\", \"obl\"]:\n",
    "                    noun = c.lemma_\n",
    "                    \n",
    "                    for c2 in c.children:\n",
    "                        if c2.dep_ == \"case\" and c2.orth_ in case_set:\n",
    "                            is_exist_case = True\n",
    "                        # if c2.dep_ == \"case\":\n",
    "                            case = c2.orth_\n",
    "                            if case == \"は\":\n",
    "                                case = \"が\"\n",
    "                            triple_list.append( (noun, case, token.lemma_ ))\n",
    "                            suit[case] = noun\n",
    "            if is_exist_case:\n",
    "                # print(suit)\n",
    "                suit_list.append(suit)\n",
    "             \n",
    "    # return triple_list\n",
    "    return suit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'が': '射撃', 'を': '[NONE]', 'に': '[NONE]', 'V': '得意'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '麻生太郎は射撃が得意だ'\n",
    "predicate_argument_structure_analysis(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141777"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 65.43it/s]\n"
     ]
    }
   ],
   "source": [
    "suit_list = []\n",
    "for utt in tqdm(all_utt[:300]):\n",
    "    suit_list += predicate_argument_structure_analysis(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def triple_tuple(suit):\n",
    "    if isinstance(suit, dict):\n",
    "        return (suit[\"が\"], suit[\"を\"], suit[\"に\"]), suit[\"V\"]\n",
    "    elif isinstance(suit, tuple):\n",
    "        return (suit[0], suit[1], suit[2]), suit[3]\n",
    "    else:\n",
    "        return ()\n",
    "    \n",
    "\n",
    "def tuple_add_V2list(p, V):\n",
    "    suit = list(p)\n",
    "    suit.append(V)\n",
    "    return suit\n",
    "\n",
    "def tuple_add_V2tuple(p, V):\n",
    "    return tuple(tuple_add_V2list(p, V))\n",
    "\n",
    "def suit2list(suit):\n",
    "    return list(suit.values())\n",
    "\n",
    "def suit2tuple(suit):\n",
    "    return tuple(suit.values())\n",
    "\n",
    "def counterfeit_error(suit):\n",
    "    base, V = triple_tuple(suit)\n",
    "    # やっぱり [NONE] が 2つあるやつは無理！\n",
    "    if list(base).count(\"[NONE]\") == 2:\n",
    "        return set()\n",
    "    permutations = list( itertools.permutations(base, 3) )\n",
    "    permutations.remove(base)\n",
    "    suit_per = set(map(tuple_add_V2tuple, permutations, [V]*len(permutations) ))\n",
    "    # return permutations\n",
    "    return suit_per\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_suit = list(map(suit2list, suit_list ))\n",
    "correct_suit_set = set(map(suit2tuple, suit_list ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "print(len(correct_suit_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:00<00:00, 542899.09it/s]\n"
     ]
    }
   ],
   "source": [
    "error_suit_set = set()\n",
    "for suit in tqdm(list(correct_suit_set)):\n",
    "    error_suit_set |= counterfeit_error(suit)\n",
    "    # print(counterfeit_error(suit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate_argument_Xy(A):\n",
    "    suit_list = []\n",
    "    for utt in tqdm(A):\n",
    "        suit_list += predicate_argument_structure_analysis(utt)\n",
    "    \n",
    "    correct_suit_set = set(map(suit2tuple, suit_list ))\n",
    "\n",
    "    error_suit_set = set()\n",
    "    for suit in tqdm(list(correct_suit_set)):\n",
    "        error_suit_set |= counterfeit_error(suit)\n",
    "    # 共通して含まれる要素は削除だ！\n",
    "    correct_suit = list(correct_suit_set - error_suit_set)\n",
    "    error_suit = list(error_suit_set-correct_suit_set)\n",
    "\n",
    "    X_str = correct_suit + error_suit\n",
    "    y = [0]*len(correct_suit) + [1]*len(error_suit)\n",
    "    return X_str, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[192] 2022-01-26 11:12:15,183 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/model.vec\n",
      "[192] 2022-01-26 11:13:17,143 Info gensim.utils :KeyedVectors lifecycle event {'msg': 'loaded (351122, 300) matrix of type float32 from ../../corpus/w2v/model.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-01-26T11:13:17.142675', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "# fasttext\n",
    "# https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "w2v_name =  \"dep-ja-300dim\"\n",
    "w2v_name =  \"model.vec\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141777/141777 [32:36<00:00, 72.48it/s] \n",
      "100%|██████████| 71007/71007 [00:00<00:00, 505548.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X_str, y = predicate_argument_Xy(all_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147640"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(X_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\", \"[NONE]\"]\n",
    "add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "SYMBOL_w2v = dict(zip(add_keys, add_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/utterance/caseframe_symbol2.pickle\n"
     ]
    }
   ],
   "source": [
    "symbol_path = \"../models/utterance/\"\n",
    "symbol_name = \"caseframe_symbol2.pickle\"\n",
    "# symbol_name = \"context_symbol_content.pickle\"\n",
    "symbolM = DataManager(symbol_path)\n",
    "symbolM.save_data(symbol_name, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(word, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    # 形態素が登録されていたとき\n",
    "    \n",
    "    if word in SYMBOL_w2v:\n",
    "        vector = SYMBOL_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOL_w2v[\"[UNK]\"]\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(X_str):\n",
    "    X = []\n",
    "    for suit in tqdm(X_str):\n",
    "        vector = np.array([w2v(w, w2v_model, SYMBOL_w2v) for w in suit ]).flatten()\n",
    "        X.append(vector)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = make_X(X_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X_str, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118112/118112 [00:01<00:00, 78623.49it/s]\n",
      "100%|██████████| 29528/29528 [00:00<00:00, 94179.70it/s] \n"
     ]
    }
   ],
   "source": [
    "X_train = torch.from_numpy( make_X(X_train_str) ) \n",
    "X_test = torch.from_numpy( make_X(X_test_str) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147640"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_str+X_test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前処理なしの大きさ\n",
    "    - 258902\n",
    "\n",
    "- set で前処理した大きさ\n",
    "    - 147640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "epoch_ = 1000\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseFrameModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(CaseFrameModel, self).__init__()    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hid1= embedding_dim*2\n",
    "        self.hid2 = embedding_dim//2\n",
    "        # self.fb_dim = 4\n",
    "        # self.fb_dim = 0\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, self.hid1)\n",
    "        self.fc2 = nn.Linear(self.hid1, self.hid2)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        # self.hidden2tag = nn.Linear(self.hid2+self.fb_dim, tagset_size)\n",
    "        self.hidden2tag = nn.Linear(self.hid2, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.hidden2tag( y )\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 2400 3\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300*4\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 3\n",
    "# seq_len = length\n",
    "print(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CaseFrameModel(EMBEDDING_DIM, OUTPUT_DIM)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 5.0481730881851945\n",
      "epoch 100 \t loss 3.6757177023626184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-04119ab57cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mscore_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mall_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "for epoch in range(epoch_): \n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = data[0].cuda().float()\n",
    "        y_t_tensor = data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        score_ = model(X_t_tensor)\n",
    "        loss_ = loss_function(score_, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score_\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "\n",
    "    \n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(test, pred):\n",
    "    if len(collections.Counter(pred)) <= 2:\n",
    "        print('confusion matrix = \\n', confusion_matrix(y_true=test, y_pred=pred))\n",
    "        print('accuracy = ', accuracy_score(y_true=test, y_pred=pred))\n",
    "        print('precision = ', precision_score(y_true=test, y_pred=pred))\n",
    "        print('recall = ', recall_score(y_true=test, y_pred=pred))\n",
    "        print('f1 score = ', f1_score(y_true=test, y_pred=pred))\n",
    "    else:\n",
    "        print('confusion matrix = \\n', confusion_matrix(y_true=test, y_pred=pred))\n",
    "        print('accuracy = ', accuracy_score(y_true=test, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[12794  1018]\n",
      " [  565 15151]]\n",
      "accuracy =  0.9463898672446491\n",
      "precision =  0.9370400148432185\n",
      "recall =  0.964049376431662\n",
      "f1 score =  0.9503528304845539\n"
     ]
    }
   ],
   "source": [
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/caseframe/CF_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/caseframe/\"\n",
    "model_name = \"CF_v2.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
