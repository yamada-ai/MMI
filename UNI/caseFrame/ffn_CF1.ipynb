{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5016/5016 [00:00<00:00, 1049727.02it/s]\n",
      "100%|██████████| 19999/19999 [00:00<00:00, 1602083.46it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../corpus/NTT/persona.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    convs = json.load(f)\n",
    "all_utt = []\n",
    "for did in tqdm( convs[\"convs\"] ) :\n",
    "    dids = list( did.keys() )[0]\n",
    "    all_utt += did[dids]\n",
    "\n",
    "with open(\"../../corpus/NTT/empathetic.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    convs = json.load(f)\n",
    "for did in tqdm( convs[\"convs\"] ) :\n",
    "    dids = list( did.keys() )[0]\n",
    "    all_utt += did[dids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_plain(text):\n",
    "    text_ = neologdn.normalize(text)\n",
    "    text_ = re.sub(r'\\(.*\\)', \"\", text_)\n",
    "    text_ = re.sub(r'\\d+', \"0\", text_)\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141777/141777 [00:02<00:00, 53327.67it/s]\n"
     ]
    }
   ],
   "source": [
    "all_utt = [clean_text_plain(t) for t in tqdm(all_utt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '静岡県の麻生太郎は1年前から射撃が得意だ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab_analyzer = Analyzer(tokenizer_)\n",
    "mecab_analyzer = Analyzer(Tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_pos(pos_split):\n",
    "    last = len(pos_split) - 1 \n",
    "    for i, _ in enumerate(pos_split):\n",
    "        if pos_split[last-i] != \"*\":\n",
    "            return \"[\"+pos_split[last-i]+\"]\"\n",
    "\n",
    "def sentence2normalize_noun_mecab(sentences):\n",
    "    normalize_sen = []\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]\n",
    "    for sen in sentences:\n",
    "        df = mecab_analyzer.analyze_with_dataframe(sen)\n",
    "        words = []\n",
    "        if df is None:\n",
    "            continue\n",
    "        for txt, pos in zip(df.surface, df.part_of_speech):\n",
    "            pos_split = pos.split(\"-\")\n",
    "            # print(pos_split)\n",
    "            if pos_split[0]==\"名詞\" :\n",
    "                if pos_split[1] == \"固有名詞\" :\n",
    "                    words.append(pos)\n",
    "                elif pos_split[1] == \"数詞\":\n",
    "                    words.append(\"0\")\n",
    "                else:\n",
    "                    words.append(txt)\n",
    "            else:\n",
    "                words.append(txt)\n",
    "        normalize_sen.append(words)\n",
    "    return normalize_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, TOKENS=[\"[PAD]\", \"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\", \"[NONE]\"]):\n",
    "        self.index2item = []\n",
    "        self.item2index = {}\n",
    "\n",
    "        for sp_token in TOKENS:\n",
    "            self.add_item(sp_token)\n",
    "\n",
    "    # 単語数\n",
    "    def __len__(self):\n",
    "        return len(self.item2index)\n",
    "\n",
    "    # 単語が含まれているか\n",
    "    def __contains__(self, item):\n",
    "        return item in self.item2index.keys()\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return str(self.item2index)\n",
    "\n",
    "    # 単語の追加\n",
    "    def add_item(self, item):\n",
    "        # もう登録されてたら登録しないよ\n",
    "        if item in self.item2index:\n",
    "            return\n",
    "        index = len(self.item2index)\n",
    "        self.index2item.append(item)\n",
    "        self.item2index[item] = index\n",
    "    \n",
    "    def add_items(self, items:list):\n",
    "        for item in items:\n",
    "            self.add_item(item)\n",
    "\n",
    "    # 単語の取得\n",
    "    def get_item(self, index):\n",
    "        if len(self.index2item) <= index:\n",
    "            return \"[UNK]\"\n",
    "        return self.index2item[index]\n",
    "\n",
    "    # 単語をidへ\n",
    "    def get_index(self, item):\n",
    "        if item not in self.item2index:\n",
    "            return self.item2index[\"[UNK]\"]\n",
    "        return self.item2index[item]\n",
    "\n",
    "    # def save_vocab(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['名詞-固有名詞-地名-一般',\n",
       "  '県',\n",
       "  'の',\n",
       "  '名詞-固有名詞-人名-姓',\n",
       "  '名詞-固有名詞-人名-名',\n",
       "  'は',\n",
       "  '0',\n",
       "  '年',\n",
       "  '前',\n",
       "  'から',\n",
       "  '射撃',\n",
       "  'が',\n",
       "  '得意',\n",
       "  'だ']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2normalize_noun_mecab(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = [\"[PAD]\", \"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\", \"[NONE]\"]\n",
    "vocab = Vocabulary(TOKENS=TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8208] 2022-02-01 17:06:58,839 Info wakame.analyzer :text is empty!\n"
     ]
    }
   ],
   "source": [
    "normalized = sentence2normalize_noun_mecab(all_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flattened = list(itertools.chain.from_iterable(normalized))\n",
    "vocab.add_items(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2ids(sentence, vocab):\n",
    "    normalized = sentence2normalize_noun_mecab(sentence)[0]\n",
    "    ids = [ vocab.get_index(c) for c in normalized]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1637, 30, 693, 46, 3590, 19, 26, 1623, 111, 21]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"手を貯金に出すのは駄目でしょうが\"\n",
    "sentence2ids(sentence, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26420"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2gram(sentence, vocab, N=5, is_id=True):\n",
    "    normalized = sentence2normalize_noun_mecab(sentence)\n",
    "    if len(normalized)==0:\n",
    "        return []\n",
    "    normalized = [\"FOS\"] + normalized[0] + [\"EOS\"]\n",
    "    # padding\n",
    "    if len(normalized) < N:\n",
    "        normalized += [\"[PAD]\"]*(N-len(normalized)) \n",
    "    # id化を同時に行う場合\n",
    "    if is_id:\n",
    "        normalized = [ vocab.get_index(c) for c in normalized]\n",
    "    ngram_text = []\n",
    "    for i in range(len(normalized)-N+1):\n",
    "            # print(L[i:i+N])\n",
    "        ngram_text.append(normalized[i:i+N])\n",
    "    return ngram_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_y_seq(A, vocab, N=5, is_id=True):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for utt in tqdm(A):\n",
    "        # ngram にした結果がかえってくる\n",
    "        ngram_text = sentence2gram(utt, vocab, N, is_id)\n",
    "        X.extend(ngram_text[:-1])\n",
    "        Y.extend(ngram_text[1:])\n",
    "    return X, Y     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FOS', '手', 'を', '貯金', 'に'],\n",
       " ['手', 'を', '貯金', 'に', '出す'],\n",
       " ['を', '貯金', 'に', '出す', 'の'],\n",
       " ['貯金', 'に', '出す', 'の', 'は'],\n",
       " ['に', '出す', 'の', 'は', '駄目'],\n",
       " ['出す', 'の', 'は', '駄目', 'でしょう'],\n",
       " ['の', 'は', '駄目', 'でしょう', 'が'],\n",
       " ['は', '駄目', 'でしょう', 'が', 'EOS']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2gram(sentence, vocab, N=5, is_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 52216/141777 [01:07<02:06, 706.42it/s][8208] 2022-02-01 17:09:42,852 Info wakame.analyzer :text is empty!\n",
      "100%|██████████| 141777/141777 [02:50<00:00, 833.52it/s] \n"
     ]
    }
   ],
   "source": [
    "X, Y = make_X_y_seq(all_utt, vocab, N=5, is_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(torch.tensor(X), torch.tensor(Y), test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMN5(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LMN5, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        nn.init.normal_(self.embed.weight, std=0.01)\n",
    "        self.linear.weight = self.embed.weight\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb_out = self.embed(x)\n",
    "        # y =self.softmax(tag_space)\n",
    "        out, hc = self.lstm1(emb_out)\n",
    "        y = self.linear(out)\n",
    "        # print(y.shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "epoch_ = 300\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = EMBEDDING_DIM\n",
    "OUTPUT_DIM = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1908041"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMN5(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7454/7454 [07:30<00:00, 16.53it/s]\n",
      "  0%|          | 0/300 [07:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cf5397e72522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mscore_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "import math\n",
    "for epoch in tqdm(range(epoch_)):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss =  0\n",
    "    for data in tqdm(trainloader):\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        # X_t_tensor = data[0].to(torch.int).cuda()\n",
    "        # y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        X_t_tensor = data[0].to(torch.int).cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        # y_t_tensor = F.one_hot(data[1], num_classes=VOCAB_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        score_ = model(X_t_tensor)\n",
    "        # print(score_.view(-1, VOCAB_SIZE).shape,  y_t_tensor.view(-1).shape)\n",
    "        loss_ = loss_function(score_.view(-1, VOCAB_SIZE),  y_t_tensor.view(-1))\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score_\n",
    "        del loss_\n",
    "    ppl = math.exp(all_loss / len(trainloader))\n",
    "    losses.append(ppl)\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", ppl)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "450*100/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def eval_perplexity(model, iterator):\n",
    "    total_loss = 0\n",
    "    hidden = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_t_tensor = data[0].to(torch.int).cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        score_ = model(X_t_tensor)\n",
    "        loss_ = loss_function(score_.view(-1, VOCAB_SIZE),  y_t_tensor.view(-1))\n",
    "        total_loss += loss.item()\n",
    "      \n",
    "    ppl = math.exp(total_loss / len(iterator))\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQaklEQVR4nO3df4xlZX3H8ffHXQQVEMqOBF2X0bhaaeoPOloUFbpaa4EsUROFSBVj3NRfoa2IbrTWbm1sa5tSqtYgpS1SINYUSykqlmWrsUvb2fJLIOpKF2XB7iBCJFpL4ds/7tk6Hebu3Jm5M5d95v1KTuac8zxz7/eZ2f3MM88990yqCknS/u8xoy5AkjQcBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIGu5iXZleQVo65DWmoGuiQ1wkDXipTkwCTnJrmr285NcmDXtibJlUnuS3Jvkq8keUzX9t4ku5P8IMnXk7x8tCORfmL1qAuQRuT9wHHA84AC/g74APCbwLuBO4Gxru9xQCV5FvBO4AVVdVeScWDV8pYt9ecMXSvVG4AtVbWnqqaA3wZ+pWt7EDgKOLqqHqyqr1TvpkcPAQcCxyQ5oKp2VdW3RlK9NAsDXSvVk4E7ph3f0Z0D+CiwE7g6ye1J3gdQVTuBXwM+BOxJclmSJyM9ShjoWqnuAo6edryuO0dV/aCq3l1VTwc2Ar+xd628qi6pqpd0n1vA7y9v2VJ/BrpWigOSHLR3Ay4FPpBkLMka4IPAxQBJTknyjCQB7qe31PJwkmcl2dC9ePpfwI+Ah0czHOmRDHStFFfRC+C920HAJHATcDPw78CHu77rgX8EHgC2A5+oqmvprZ//HnAP8F3gScDm5RuCtG/xD1xIUhucoUtSIwx0SWqEgS5JjTDQJakRI3vr/5o1a2p8fHxUTy9J+6UdO3bcU1Vjs7WNLNDHx8eZnJwc1dNL0n4pyR392lxykaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNWKgQE+yK8nNSW5IMjlLe5Kcl2RnkpuSHDv8UiVJ+7J6Hn1/oaru6dP2y8D6bvt54M+6j5KkZTKsJZdTgYuq5zrgsCRHDemxJUkDGDTQC7g6yY4km2ZpfwrwnWnHd3bn/p8km5JMJpmcmpqaf7WSpL4GDfSXVNWx9JZW3pHkZQt5sqo6v6omqmpibGxsIQ8hSepjoECvqt3dxz3A5cALZ3TZDTx12vHa7pwkaZnMGehJnpDkkL37wCuBr83odgXwxu5ql+OA+6vq7qFXK0nqa5CrXI4ELk+yt/8lVfWFJL8KUFWfBK4CTgJ2Aj8E3rw05UqS+pkz0KvqduC5s5z/5LT9At4x3NIkSfPhO0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRg40JOsSnJ9kitnaTs6yTVJbkqyLcna4ZYpSZrLfGboZwG39Wn7Q+CiqnoOsAX4yGILkyTNz0CB3s24TwYu6NPlGGBrt38tcOriS5MkzcegM/RzgXOAh/u03wi8ptt/NXBIkiNmdkqyKclkksmpqan51ipJ2oc5Az3JKcCeqtqxj25nAyckuR44AdgNPDSzU1WdX1UTVTUxNja20JolSbNYPUCf44GNSU4CDgIOTXJxVZ2xt0NV3UU3Q09yMPDaqrpvCeqVJPUx5wy9qjZX1dqqGgdOA7ZOD3OAJGuS7H2szcCFQ69UkrRPC74OPcmWJBu7wxOBryf5BnAk8LtDqE2SNA+pqpE88cTERE1OTo7kuSVpf5VkR1VNzNbmO0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRg40JOsSnJ9kitnaVuX5Nqu/aYkJw23TEnSXOYzQz8LuK1P2weAz1TV84HTgE8stjBJ0vwMFOhJ1gInAxf06VLAod3+E4G7Fl+aJGk+Vg/Y71zgHOCQPu0fAq5O8i7gCcArZuuUZBOwCWDdunXzqVOSNIc5Z+hJTgH2VNWOfXQ7HfjLqloLnAR8OskjHruqzq+qiaqaGBsbW3DRkqRHGmTJ5XhgY5JdwGXAhiQXz+jzFuAzAFW1HTgIWDPEOiVJc5gz0Ktqc1Wtrapxei94bq2qM2Z0+zbwcoAkz6YX6FNDrlWStA8Lvg49yZYkG7vDdwNvTXIjcClwZlXVMAqUJA1m0BdFAaiqbcC2bv+D087fSm9pRpI0Ir5TVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSI1YN2TLIKmAR2V9UpM9r+GPiF7vDxwJOq6rBhFSlJmtvAgQ6cBdwGHDqzoap+fe9+kncBz198aZKk+RhoySXJWuBk4IIBup8OXLqYoiRJ8zfoGvq5wDnAw/vqlORo4GnA1sWVJUmarzkDPckpwJ6q2jHA450GfLaqHurzWJuSTCaZnJqammepkqR9GWSGfjywMcku4DJgQ5KL+/Q9jX0st1TV+VU1UVUTY2Nj8y5WktTfnIFeVZuram1VjdML7K1VdcbMfkl+Gjgc2D70KiVJc1rwdehJtiTZOO3UacBlVVWLL0uSNF/zuWyRqtoGbOv2Pzij7UPDKkqSNH++U1SSGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhoxcKAnWZXk+iRX9ml/XZJbk9yS5JLhlShJGsTqefQ9C7gNOHRmQ5L1wGbg+Kr6fpInDak+SdKABpqhJ1kLnAxc0KfLW4GPV9X3Aapqz3DKkyQNatAll3OBc4CH+7Q/E3hmkq8muS7Jq2brlGRTkskkk1NTU/OvVpLU15yBnuQUYE9V7dhHt9XAeuBE4HTgU0kOm9mpqs6vqomqmhgbG1tYxZKkWQ0yQz8e2JhkF3AZsCHJxTP63AlcUVUPVtV/AN+gF/CSpGUyZ6BX1eaqWltV48BpwNaqOmNGt8/Rm52TZA29JZjbh1qpJGmfFnwdepItSTZ2h18EvpfkVuBa4D1V9b1hFChJGkyqaiRPPDExUZOTkyN5bknaXyXZUVUTs7X5TlFJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0Y2Z+gSzIF3DGSJ1+cNcA9oy5ima20Ma+08YJj3p8cXVVjszWMLND3V0km+/09v1attDGvtPGCY26FSy6S1AgDXZIaYaDP3/mjLmAEVtqYV9p4wTE3wTV0SWqEM3RJaoSBLkmNMNA7SV6V5OtJdiZ53yztRye5JslNSbYlWTutbV2Sq5PcluTWJOPLWvwCLXLMf5Dklm7M5yXJ8la/MEkuTLInydf6tKcbz85u3MdOa3tTkm9225uWr+qFW+h4kzwvyfbue3xTktcvb+ULt5jvcdd+aJI7k3xseSoeoqpa8RuwCvgW8HTgscCNwDEz+vwN8KZufwPw6Wlt24Bf7PYPBh4/6jEt5ZiBFwNf7R5jFbAdOHHUYxpw3C8DjgW+1qf9JODzQIDjgH/pzv8UcHv38fBu//BRj2cJx/tMYH23/2TgbuCwUY9nKcc8rf1PgEuAj416LPPdnKH3vBDYWVW3V9V/A5cBp87ocwywtdu/dm97kmOA1VX1JYCqeqCqfrg8ZS/KgscMFHAQvR8EBwIHAP+55BUPQVV9Gbh3H11OBS6qnuuAw5IcBfwS8KWqureqvg98CXjV0le8OAsdb1V9o6q+2T3GXcAeYNZ3Jz7aLOJ7TJKfA44Erl76SofPQO95CvCdacd3duemuxF4Tbf/auCQJEfQm8ncl+Rvk1yf5KNJVi15xYu34DFX1XZ6AX93t32xqm5b4nqXS7+vyyBfr/3RnONK8kJ6P7y/tYx1LaVZx5zkMcAfAWePpKohMNAHdzZwQpLrgROA3cBDwGrgpV37C+gtYZw5ohqHbdYxJ3kG8GxgLb3/HBuSvHR0ZWqpdDPXTwNvrqqHR13PEns7cFVV3TnqQhZq9agLeJTYDTx12vHa7tz/6X7tfA1AkoOB11bVfUnuBG6oqtu7ts/RW5f782WoezEWM+a3AtdV1QNd2+eBFwFfWY7Cl1i/r8tu4MQZ57ctW1VLp++/gySHAv8AvL9bmmhFvzG/CHhpkrfTey3ssUkeqKpHXDDwaOUMveffgPVJnpbkscBpwBXTOyRZ0/1KBrAZuHDa5x6WZO/64gbg1mWoebEWM+Zv05u5r05yAL3ZeytLLlcAb+yuhDgOuL+q7ga+CLwyyeFJDgde2Z3b38063u7fxOX01po/O9oSh27WMVfVG6pqXVWN0/vt9KL9KczBGToAVfU/Sd5J7z/oKuDCqrolyRZgsqquoDc7+0iSAr4MvKP73IeSnA1c0126twP41CjGMR+LGTPwWXo/uG6m9wLpF6rq75d7DAuR5FJ641rT/Xb1W/Re1KWqPglcRe8qiJ3AD4E3d233Jvkdej8IAbZU1b5eeHtUWOh4gdfRu1rkiCRndufOrKoblqv2hVrEmPd7vvVfkhrhkoskNcJAl6RGGOiS1AgDXZIaYaBLUiMMdDUryUNJbpi2De2a4iTj/e7mJ42K16GrZT+qqueNughpuThD14qTZFd693O/Ocm/dvem2Tvr3trdI/uaJOu680cmuTzJjd324u6hViX5VHfP8KuTPG5kg5Iw0NW2x81Ycpn+Rxrur6qfBT4GnNud+1Pgr6rqOcBfA+d1588D/qmqnkvvPtu3dOfXAx+vqp8B7gNeu6SjkebgO0XVrO7GSgfPcn4XsKGqbu/uRfPdqjoiyT3AUVX1YHf+7qpak2QKWFtVP572GOP07o++vjt+L3BAVX14GYYmzcoZulaq6rM/Hz+etr/3VsrSyBjoWqleP+3j9m7/n+nddRLgDfzkdsDXAG8DSLIqyROXq0hpPpxRqGWPS3LDtOMvTLsd6uFJbqI3yz69O/cu4C+SvAeY4id34TsLOD/JW+jNxN9G7y81SY8qrqFrxenW0Ceq6p5R1yINk0suktQIZ+iS1Ahn6JLUCANdkhphoEtSIwx0SWqEgS5JjfhfyiH850gSVgsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/CF/\"\n",
    "model_name = \"ffn_CF1.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_path = \"../models/base/\"\n",
    "vocab_name = \"vocab_CF1.pickle\"\n",
    "modelM.save_data(vocab_name, vocab)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
