{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "sys.path.append(\"../response/\")\n",
    "from feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/response/forback1.pickle\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "F1_path = \"../X_y_data/response/\"\n",
    "F1_name = \"forback1.pickle\"\n",
    "featureM1 = DataManager(F1_path)\n",
    "\n",
    "F_fb = featureM1.load_data(F1_name)\n",
    "F_fb.set_preprocessor(Preprocessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response/forback_clf.pickle\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model1_path = \"../models/response/\"\n",
    "model1_name = \"forback_clf.pickle\"\n",
    "modelM1 = DataManager(model1_path)\n",
    "\n",
    "clf_fb = modelM1.load_data(model1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残す形態素\n",
    "pos_sets = set(\"名詞 代名詞 動詞 形容詞 接続詞 連体詞\".split() )\n",
    "def utt2vecter_s(ut):\n",
    "    vector = np.zeros(300)\n",
    "    remains = 0\n",
    "    for token in nlp(ut.utt):\n",
    "        tag = token.tag_.split(\"-\")[0]\n",
    "        if tag in pos_sets:\n",
    "            vector += token.vector\n",
    "            remains += 1\n",
    "    \n",
    "    if remains < 2:\n",
    "        return vector\n",
    "    else:\n",
    "        return vector/remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utt2vecter_BERT(ut):\n",
    "    return Nmodel.encode(ut.utt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_X_diff(convs, errors:list,  dim=300):\n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in tqdm(convs):\n",
    "        # X_ = [ np.zeros(2*dim+2) for _ in range(length-1) ] \n",
    "        # X_ = [ np.zeros(dim+2) for _ in range(length-1) ] \n",
    "        # prev_vector = np.zeros( dim )\n",
    "        for i, ut in enumerate(conv):\n",
    "            # print(i)\n",
    "            if i==0:\n",
    "                continue\n",
    "            if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "            # fb\n",
    "            # 0: 後ろ向き   1: 前向き\n",
    "            # f = F_fb.featurization(ut.utt)\n",
    "            # f2 = F_fb.featurization(conv[i-1].utt)\n",
    "            # fb_proba = clf_fb.predict(f.reshape(1, -1))\n",
    "            # fb2 = clf_fb.predict(f2.reshape(1, -1))\n",
    "            f = [ F_fb.featurization(s) for s in [conv[i-1].utt, ut.utt] ]\n",
    "            fb = clf_fb.predict(f)\n",
    "            fb_ = np.zeros(4)\n",
    "            for j, p in enumerate(fb):\n",
    "                fb_[2*j+int(p)] = 1\n",
    "            # print(i, fb, conv[i-1], ut.utt)\n",
    "            # vector = utt2vecter_mini(ut, dim)\n",
    "            if dim==768:\n",
    "                vector = utt2vecter_BERT(conv[i-1]) - utt2vecter_BERT(ut)\n",
    "            else:\n",
    "                vector = utt2vecter_s(conv[i-1]) - utt2vecter_s(ut)\n",
    "            # prev_vector = vector\n",
    "            x_cat_vector = np.concatenate( [fb_, vector] )\n",
    "            # X_.append( x_cat_vector )\n",
    "            X.append(x_cat_vector)\n",
    "            \n",
    "            y.append(1 if ut.is_error_included(errors) else 0)\n",
    "        # break\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:03<11:08,  3.36s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1644cce748>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yamada/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/yamada/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1291, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "100%|██████████| 200/200 [12:20<00:00,  3.70s/it]\n"
     ]
    }
   ],
   "source": [
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "X, y = make_X_diff(convs, errors, dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/context/topic_proposal_bert.pickle\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../X_y_data/context/\"\n",
    "data_name = \"topic_proposal_bert.pickle\"\n",
    "dataM = DataManager(data_path)\n",
    "dataM.save_data(data_name, [X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/context/topic_proposal_bert.pickle\n"
     ]
    }
   ],
   "source": [
    "if dataM.is_exist(data_name):\n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X = DATA_Xy[0]\n",
    "    y = DATA_Xy[1]\n",
    "else:\n",
    "    dataM.save_data(data_name, [X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 103473.64it/s]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "errors = errors[:1]\n",
    "for conv in tqdm(convs):\n",
    "    for i, ut in enumerate(conv):\n",
    "        if i==0:\n",
    "                continue\n",
    "        if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "        y.append(1 if ut.is_error_included(errors) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(TopicModel, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hid1= embedding_dim*2\n",
    "        self.hid2 = embedding_dim//2\n",
    "        self.fb_dim = 4\n",
    "        # self.fb_dim = 0\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, self.hid1)\n",
    "        self.fc2 = nn.Linear(self.hid1, self.hid2)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        # self.hidden2tag = nn.Linear(self.hid2+self.fb_dim, tagset_size)\n",
    "        self.hidden2tag = nn.Linear(self.hid2, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fb = x[:, :self.fb_dim]\n",
    "        # print(x.shape, fb.shape)\n",
    "        y = F.relu(self.fc1(x[:, self.fb_dim:]))\n",
    "        # y = F.relu(self.fc1(x[]))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        # print(x.shape, torch.cat( (y, fb), 1 ).shape)\n",
    "        # y = self.hidden2tag( torch.cat( (y, fb), 1 ) )\n",
    "        y = self.hidden2tag( y )\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n",
      "1, 2, 3, 5, 6, 10, 15, 30, 43, 86, 129, 215, 258, 430, 645, 1290, "
     ]
    }
   ],
   "source": [
    "X_train = X_train[:-1]\n",
    "y_train = y_train[:-1]\n",
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 129\n",
    "epoch_ = 800\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = emb_dim\n",
    "# EMBEDDING_DIM = 300\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1614, 772)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicModel(EMBEDDING_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 3.1916584968566895\n",
      "epoch 100 \t loss 2.0309180468320847\n",
      "epoch 150 \t loss 1.0906330347061157\n",
      "epoch 200 \t loss 0.7148786447942257\n",
      "epoch 250 \t loss 0.5743600316345692\n",
      "epoch 300 \t loss 0.5197888687252998\n",
      "epoch 350 \t loss 0.494805708527565\n",
      "epoch 400 \t loss 0.48041120544075966\n",
      "epoch 450 \t loss 0.4772825539112091\n",
      "epoch 500 \t loss 0.4708304591476917\n",
      "epoch 550 \t loss 0.4693285981193185\n",
      "epoch 600 \t loss 0.46872336231172085\n",
      "epoch 650 \t loss 0.4638027437031269\n",
      "epoch 700 \t loss 0.465950820595026\n",
      "epoch 750 \t loss 0.4692418519407511\n",
      "epoch 800 \t loss 0.46886991895735264\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        # y_t_tensor = torch.tensor(data[1].reshape(batch_size, 1), device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[267  12]\n",
      " [ 38   6]]\n",
      "accuracy =  0.8452012383900929\n",
      "precision =  0.3333333333333333\n",
      "recall =  0.13636363636363635\n",
      "f1 score =  0.1935483870967742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[262  17]\n",
      " [ 36   8]]\n",
      "accuracy =  0.8359133126934984\n",
      "precision =  0.32\n",
      "recall =  0.18181818181818182\n",
      "f1 score =  0.2318840579710145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1614, 772)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
