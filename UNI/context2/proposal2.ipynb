{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "sys.path.append(\"../response/\")\n",
    "from feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/response/forback1.pickle\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "F1_path = \"../X_y_data/response/\"\n",
    "F1_name = \"forback1.pickle\"\n",
    "featureM1 = DataManager(F1_path)\n",
    "\n",
    "F_fb = featureM1.load_data(F1_name)\n",
    "F_fb.set_preprocessor(Preprocessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response/forback_clf.pickle\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model1_path = \"../models/response/\"\n",
    "model1_name = \"forback_clf.pickle\"\n",
    "modelM1 = DataManager(model1_path)\n",
    "\n",
    "clf_fb = modelM1.load_data(model1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残す形態素\n",
    "pos_sets = set(\"名詞 代名詞 動詞 形容詞 接続詞 連体詞\".split() )\n",
    "def utt2vecter_s(ut):\n",
    "    vector = np.zeros(300)\n",
    "    remains = 0\n",
    "    for token in nlp(ut.utt):\n",
    "        tag = token.tag_.split(\"-\")[0]\n",
    "        if tag in pos_sets:\n",
    "            vector += token.vector\n",
    "            remains += 1\n",
    "    \n",
    "    if remains < 2:\n",
    "        return vector\n",
    "    else:\n",
    "        return vector/remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utt2vecter_BERT(ut):\n",
    "    return Nmodel.encode(ut.utt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_X_diff(convs, errors:list,  dim=300):\n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in tqdm(convs):\n",
    "        # X_ = [ np.zeros(2*dim+2) for _ in range(length-1) ] \n",
    "        # X_ = [ np.zeros(dim+2) for _ in range(length-1) ] \n",
    "        # prev_vector = np.zeros( dim )\n",
    "        for i, ut in enumerate(conv):\n",
    "            # print(i)\n",
    "            if i==0:\n",
    "                continue\n",
    "            if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "            # fb\n",
    "            # 0: 後ろ向き   1: 前向き\n",
    "            # f = F_fb.featurization(ut.utt)\n",
    "            # f2 = F_fb.featurization(conv[i-1].utt)\n",
    "            # fb_proba = clf_fb.predict(f.reshape(1, -1))\n",
    "            # fb2 = clf_fb.predict(f2.reshape(1, -1))\n",
    "            f = [ F_fb.featurization(s) for s in [conv[i-1].utt, ut.utt] ]\n",
    "            fb = clf_fb.predict(f)\n",
    "            fb_ = np.zeros(4)\n",
    "            for j, p in enumerate(fb):\n",
    "                fb_[2*j+int(p)] = 1\n",
    "            # print(i, fb, conv[i-1], ut.utt)\n",
    "            # vector = utt2vecter_mini(ut, dim)\n",
    "            if dim==768:\n",
    "                vector = utt2vecter_BERT(conv[i-1]) - utt2vecter_BERT(ut)\n",
    "            else:\n",
    "                vector = utt2vecter_s(conv[i-1]) - utt2vecter_s(ut)\n",
    "            # prev_vector = vector\n",
    "            x_cat_vector = np.concatenate( [fb_, vector] )\n",
    "            # X_.append( x_cat_vector )\n",
    "            X.append(x_cat_vector)\n",
    "            \n",
    "            y.append(1 if ut.is_error_included(errors) else 0)\n",
    "        # break\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:03<11:08,  3.36s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f1644cce748>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yamada/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/yamada/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1291, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "100%|██████████| 200/200 [12:20<00:00,  3.70s/it]\n"
     ]
    }
   ],
   "source": [
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "X, y = make_X_diff(convs, errors, dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/context/topic_proposal_bert.pickle\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../X_y_data/context/\"\n",
    "data_name = \"topic_proposal_bert.pickle\"\n",
    "dataM = DataManager(data_path)\n",
    "dataM.save_data(data_name, [X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 74406.67it/s]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "# errors = errors[:1]\n",
    "for conv in tqdm(convs):\n",
    "    for i, ut in enumerate(conv):\n",
    "        if i==0:\n",
    "                continue\n",
    "        if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "        y.append(1 if ut.is_error_included(errors) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(TopicModel, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hid1= embedding_dim*2\n",
    "        self.hid2 = embedding_dim//2\n",
    "        self.fb_dim = 4\n",
    "        # self.fb_dim = 0\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, self.hid1)\n",
    "        self.fc2 = nn.Linear(self.hid1, self.hid2)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(self.hid2+self.fb_dim, tagset_size)\n",
    "        # self.hidden2tag = nn.Linear(self.hid2, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fb = x[:, :self.fb_dim]\n",
    "        # print(x.shape, fb.shape)\n",
    "        y = F.relu(self.fc1(x[:, self.fb_dim:]))\n",
    "        # y = F.relu(self.fc1(x[]))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        # print(x.shape, torch.cat( (y, fb), 1 ).shape)\n",
    "        y = self.hidden2tag( torch.cat( (y, fb), 1 ) )\n",
    "        # y = self.hidden2tag( y )\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n",
      "1, 2, 3, 5, 6, 10, 15, 30, 43, 86, 129, 215, 258, 430, 645, 1290, "
     ]
    }
   ],
   "source": [
    "X_train = X_train[:-1]\n",
    "y_train = y_train[:-1]\n",
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 129\n",
    "epoch_ = 1000\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = emb_dim\n",
    "# EMBEDDING_DIM = 300\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1614, 772)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicModel(EMBEDDING_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 1.7360963821411133\n",
      "epoch 100 \t loss 1.5479111820459366\n",
      "epoch 150 \t loss 1.4854982942342758\n",
      "epoch 200 \t loss 1.449194461107254\n",
      "epoch 250 \t loss 1.4438022002577782\n",
      "epoch 300 \t loss 1.461687296628952\n",
      "epoch 350 \t loss 1.419068731367588\n",
      "epoch 400 \t loss 1.4484570547938347\n",
      "epoch 450 \t loss 1.4310324639081955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-ddd3907c3989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mall_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        # y_t_tensor = torch.tensor(data[1].reshape(batch_size, 1), device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEWCAYAAABPON1ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkM0lEQVR4nO3dd3xc1Z338c9vRmXUu1Us27KxcQVcRAtl6WAgJAskISFsQgq72WwCWZ4ESMiG7MKyC7sJLU8IISHPZiGwEAg1IRTTNoAtV3Dvlqxu9S6NzvPHjIRsy7ZsNJ72fb9e8/LMvVej3722v3Pm3HPPNeccIiISuTzhLkBERA5OQS0iEuEU1CIiEU5BLSIS4RTUIiIRTkEtIhLhFNQiIhFOQS1Rzcx2mNl54a5DJJQU1CIiEU5BLTHHzJLN7B4zqw4+7jGz5OC6fDN7wcxazKzJzN42M09w3U1mttvM2s1so5mdG949EQlICHcBIiHwA+AUYD7ggGeBW4EfAjcCVUBBcNtTAGdmM4F/AE50zlWbWRngPbpli4xOLWqJRVcD/+ycq3fONQA/Bq4JrusHioEpzrl+59zbLjDhjR9IBuaYWaJzbodzbmtYqhfZh4JaYlEJsHPE653BZQB3A1uAP5vZNjO7GcA5twW4AbgNqDezx82sBJEIoKCWWFQNTBnxenJwGc65dufcjc65acBlwD8O9UU75x5zzp0e/FkH/PvRLVtkdApqiQWJZuYbegC/A241swIzywf+CfhvADO71Mymm5kBrQS6PAbNbKaZnRM86dgDdAOD4dkdkb0pqCUWvEQgWIcePqACWAN8AKwAbg9uOwN4FegA3gX+r3NuCYH+6X8DGoFaYAJwy9HbBZEDM904QEQksqlFLSIS4RTUIiIRTkEtIhLhFNQiIhEuJJeQ5+fnu7KyslC8tYhITFq+fHmjc65gtHUhCeqysjIqKipC8dYiIjHJzHYeaJ26PkREIpyCWkQkwh0yqIOX1q4a8WgzsxuOQm0iIsIY+qidcxsJzOuLmXmB3cAzoS1LRESGHG7Xx7nAVufcATu9RURkfB1uUF9FYGay/ZjZdWZWYWYVDQ0NH78yEREBDiOozSyJwPy9T4623jn3kHOu3DlXXlAw6lBAERE5AofTol4MrHDO1YWiEOcc9722mTc3qTUuIjLS4QT15zlAt8d4MDN++dY2lmyoD9WvEBGJSmMKajNLA84Hng5lMXnpSTR29IbyV4iIRJ0xXULunOsE8kJcC/npyezp6Av1rxERiSoRdWViXnoSezrVohYRGSnCgjqZRrWoRUT2ElFBnZ+WRHNXHwN+3fxZRGRIZAV1RjLOQXNXf7hLERGJGBEV1HlpyQAa+SEiMkJEBXVRViCoa1t7wlyJiEjkiKigLslOAWB3S3eYKxERiRwRFdQTMnx4PUZNq4JaRGRIRAW112MUZfqoblHXh4jIkIgKaoCSbB/V6voQERkWcUE9MTuFqmYFtYjIkIgL6sl5adS0dtM74A93KSIiESHignpKbiqDDrWqRUSCIi6oy/JTAdi1pyvMlYiIRIaIC+rJuWkA7NzTGeZKREQiQ8QFdX56EmlJXnaoRS0iAkRgUJsZk/PS2NWkoBYRgQgMaoCyvFR2qOtDRASI0KCenJdKVVM3/kEX7lJERMIuIoN6Sm4aff5BzfkhIkKEBnVZnoboiYgMicignhwM6p06oSgiEplBXZyVQpLXoxOKIiJEaFB7PUZpboq6PkREiNCghsCcH7roRUQkkoM6L41dezpxTkP0RCS+RXBQp9LZ52dPZ1+4SxERCasxBbWZZZvZU2a2wczWm9mpoS6sLE+TM4mIwNhb1PcCf3LOzQJOANaHrqSA4SF66qcWkTiXcKgNzCwLOBP4MoBzrg8IeX9EaU4KHkMnFEUk7o2lRT0VaAAeMbOVZvawmaXtu5GZXWdmFWZW0dDQ8LELS07wUpyVwi51fYhInBtLUCcAC4GfO+cWAJ3Azftu5Jx7yDlX7pwrLygoGJfipuRpiJ6IyFiCugqocs69H3z9FIHgDrkpmpdaROTQQe2cqwUqzWxmcNG5wLqQVhU0JS+Vps4+2nv6j8avExGJSIc8mRj0LeBRM0sCtgHXhq6kj5TmpABQ3dLDzKLEo/ErRUQizpiC2jm3CigPbSn7K8keCupuZhZlHO1fLyISESL2ykSAicGg3t2iGwiISPyK6KAuSE8m0WtUK6hFJI5FdFB7PEZRlk9BLSJxLaKDGqAkK4Xqlp5wlyEiEjYRH9QTs1PURy0icS3ig7okO4Xath78g5qXWkTiU1QEtX/QUd+u7g8RiU9RENQ+AJ1QFJG4FfFB/dFYarWoRSQ+RXxQF4+4OlFEJB5FfFCnJyeQlZKooBaRuBXxQQ2BE4oKahGJV1ER1BOzfeqjFpG4FRVBXZKdwu5m3UBAROJT1AR1W8+AbiAgInEpaoIaoKZV3R8iEn+iIqgnBi960ZwfIhKPoiKoSzSWWkTiWFQE9YQMH16PbiAgIvEpKoLa6zGKMn2al1pE4lJUBDVoXmoRiV9RE9Ql2boll4jEpygK6hRqW3UDARGJP1EV1AODjob23nCXIiJyVEVNUH80L7W6P0QkvkRNUGsstYjEqygKat2SS0TiU8JYNjKzHUA74AcGnHPloSxqNBm+RDJ9CQpqEYk7YwrqoLOdc40hq2QMSrJTNC+1iMSdqOn6gMAJRbWoRSTejDWoHfBnM1tuZteNtoGZXWdmFWZW0dDQMH4VjlCc7aO6VUEtIvFlrEF9unNuIbAY+KaZnbnvBs65h5xz5c658oKCgnEtckhxVgotXf109/lD8v4iIpFoTEHtnNsd/LMeeAY4KZRFHUhxVmDkR22b+qlFJH4cMqjNLM3MMoaeAxcAH4a6sNEUBYO6Rt0fIhJHxjLqoxB4xsyGtn/MOfenkFZ1AMVZgYteanVLLhGJI4cMaufcNuCEo1DLIRVlqutDROJPVA3PS0nykpWSqBa1iMSVqApqCJxQ1N3IRSSeRF1QF2X51KIWkbgSdUGtFrWIxJuoC+qizBQaO3rpGxgMdykiIkdF1AX10EUvdRr5ISJxIuqCukhXJ4pInIm6oC4evjpRQS0i8SHqgrpwqEWty8hFJE5EXVBnJCeQluSltlV3IxeR+BB1QW1mgbHUbWpRi0h8iLqghsDkTOqjFpF4EZVBrasTRSSeRGVQF2f5qG/vZcCvi15EJPZFZVAXZfnwDzoaO/rCXYqISMhFZVAX604vIhJHojKoizJ1pxcRiR/RGdS6OlFE4khUBnVOaiJJCR7N9yEicSEqg9rMKNYQPRGJE1EZ1BC40a2CWkTiQdQGdXGWjxpdRi4icSBqg7ooK4W61l4GB124SxERCamoDeriLB99/kGaunTRi4jEtqgN6uE7vaifWkRiXPQGdabGUotIfIjaoC7WnV5EJE6MOajNzGtmK83shVAWNFZ56ckkeEwtahGJeYfTor4eWB+qQg6X12MUaiy1iMSBMQW1mZUClwAPh7acwxO4JZeCWkRi21hb1PcA3wMOOFO/mV1nZhVmVtHQ0DAetR2S7vQiIvHgkEFtZpcC9c655Qfbzjn3kHOu3DlXXlBQMG4FHkxxpo+a1h6c00UvIhK7xtKiPg24zMx2AI8D55jZf4e0qjEqyvLR3e+nrXsg3KWIiITMIYPaOXeLc67UOVcGXAW87pz7YsgrG4Pheak154eIxLCoHUcNI2/JpX5qEYldCYezsXPuDeCNkFRyBIqydEsuEYl9Ud2inpCRjJla1CIS26I6qBO9HgrSk3UZuYjEtKgOagjeQEAtahGJYVEf1EVZPup0daKIxLCoD+rirBS1qEUkpkV9UBdl+WjvGaCjVxe9iEhsiv6gztSdXkQktkV/UOuWXCIS46I+qD+6OlFD9EQkNkV9UBeq60NEYlzUB7Uv0UtuWhI1GqInIjEq6oMaAicU1aIWkVgVE0GtqxNFJJbFRFAHbsmlk4kiEptiI6gzfTR39dPT7w93KSIi4y42gjo4RE9zfohILIqJoC4O3kBA/dQiEotiIqiLdNGLiMSwmAjqkuxAUFe3qEUtIrEnJoI6NSmB/PQkKpu6wl2KiMi4i4mgBpiYk0pls4JaRGJPzAT1pJwUKpvURy0isSd2gjo3leqWbvyDLtyliIiMq9gJ6pxUBgYdtRpLLSIxJnaCOjcwllonFEUk1sROUOekAgpqEYk9hwxqM/OZ2VIzW21ma83sx0ejsMNVkp2CGVQ264SiiMSWhDFs0wuc45zrMLNE4B0z+6Nz7r0Q13ZYkhI8FGf6qFKLWkRizCGD2jnngI7gy8TgIyKHVpTmpFKlFrWIxJgx9VGbmdfMVgH1wCvOufdDWtURmpiTwu4WBbWIxJYxBbVzzu+cmw+UAieZ2bx9tzGz68yswswqGhoaxrnMsZmYnUJtWw8D/sGw/H4RkVA4rFEfzrkWYAlw0SjrHnLOlTvnygsKCsapvMMzMScFv8ZSi0iMGcuojwIzyw4+TwHOBzaEuK4jUpoTGEu9W/3UIhJDxtKiLgaWmNkaYBmBPuoXQlvWkSkNjqXepZEfIhJDxjLqYw2w4CjU8rFNykkhyetha0NnuEsRERk3MXNlIkCC10NZfipb6jsOvbGISJSIqaAGOKYgna0NCmoRiR0xF9TTJ6Szc08nvQP+cJciIjIuYjKoBx3saNQJRRGJDTEX1McUpAOon1pEYkZMBrWZglpEYkfMBXVKkpeJ2Sls0QlFEYkRMRfUEGhVq0UtIrEiJoN6+oR0tjV0MKgb3YpIDIjZoO4dGNSUpyISE2IyqGdMCIz82FTXHuZKREQ+vpgM6plFGQCsq24LcyUiIh9fTAZ1hi+RKXmprFVQi0gMiMmgBphbksnamtZwlyEi8rHFcFBnUdnUTWt3f7hLERH5WGI2qOeUZALqpxaR6BezQT13KKhrFNQiEt1iNqgnZPgozExmTVVLuEsREflYYjaoARZNyaFiR3O4yxAR+VhiPKhz2d3STW1rT7hLERE5YjEd1CdPzQXg3W2NYa5EROTIxXRQzynOJDctibc3KahFJHrFdFB7PMZp0/N5a3MjzmkmPRGJTjEd1ABnzMinsaOXDbWaoElEolNcBDXA86urw1yJiMiRifmgLs5K4ZMnlPDgm1vZuacz3OWIiBy2mA9qgO9dOJNBB6+urw93KSIih+2QQW1mk8xsiZmtM7O1Znb90ShsPE3KTWX6hHReW18X7lJERA7bWFrUA8CNzrk5wCnAN81sTmjLGn8Xzyvi3W17qGnV7blEJLocMqidczXOuRXB5+3AemBiqAsbb1csKsU5eHrF7nCXIiJyWA6rj9rMyoAFwPshqSaEpuSlcdLUXJ6sqNSYahGJKmMOajNLB34P3OCc22/uUDO7zswqzKyioaFhPGscN1cuKmXHni7WVOnOLyISPcYU1GaWSCCkH3XOPT3aNs65h5xz5c658oKCgvGscdxcMKcQr8d4eW1tuEsRERmzsYz6MOBXwHrn3E9CX1LoZKcmceaMfP7r3Z1sbegIdzkiImMylhb1acA1wDlmtir4uDjEdYXMv15+HF6P8f2nP1BftYhEhbGM+njHOWfOueOdc/ODj5eORnGhUJyVwk0XzeL97U08ubyKjt4Begf84S5LROSA4uLKxH1ddeIkTirL5XtPrWHej17mjhfXh7skEZEDisug9niMX197Il87fSoAL31QE+aKREQOLC6DGiA9OYFbL53DDy6eTWNHH39UWItIhIrboB5yxrH5mME3Hl3BFx9+n8qmrnCXJCKyl7gP6llFmSz7wXlcd+Y0lu9s5qJ73uKp5VXhLktEZFhCuAuIBPnpyXz/4tlcvnAitz7zId97ajUdPf0UZvro6B3gsvkl/HltHRfMLSQ5wRvuckUkzlgoxhKXl5e7ioqKcX/fo6Gzd4CrH36fVZUt+6377oUz+dszp5HgjfsvIiIyzsxsuXOufLR1Spx9pCUn8OTfncp3L5zJpNwU7rz8OAozkwG4++WNXHDPW2ypH/tVjf5BR2tXP/VtPWP+mb6BQdZUtRxu6SISo9SiPgjnHGaGf9Bx4h2v0tTZB4Av0cO3zpnB1SdPZl11G33+Qc6aOWGvn3l1XR33vrYZj0FBho/VVS1cffJk0pMT+NoZ02js6CUvLYm11W2sq2njs+WThn/v7S+s4+F3tvOnG85gVlFmWPZdIktjRy93/2kj371oJvnpyeEuR0LgYC1q9VEfRGCaE/B6jAe+sIDX19fz9TOn8Q+PreDulzdy98sbh7e95PhiijJ9LN/ZTFVzF40dfSPeKTBb3z2vbgagprWHX72znds/PY9b//AhAL94cyv3fG4BWxs6eCV4J5oX19QclaCuae2mrq2X+ZOyQ/67jlRNazc3/s9qfvLZ+RRl+cJdzlH39IoqnqiopK69h99ce1K4y4kpz6+u5uW1tdxw3gymT8gIdzmjUov6CG2obePxpZX85i87AEjwGAODHx3LSbkp/PzqRVx6/zsAzCnOZF3NfrPDHtSEjGQ+U15KktfL3501jadX7ObiecX89NVN9PsHOXlaHqdMzWVC5sGDq6q5i8JMH4kj+tZ7+v14PUai18OFP32LjXXtvHvLORRnpYz6HkPfFEazpqqF/6mo5MeXzcPrGX2bsbzPwQx9y/j2uTP4x/OP3W99S1cf2alJh/2+0eDht7dxe/Dq2SSvh1U/Op/UpOhuY/X0++noHaCquTvsDYTF977N+po2vnTqFH78qXlhq0Mt6hCYVZTJbZfN5bbL5vL+tj1MzU9jycZ6Mn2JpPsSWDQlh9SkBP7jMydQ19bDtaeV8fjSSv75hXV7vc9Fc4tYtqOJPZ19ey2/9ZLZ3P7ien62ZCsASzbWs6qyhVue/mB4m0ff3xWsJYO/OraAL59WRlpyApVNXcwuysQMKnY285kH3wXg+nMD3TUVO5v51u9WcuHcQu67agEb69oBePCNrdy0eBYpiV6eW13Nyl0tfPkTZexu6ebvfruc+7+wgLNmTmBHYyfF2T6SE7xsrmvnsgf+F4AFk3KYkpdKeVnuXvvinOOJZZXc//oWystyuPeqBcPrBvyD9PsdKUmB0TTdfX4GnSMtee9/mtsbA3eQ3zrK+YE3NzXwld8s498uP47PjOhCGk1Vcxeb6zo49Zg8khM8mBnbGztp6uxl0ZRA3U8s28Wbmxqoau7mzsuPY25J1gHf753NjaQle1kwOeeA2zR19nHFz//CDy+dzTmzCg9a35DO3gESvEaS18N//nkTAMcWprOproOnV+zmi6dMOeR7tPf089v3djK7KJOzZ00Y0+89Wv7Pk6t5YU3gIrPHvnYyn5ieH5Y6uvoG2FgbaEC9t60pLDWMhVrUR9GAf5A7/7iBaQVpzC3JYmp+GlkpifgHHVsbOqhp7eFvf1tBT/8g2++8mKm3jD731cLJ2dS19TKzKIP3tu2hqy8wqZTXY6QnJ9Da3T/mmnJSE2nu6ic5wUPvwCAApTkpVDXvf2/JlEQvNy+exY+eW8s5syZw7uwJ3PHi+uHfP+S82YXcfeXx+J1j6fYm3trUwOPLKofXf2p+CTcvnkVv/yD3vbaZV9bX8eK3zmB1VQt3vLgev3O8c9PZ1Lf18samBo6dkM6XHllKT3+gvruuOJ75k7MpzUmht3+QC+95i/r2XvLTk/ivr5zMnJKPuoteWVfHA69vpqW7n3uvWsC3f7eSXcGLmj5bXso3z57OX939BgCbbl+M12Mc8/2Pjntqkpfff+MTzCrKwMz4cHcrT6/YzdmzCvjRc2vZ1tBJgsfY8q+BCSXf27aHXXu62NXUxSnT8piSl8rvV1Rxz6ub+esFE/np5+bTO+Dn5bV1XDCnEF/i/sM9e/r9XHzf2zgHt102ly/9eik3XTSLq0+ZzMl3vEZ3v59vnTOdGy+YSU+/f7/3eHzpLmYUpvPoe7t4euVuMnwJLPvBecPb/eqd7TyxbBfP/P1p+30gjtTc2UdyomfMrffO3gGau/oozUnda3nvgB/n2KvOsptfHH7+nfOO5frzZgCB/yPAqCOrfvLKJlbuauaXf1O+13sN+Ad5bOkurlhYesD9OdA3ub9sbeQLv3yfk8pyWbqjife/fy6FmT62N3by1qYGrlx04PccbwdrUSuoI0xtaw+9A36m5KXxm//dzr/9aQOL5xXz7KrdPPP3p7GrqYuTpuaSn5483M3wize38viyShraezl5ai6FWT4eC7a2ARbPK+KG846lprWbHz+/jp5+P1kpiWyobWdaQRqfWTSJvzq2gFv/8AErdrUAcNbMAjbXdVDX1oPHYxxbmE51S8/wCdUhmb4Evnn2dOaWZPHq+jq2NnTw9ubGUfft3684jpt+/8Go6/blS/QMB/OQ3339FO56eQMrgzXmpiXhMaO1u49bFs/mP/+8kdTkBC5fOJGmjj4um1/CtY8s26tL6mAyfAksnJzDm5sCdyialp/GtmBLPtFrTMjwsbtl9JsjF2f5mJqfxl+27jngfiR6jVsWz+bZ1dWsrmzhq6dPZW5JJjWtPVw4t5A7XlxPU1c/u/Z00tz10YdtRnICb990NtmpSWxv7OQnr2zi+dXVXLGwlOdXV3NR8MbNualJTMhM5u3NjSR6jX6/Y9GUHJbvbObi44qYPymbC+cWccXP36Wxo5dvnHUMN55/LM+vqWbp9maaOnv5l0/No7vfT2evn0vuf5tFk3N48JpF5Kcns7G2nR8++yH56YHjnp2ayKScVL5+xjQ8HuPLjyzljY0NrPjh+ayqbObe17Zw8tRclmyop7K5i9s/fRxXLiqlvr2Hk+54jaQED30Dg5w2PY8HPr+Q659YxVubGphVlMFdVx7P8aXZrKlq4fYX1nPdmdP42n8FMmXexExuvWQOBRnJDPgdG2rbuP7xVXy2vJRMXyLbGzt58JpFJHo9VDV38cDrW3hrUwPnzymktq2H2y6bS3NnP6W5KXzlkWVsqG3nd18/hU8+EOimfPUfz+S259bxzpZGEjzGtaeV8YNL5rC1oYOJ2Smsq2lj+oR0Mn2JQOBE7+Cg484/buA75x3L5Ly9P6jGSkEdxZxztHT1s31PJwsP8vV635/ZuaeL0pwUvB47YJ/wil3NzC3J3OsinmU7mvjXl9bzi2sWkZLoJdHrwQy8ZnT1+/mwqpWZRRk8tbyKlCQv15wyZb/3P/OuJexq6uLyhRO5+uQp3P/6Zjxm/OwLC7nhiZXkpycPd9sAnDQ1lwWTs+nq9fPY0l1MyU3l+NIszIxPzS9hR2MnXq+Ha06ZwtaGDu58aT1zS7LY2tBB78Ag1505jRPLcvnDyt3c8MQqgOEQAHjk2hOZXZTJv7ywjk117fzqSyfywe5WVlU2M31COpNyU7nh8VX0DgzS2t1PbloSc0sy+f7Fs/n1O9t5cnkVly+cyOrKFnY1dfHgFxfxi7e2BeaLuWQ25/znm/sd2zsvP46BQccPgyeLv3HWMTz45lacg6yURAado71n4IB/h//xmRN4fUMdL31Qyw8vncNXgxOIQaAr5ZP3v7Pfh8bQeZKSLB8l2Sm09fTz7DdP57tPfdTNMGRuSSZrq9vISknc6xtYSZaP9t6B/Wo7aWouS7cHugYmZqdgxqjfugDK8lKpbO7GP+IDcmJ2ynC9Gb4Euvv8vHT9GTy9YjcPvrl1v/cwCzQw3tzYQOeIb2zfPncG/7OsktpDDHfNTUsiOzWR6pbu/T7w93X9uTP4zvnH8pXfLOP1DfV77bMv0ctbmxqGP3DTkrx09vnJTk3k8gWltHb38/sVH13JfEJpFs/8/Wl4DnGuZjQKajmqtjZ0sLmug4vmFQH7f+10zrG2ug2zwH+okScwBwfdEf0jH/rZlz6soSQ7hYL0ZC69/x0+W17KDy6Zc8if7fcPkuAxKpu6KchIHu4z7xsYpK6th0m5gVZS74B/+INtaL9WV7bQ2NFLhi8RM5icm0ph8ATv9sZO6tt6OHlaHvXtPQwOQn56Eg549L2dpCUnUNvaw/NrqplRmMGLa2p45TtnMqMwg7aefjbWtnPiPn3+sHff/rOrdlOYGWjRJ3iM1KQEfIkeBgYdiV7P8Id9dWs3L6+t4/Tp+RxTkMb3n/mAvoFBPnfiZM6fU8jqqhZ+9OxaPtjdyvlzCrlgTiFvbGqgb2CQps4+qlu6ue7MaVx72tThY3HrMx/yZHDKhXkTMzln5gRe+KCGybmp3H3lCfzHyxu55PhiTizL5duPr6Sn309BRjKL5xVz/pxCKpu6OOOuJXg9xm+/chKnHpPHns4+fvTsWt7YWM+ZxxZwyfHF/MNjK0lO8LDx9sU0dvTyxYffZ0NtO5+eX8IfVlWTkZzAZfNLOGvmBHbu6eQvW/dQ3dLNMRPSuerESUwrSGfnnk4a2nt5sqIKX6KHd7fu4V8+PY+/XjBxeBjubc+t5bfv7eSqEydx66VzaO7s44y7lgDw1wsmsqupi0xfAkkJHl5ZV8egC1zZ3NjRS1KCh08eX8Ltn543/O/ncCioJS519/nxJXqOaJRJOAwOOna3dA9/KISrhs6+ATKCX+vHoqWrj831HaN+oIzFh7tbmZKXetDf+fDb2ygvyx0eIeIfdLT39JOdmkRDey++RM9h1QyjNwp6+v1srG3nhBEjUX7+xlbmT8rm1GPy9tq2trWHtp5+slMSWVXZwrmzCw856ulgFNQiIhFOl5CLiEQxBbWISIRTUIuIRDgFtYhIhFNQi4hEOAW1iEiEU1CLiEQ4BbWISIQLyQUvZtYA7DyCH80HRp/RJ37E+zGI9/0HHQOIz2MwxTlXMNqKkAT1kTKzigNdmRMv4v0YxPv+g44B6BjsS10fIiIRTkEtIhLhIi2oHwp3AREg3o9BvO8/6BiAjsFeIqqPWkRE9hdpLWoREdmHglpEJMJFTFCb2UVmttHMtpjZzeGuJxTM7NdmVm9mH45Ylmtmr5jZ5uCfOcHlZmb3BY/HGjNbGL7Kx4+ZTTKzJWa2zszWmtn1weVxcRzMzGdmS81sdXD/fxxcPtXM3g/u5xNmlhRcnhx8vSW4viysOzCOzMxrZivN7IXg67g7BmMVEUFtZl7gZ8BiYA7weTM79I3uos9vgIv2WXYz8JpzbgbwWvA1BI7FjODjOuDnR6nGUBsAbnTOzQFOAb4Z/LuOl+PQC5zjnDsBmA9cZGanAP8O/NQ5Nx1oBr4a3P6rQHNw+U+D28WK64H1I17H4zEYG+dc2B/AqcDLI17fAtwS7rpCtK9lwIcjXm8EioPPi4GNwee/AD4/2nax9ACeBc6Px+MApAIrgJMJXIWXEFw+/P8BeBk4Nfg8Ibidhbv2cdj3UgIfyOcALwAWb8fgcB4R0aIGJgKVI15XBZfFg0LnXE3weS1QGHwe88ck+BV2AfA+cXQcgl/5VwH1wCvAVqDFOTcQ3GTkPg7vf3B9K7D3XVaj0z3A94DB4Os84u8YjFmkBLUALtBkiIvxkmaWDvweuME51zZyXawfB+ec3zk3n0Cr8iRgVngrOrrM7FKg3jm3PNy1RItICerdwKQRr0uDy+JBnZkVAwT/rA8uj9ljYmaJBEL6Uefc08HFcXccnHMtwBICX/OzzSwhuGrkPg7vf3B9FrDn6FY67k4DLjOzHcDjBLo/7iW+jsFhiZSgXgbMCJ71TQKuAp4Lc01Hy3PAl4LPv0Sgz3Zo+d8ERz2cArSO6BqIWmZmwK+A9c65n4xYFRfHwcwKzCw7+DyFQP/8egKBfWVws333f+i4XAm8HvzGEbWcc7c450qdc2UE/q+/7py7mjg6Boct3J3kI04uXAxsItBf94Nw1xOiffwdUAP0E+iD+yqBvrbXgM3Aq0BucFsjMBJmK/ABUB7u+sfpGJxOoFtjDbAq+Lg4Xo4DcDywMrj/HwL/FFw+DVgKbAGeBJKDy33B11uC66eFex/G+XicBbwQz8dgLA9dQi4iEuEipetDREQOQEEtIhLhFNQiIhFOQS0iEuEU1CIiEU5BLVHJzPxmtmrEY9xmXDSzspEzHIqEW8KhNxGJSN0ucBm2SMxTi1piipntMLO7zOyD4LzP04PLy8zs9eCc1q+Z2eTg8kIzeyY4P/RqM/tE8K28ZvbL4JzRfw5eRSgSFgpqiVYp+3R9fG7Eulbn3HHAAwRmaQO4H/h/zrnjgUeB+4LL7wPedIH5oRcCa4PLZwA/c87NBVqAK0K6NyIHoSsTJSqZWYdzLn2U5TsITMy/LTj5U61zLs/MGgnMY90fXF7jnMs3swag1DnXO+I9yoBXXOAmBpjZTUCic+72o7BrIvtRi1pikTvA88PRO+K5H53PkTBSUEss+tyIP98NPv8LgZnaAK4G3g4+fw34BgxP6J91tIoUGSu1EiRapQTvkjLkT865oSF6OWa2hkCr+PPBZd8CHjGz7wINwLXB5dcDD5nZVwm0nL9BYIZDkYihPmqJKcE+6nLnXGO4axEZL+r6EBGJcGpRi4hEOLWoRUQinIJaRCTCKahFRCKcglpEJMIpqEVEItz/B/BRxvmwAP9MAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[110  62]\n",
      " [ 80  71]]\n",
      "accuracy =  0.5603715170278638\n",
      "precision =  0.5338345864661654\n",
      "recall =  0.47019867549668876\n",
      "f1 score =  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[107  65]\n",
      " [ 85  66]]\n",
      "accuracy =  0.5356037151702786\n",
      "precision =  0.5038167938931297\n",
      "recall =  0.4370860927152318\n",
      "f1 score =  0.46808510638297873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  1.        , ...,  0.54550761,\n",
       "        -1.60003138,  0.66063464],\n",
       "       [ 1.        ,  0.        ,  1.        , ...,  1.23768914,\n",
       "        -0.61395842,  0.03287311],\n",
       "       [ 0.        ,  1.        ,  1.        , ...,  0.47885269,\n",
       "        -1.83739126, -0.64305329],\n",
       "       ...,\n",
       "       [ 1.        ,  0.        ,  0.        , ..., -0.98536295,\n",
       "        -0.2166487 ,  0.24594551],\n",
       "       [ 1.        ,  0.        ,  0.        , ..., -0.4329949 ,\n",
       "        -0.45106125,  0.09849131],\n",
       "       [ 1.        ,  0.        ,  0.        , ...,  0.41134602,\n",
       "         0.71596408,  1.44327331]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
