{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../response/\")\n",
    "from feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/response/forback1.pickle\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "F1_path = \"../X_y_data/response/\"\n",
    "F1_name = \"forback1.pickle\"\n",
    "featureM1 = DataManager(F1_path)\n",
    "\n",
    "F_fb = featureM1.load_data(F1_name)\n",
    "F_fb.set_preprocessor(Preprocessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response/forback_clf.pickle\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model1_path = \"../models/response/\"\n",
    "model1_name = \"forback_clf.pickle\"\n",
    "modelM1 = DataManager(model1_path)\n",
    "\n",
    "clf_fb = modelM1.load_data(model1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残す形態素\n",
    "pos_sets = set(\"名詞 代名詞 動詞 形容詞 接続詞 連体詞\".split() )\n",
    "def utt2vecter_s(ut):\n",
    "    vector = np.zeros(300)\n",
    "    remains = 0\n",
    "    for token in nlp(ut.utt):\n",
    "        tag = token.tag_.split(\"-\")[0]\n",
    "        if tag in pos_sets:\n",
    "            vector += token.vector\n",
    "            remains += 1\n",
    "    \n",
    "    if remains < 2:\n",
    "        return vector\n",
    "    else:\n",
    "        return vector/remains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utt2vecter_mini(ut, dim=10):\n",
    "    vector = np.zeros(300)\n",
    "    remains = 0\n",
    "    for token in nlp(ut.utt):\n",
    "        tag = token.tag_.split(\"-\")[0]\n",
    "        if tag in pos_sets:\n",
    "            vector += token.vector\n",
    "            remains += 1\n",
    "    \n",
    "    if remains < 2:\n",
    "        return vector[:dim]\n",
    "    else:\n",
    "        return vector[:dim]/remains\n",
    "    # return np.zeros(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utt2vecter_BERT(ut):\n",
    "    return Nmodel.encode(ut.utt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_Xy_conv(convs, length, errors:list,  dim=300):\n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in tqdm(convs):\n",
    "        # X_ = [ np.zeros(2*dim+2) for _ in range(length-1) ] \n",
    "        X_ = [ np.zeros(dim+2) for _ in range(length-1) ] \n",
    "        prev_vector = np.zeros( dim )\n",
    "        for i, ut in enumerate(conv):\n",
    "\n",
    "            # fb\n",
    "            # 0: 後ろ向き   1: 前向き\n",
    "            f = F_fb.featurization(ut.utt)\n",
    "            fb_proba = clf_fb.predict_proba(f.reshape(1, -1))[0]\n",
    "            # vector = utt2vecter_mini(ut, dim)\n",
    "            if dim==768:\n",
    "                vector = utt2vecter_BERT(ut)\n",
    "            else:\n",
    "                vector = utt2vecter_s(ut)\n",
    "\n",
    "            # x_cat_vector = np.concatenate( [fb_proba, prev_vector-vector, vector] )\n",
    "            x_cat_vector = np.concatenate( [fb_proba, prev_vector-vector] )\n",
    "            prev_vector = vector\n",
    "            X_.append( x_cat_vector )\n",
    "            if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "            X.append(X_[-length:])\n",
    "            \n",
    "            y.append(1 if ut.is_error_included(errors) else 0)\n",
    "        # break\n",
    "    return np.array(X), np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy_conv2(convs, length, errors:list,  dim=300):\n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in tqdm(convs):\n",
    "        # X_ = [ np.zeros(2*dim+2) for _ in range(length-1) ] \n",
    "        X_ = [ np.zeros(dim+2) for _ in range(length-1) ] \n",
    "        prev_vector = np.zeros( dim )\n",
    "        for i, ut in enumerate(conv):\n",
    "            if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "            # fb\n",
    "            # 0: 後ろ向き   1: 前向き\n",
    "            f = F_fb.featurization(ut.utt)\n",
    "            fb_proba = clf_fb.predict_proba(f.reshape(1, -1))[0]\n",
    "            # vector = utt2vecter_mini(ut, dim)\n",
    "            if dim==768:\n",
    "                vector = utt2vecter_BERT(ut)\n",
    "            else:\n",
    "                vector = utt2vecter_s(ut)\n",
    "\n",
    "            # x_cat_vector = np.concatenate( [fb_proba, prev_vector-vector, vector] )\n",
    "            x_cat_vector = np.concatenate( [fb_proba, prev_vector-vector] )\n",
    "            prev_vector = vector\n",
    "            X_.append( x_cat_vector )\n",
    "\n",
    "            if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "            X.append(X_[-length:])\n",
    "            \n",
    "            y.append(1 if ut.is_error_included(errors) else 0)\n",
    "        # break\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [20:29<00:00,  6.15s/it]\n"
     ]
    }
   ],
   "source": [
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "# errors = errorprint(errors)\n",
    "emb_dim = 768\n",
    "X, _ = make_Xy_conv(convs, length, errors, dim=emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 78670.24it/s]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "errors = errors[:1]\n",
    "for conv in tqdm(convs):\n",
    "    for i, ut in enumerate(conv):\n",
    "        if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "        y.append(1 if ut.is_error_included(errors) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/context/topic_proposal_bert1.pickle\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../X_y_data/context/\"\n",
    "data_name = \"topic_proposal_bert1.pickle\"\n",
    "dataM = DataManager(data_path)\n",
    "\n",
    "if dataM.is_exist(data_name):\n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X = DATA_Xy[0]\n",
    "    y = DATA_Xy[1]\n",
    "else:\n",
    "    dataM.save_data(data_name, [X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True, bidirectional=True )\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        # _, hidden_layer = self.lstm(x[:, :, 2:])\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        bilstm_out = torch.cat([hidden_layer[0][0], hidden_layer[0][1]], dim=1)\n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        y = self.hidden2tag(bilstm_out)\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split = int(len(X)*0.8)\n",
    "# X_train, X_test = X[:split], X[split:]\n",
    "# y_train, y_test = y[:split], y[split:]\n",
    "# print(len(X), len(X_test))\n",
    "np.count_nonzero(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450\n",
      "1, 2, 5, 10, 25, 29, 50, 58, 145, 290, 725, 1450, "
     ]
    }
   ],
   "source": [
    "X_train = X_train[:-1]\n",
    "y_train = y_train[:-1]\n",
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 145\n",
    "epoch_ = 400\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1814, 4, 770)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = emb_dim+2\n",
    "HIDDEN_DIM = emb_dim//2\n",
    "\n",
    "# EMBEDDING_DIM = 2*emb_dim+2\n",
    "# HIDDEN_DIM = emb_dim\n",
    "\n",
    "OUTPUT_DIM = 2\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 0.1391377728432417\n",
      "epoch 100 \t loss 0.01408341689966619\n",
      "epoch 150 \t loss 0.0046214310568757355\n",
      "epoch 200 \t loss 0.0021462365984916687\n",
      "epoch 250 \t loss 0.0011725914737326093\n",
      "epoch 300 \t loss 0.0007029699554550461\n",
      "epoch 350 \t loss 0.0004470148924156092\n",
      "epoch 400 \t loss 0.00029588289908133447\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        # y_t_tensor = torch.tensor(data[1].reshape(batch_size, 1), device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[317   9]\n",
      " [ 34   3]]\n",
      "accuracy =  0.8815426997245179\n",
      "precision =  0.25\n",
      "recall =  0.08108108108108109\n",
      "f1 score =  0.12244897959183675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_X_str(convs, length):\n",
    "    X_str = []\n",
    "    y = []\n",
    "    for conv in tqdm(convs):\n",
    "        X_ = []\n",
    "        for i, ut in enumerate(conv):\n",
    "            X_.append( ut.utt )\n",
    "            if not ut.is_system() or ut.is_utt_level_error():\n",
    "                continue\n",
    "            X_str.append(X_[-length:])\n",
    "        # break\n",
    "    return X_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 122069.38it/s]\n"
     ]
    }
   ],
   "source": [
    "X_str = make_X_str(convs, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y_test)):\n",
    "\n",
    "#         # 本来エラーではないけどエラー扱い\n",
    "#         # if y_test[j]==0 and y_pred[j]==1:\n",
    "#         #     print(\"本来エラーではないけどエラー扱い\", ut.did)\n",
    "#         #     print(conv_list[-length:])\n",
    "#         #     print()\n",
    "#         # if y_test[j]==1 and y_pred[j]==0:\n",
    "#         #     print(\"本来エラーなのに非エラー扱い\", ut.did)\n",
    "#         #     print(conv_list)\n",
    "#         #     print()\n",
    "#     j = i+split\n",
    "#     if y_test[i]==1 and y_pred[i]==1:\n",
    "#         print(\"よく検出した！えらいぞ\", ut.did)\n",
    "#         print(X_str[j])\n",
    "#         print()\n",
    "#     pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
