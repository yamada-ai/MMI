{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import select\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaConfig\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "# from pyknp import Juman\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        # self.model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "        # self.sen_model = SentenceTransformer(self.model_path, show_progress_bar=False)\n",
    "\n",
    "        # 半角全角英数字\n",
    "        # self.DELETE_PATTERN_1 = re.compile(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+')\n",
    "        # 記号\n",
    "        # self.DELETE_PATTERN_2 = re.compile(\n",
    "        #     r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+')\n",
    "        \n",
    "        self.emb_size = self.get_sentence_vec(\"emb\").shape[0]\n",
    "        print(self.emb_size)\n",
    "\n",
    "    def get_sentence_vec(self, sen) -> np.array:\n",
    "        # sen_ = self.DELETE_PATTERN_1.sub(sen)\n",
    "        # sen_ = self.DELETE_PATTERN_2.sub(\"\", sen)\n",
    "        sentence_vec = self.nlp(sen).vector\n",
    "        # sentence_vec = self.sen_model.encode(sen)[0]\n",
    "        return sentence_vec\n",
    "    \n",
    "    def read_json_with_NoErr(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" :\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            if t[\"error_category\"]:\n",
    "                                ec = t[\"error_category\"]\n",
    "                            else:\n",
    "                                ec = [\"No-Err\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "    \n",
    "    def make_error_dict(self, error_types):\n",
    "        error_dict = {}\n",
    "        for e in error_types:\n",
    "            error_dict[e] = len(error_dict)\n",
    "        return error_dict\n",
    "    \n",
    "    def extract_X_y(self, df:pd.DataFrame, error_types, prev_num) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        n = prev_num\n",
    "        # print(did)\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "\n",
    "        # 初期の調整 padding\n",
    "        for i in range(n-1):\n",
    "            sequence_did.append(\n",
    "                np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "            )\n",
    "\n",
    "        # didごとに返却する？\n",
    "        # エラーが発生したら、開始からエラーまでの文脈を入力とする(N=5の固定長でも可能)\n",
    "        # 先にこのベクトル列を作成し，Tensorに変換して， List に保持\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                did = d\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "                # 初期の調整 padding\n",
    "                for i in range(n-1):\n",
    "                    sequence_did.append(\n",
    "                            np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "                        )\n",
    "                # break\n",
    "\n",
    "            # sequence_did.append([u, s])\n",
    "            sequence_did.append(\n",
    "                    np.concatenate(\n",
    "                        [self.get_sentence_vec(u), self.get_sentence_vec(s)]\n",
    "                    )\n",
    "                # [u, s]\n",
    "            )\n",
    "            if e[0] == \"No-Err\":\n",
    "                continue\n",
    "            else:\n",
    "                y_each_error_label = np.zeros(len(error_types))\n",
    "                for e_ in e:\n",
    "                    y_each_error_label[error_dict[e_]] = 1\n",
    "                X_data.append(sequence_did[-n:])\n",
    "                # y_did = np.array(y_each_error_label)\n",
    "                y_data.append(y_each_error_label)\n",
    "        return np.array(X_data), np.array(y_data)\n",
    "    \n",
    "    # 特定のエラーを取得\n",
    "    def particular_error_usr(self, df:pd.DataFrame, error_set, prev_num) -> np.array:\n",
    "        sequence_did = []\n",
    "        n = prev_num\n",
    "        X_data = []\n",
    "        y_data = [] \n",
    "        for d, u, s, ec in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            for e in ec:\n",
    "                if e in error_set:\n",
    "                    X_data.append(self.get_sentence_vec(u))\n",
    "                    y_data.append(1)\n",
    "        # エラーの回数\n",
    "        error_num = len(X_data)\n",
    "        # df_sample = df.sample(n=errror_num+50)\n",
    "        i = 0\n",
    "        print(\"error num:{0}\".format(error_num))\n",
    "        while True:\n",
    "            df_ = df.sample()\n",
    "            u = str(df_.usr).split()[1]\n",
    "            if \"？\" in u or \"?\" in u:\n",
    "                continue    \n",
    "            for e in ec:\n",
    "                if e in error_set:\n",
    "                    continue\n",
    "            \n",
    "            X_data.append(self.get_sentence_vec(u))\n",
    "            y_data.append(0)\n",
    "            # print(u)\n",
    "            i += 1\n",
    "            if i % 50 == 0:\n",
    "                print(\"i is {0}\".format(i))\n",
    "            if i == error_num:\n",
    "                break\n",
    "        \n",
    "        return np.array(X_data),np.array(y_data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pre = preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "path = './error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "# datalist = ['DCM']\n",
    "    # List of error types\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    "    'Ignore question', 'Topic transition error', \n",
    "    'Lack of information', 'Repetition', \n",
    "    'Contradiction', 'Self-contradiction',\n",
    "    'Lack of common sense', 'Semantic error',\n",
    "    'Grammatical error', 'Ignore proposal', \n",
    "    'Ignore offer', 'Lack of sociality', \n",
    "    'Uninterpretable', 'Ignore greeting', \n",
    "    'No-Err']\n",
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Ignore question'}"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "error_set = set([\"Ignore question\"])\n",
    "error_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問しているかを判定する識別機\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.f1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.f2 = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.f1(x)\n",
    "        y = torch.relu(y)\n",
    "        y = self.f2(y)\n",
    "        y = F.log_softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = pre.emb_size\n",
    "HIDDEN_DIM = pre.emb_size*2\n",
    "OUTPUT_DIM = 2\n",
    "mode = \"ginza\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error num:305\n",
      "i is 50\n",
      "i is 100\n",
      "i is 150\n",
      "i is 200\n",
      "i is 250\n",
      "i is 300\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = pre.particular_error_usr(df, error_set, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "427\n1, 7, 61, 427, "
     ]
    }
   ],
   "source": [
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 61\n",
    "# epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, batch_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "epoch 50 \t loss 1.3962918370962143\n",
      "epoch 100 \t loss 0.8420506790280342\n",
      "epoch 150 \t loss 0.6225544288754463\n",
      "epoch 200 \t loss 0.49515509232878685\n",
      "epoch 250 \t loss 0.41137826442718506\n",
      "epoch 300 \t loss 0.36107369139790535\n",
      "epoch 350 \t loss 0.3224391434341669\n",
      "epoch 400 \t loss 0.2912396192550659\n",
      "epoch 450 \t loss 0.2706135744228959\n",
      "epoch 500 \t loss 0.24983276147395372\n",
      "epoch 550 \t loss 0.2346956105902791\n",
      "epoch 600 \t loss 0.22321150824427605\n",
      "epoch 650 \t loss 0.21316982433199883\n",
      "epoch 700 \t loss 0.2014686381444335\n",
      "epoch 750 \t loss 0.19621401745826006\n",
      "epoch 800 \t loss 0.18643072480335832\n",
      "epoch 850 \t loss 0.1765866477508098\n",
      "epoch 900 \t loss 0.16973617300391197\n",
      "epoch 950 \t loss 0.16625222098082304\n",
      "epoch 1000 \t loss 0.15863456041552126\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(1000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred = np.array(model(X_tensor).cpu()).argmax(axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9016393442622951"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  app.launch_new_instance()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "# query = [\"銀閣寺は何故銀色ではないのですか？\", \"それは違うと思います\"]\n",
    "query = \"それは違うと思います\"\n",
    "vec = pre.get_sentence_vec(query)\n",
    "vec_ = torch.tensor(vec, device='cuda:0').float()\n",
    "score = model(vec_)\n",
    "score_np = score.cpu().detach().numpy()\n",
    "score_np.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}