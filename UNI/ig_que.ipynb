{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import select\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaConfig\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "# from pyknp import Juman\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        # self.model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "        # self.sen_model = SentenceTransformer(self.model_path, show_progress_bar=False)\n",
    "\n",
    "        # 半角全角英数字\n",
    "        # self.DELETE_PATTERN_1 = re.compile(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+')\n",
    "        # 記号\n",
    "        # self.DELETE_PATTERN_2 = re.compile(\n",
    "        #     r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+')\n",
    "        \n",
    "        self.emb_size = self.get_sentence_vec(\"emb\").shape[0]\n",
    "        print(self.emb_size)\n",
    "\n",
    "    def get_sentence_vec(self, sen) -> np.array:\n",
    "        # sen_ = self.DELETE_PATTERN_1.sub(sen)\n",
    "        # sen_ = self.DELETE_PATTERN_2.sub(\"\", sen)\n",
    "        sentence_vec = self.nlp(sen).vector\n",
    "        # sentence_vec = self.sen_model.encode(sen)[0]\n",
    "        return sentence_vec\n",
    "    \n",
    "    def read_json_with_NoErr(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" :\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            if t[\"error_category\"]:\n",
    "                                ec = t[\"error_category\"]\n",
    "                            else:\n",
    "                                ec = [\"No-Err\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "    \n",
    "    def make_error_dict(self, error_types):\n",
    "        error_dict = {}\n",
    "        for e in error_types:\n",
    "            error_dict[e] = len(error_dict)\n",
    "        return error_dict\n",
    "    \n",
    "    def extract_X_y(self, df:pd.DataFrame, error_types, prev_num) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        n = prev_num\n",
    "        # print(did)\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "\n",
    "        # 初期の調整 padding\n",
    "        for i in range(n-1):\n",
    "            sequence_did.append(\n",
    "                np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "            )\n",
    "\n",
    "        # didごとに返却する？\n",
    "        # エラーが発生したら、開始からエラーまでの文脈を入力とする(N=5の固定長でも可能)\n",
    "        # 先にこのベクトル列を作成し，Tensorに変換して， List に保持\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                did = d\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "                # 初期の調整 padding\n",
    "                for i in range(n-1):\n",
    "                    sequence_did.append(\n",
    "                            np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "                        )\n",
    "                # break\n",
    "\n",
    "            # sequence_did.append([u, s])\n",
    "            sequence_did.append(\n",
    "                    np.concatenate(\n",
    "                        [self.get_sentence_vec(u), self.get_sentence_vec(s)]\n",
    "                    )\n",
    "                # [u, s]\n",
    "            )\n",
    "            if e[0] == \"No-Err\":\n",
    "                continue\n",
    "            else:\n",
    "                y_each_error_label = np.zeros(len(error_types))\n",
    "                for e_ in e:\n",
    "                    y_each_error_label[error_dict[e_]] = 1\n",
    "                X_data.append(sequence_did[-n:])\n",
    "                # y_did = np.array(y_each_error_label)\n",
    "                y_data.append(y_each_error_label)\n",
    "        return np.array(X_data), np.array(y_data)\n",
    "    \n",
    "    # 特定のエラーを取得\n",
    "    def particular_error_usr(self, df:pd.DataFrame, error_set, prev_num) -> np.array:\n",
    "        sequence_did = []\n",
    "        n = prev_num\n",
    "        X_data = []\n",
    "        y_data = [] \n",
    "        for d, u, s, ec in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            for e in ec:\n",
    "                if e in error_set:\n",
    "                    X_data.append(self.get_sentence_vec(u))\n",
    "                    y_data.append(1)\n",
    "        # エラーの回数\n",
    "        error_num = len(X_data)\n",
    "        # df_sample = df.sample(n=errror_num+50)\n",
    "        i = 0\n",
    "        print(\"error num:{0}\".format(error_num))\n",
    "        while True:\n",
    "            df_ = df.sample()\n",
    "            u = str(df_.usr).split()[1]\n",
    "            if \"？\" in u or \"?\" in u:\n",
    "                continue    \n",
    "            for e in ec:\n",
    "                if e in error_set:\n",
    "                    continue\n",
    "            \n",
    "            X_data.append(self.get_sentence_vec(u))\n",
    "            y_data.append(0)\n",
    "            # print(u)\n",
    "            i += 1\n",
    "            if i % 50 == 0:\n",
    "                print(\"i is {0}\".format(i))\n",
    "            if i == error_num:\n",
    "                break\n",
    "        \n",
    "        return np.array(X_data),np.array(y_data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pre = preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "path = './error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "# datalist = ['DCM']\n",
    "    # List of error types\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    "    'Ignore question', 'Topic transition error', \n",
    "    'Lack of information', 'Repetition', \n",
    "    'Contradiction', 'Self-contradiction',\n",
    "    'Lack of common sense', 'Semantic error',\n",
    "    'Grammatical error', 'Ignore proposal', \n",
    "    'Ignore offer', 'Lack of sociality', \n",
    "    'Uninterpretable', 'Ignore greeting', \n",
    "    'No-Err']\n",
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ignore question'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_set = set([\"Ignore question\"])\n",
    "error_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問しているかを判定する識別機\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.f1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.f2 = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.f1(x)\n",
    "        y = torch.relu(y)\n",
    "        y = self.f2(y)\n",
    "        y = F.log_softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = pre.emb_size\n",
    "HIDDEN_DIM = pre.emb_size*2\n",
    "OUTPUT_DIM = 2\n",
    "mode = \"ginza\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particular_name = \"particular\"\n",
    "particular_error= \"ig_question\"\n",
    "model_path = \"./models/{0}/\".format(particular_name)\n",
    "model_name = \"model_{0}_{1}.pickle\".format(particular_error, mode)\n",
    "modelM = DataManager(model_path)\n",
    "model_name\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8ed23e5c04ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodelM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-0e42789e0066>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"success load : {0}{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    136\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "if modelM.is_exist(model_name):\n",
    "    model = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error num:305\n",
      "i is 50\n",
      "i is 100\n",
      "i is 150\n",
      "i is 200\n",
      "i is 250\n",
      "i is 300\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = pre.particular_error_usr(df, error_set, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n",
      "1, 7, 61, 427, "
     ]
    }
   ],
   "source": [
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 61\n",
    "# epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, batch_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "ipykernel_launcher:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "ipykernel_launcher:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 1.5156225711107254\n",
      "epoch 100 \t loss 0.8507134988903999\n",
      "epoch 150 \t loss 0.5924378745257854\n",
      "epoch 200 \t loss 0.43736477941274643\n",
      "epoch 250 \t loss 0.3377938885241747\n",
      "epoch 300 \t loss 0.2623869962990284\n",
      "epoch 350 \t loss 0.2042178250849247\n",
      "epoch 400 \t loss 0.1626150319352746\n",
      "epoch 450 \t loss 0.12914054188877344\n",
      "epoch 500 \t loss 0.1023526107892394\n",
      "epoch 550 \t loss 0.0822045598179102\n",
      "epoch 600 \t loss 0.0653474589344114\n",
      "epoch 650 \t loss 0.05322202853858471\n",
      "epoch 700 \t loss 0.0419496838003397\n",
      "epoch 750 \t loss 0.034301294945180416\n",
      "epoch 800 \t loss 0.027080659056082368\n",
      "epoch 850 \t loss 0.022061792318709195\n",
      "epoch 900 \t loss 0.017942906357347965\n",
      "epoch 950 \t loss 0.014610492973588407\n",
      "epoch 1000 \t loss 0.011934398906305432\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(1000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred = np.array(model(X_tensor).cpu()).argmax(axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912568306010929"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = [\"銀閣寺は何故銀色ではないのですか？\", \"それは違うと思います\"]\n",
    "query = \"ほんとうですか？\"\n",
    "vec = pre.get_sentence_vec(query)\n",
    "vec_ = torch.tensor(vec, device='cuda:0').float()\n",
    "score = model(vec_)\n",
    "score_np = score.cpu().detach().numpy()\n",
    "score_np.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ./models/particular/model_ig_question_ginza.pickle\n"
     ]
    }
   ],
   "source": [
    "if not modelM.is_exist(model_name):\n",
    "    modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# 質問扱いのものがいくつ存在するか\n",
    "\n",
    "user_list = []\n",
    "\n",
    "for u in df.usr:\n",
    "    user_list.append(pre.get_sentence_vec(u))\n",
    "print(len(user_list))\n",
    "# 推論\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(user_list, device='cuda:0').float()\n",
    "    y_pred = np.array(model(X_tensor).cpu()).argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6a2dcd2a4688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0merror_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_q\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_q\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"？\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"?\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(np.nonzero(y_pred>0)[0]))\n",
    "num = 0\n",
    "error_part = 0\n",
    "for d, u, s, ec, is_q in zip(df.did, df.usr, df.sys, df.ec, y_pred):\n",
    "    if is_q and (\"？\" in u or \"?\" in u):\n",
    "        # print(u)\n",
    "        num += 1\n",
    "        is_error = False\n",
    "        for e in ec:\n",
    "            if e in error_set:\n",
    "                error_part += 1\n",
    "                is_error = True\n",
    "        if not is_error:\n",
    "            print(u, s)\n",
    "print(\"ある程度質問形式とみられる数\", num)\n",
    "print(\"質問と見られる中で．\", error_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}