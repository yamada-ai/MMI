{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import select\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaConfig\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        self.model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "        self.sen_model = SentenceTransformer(self.model_path, show_progress_bar=False)\n",
    "\n",
    "        # 半角全角英数字\n",
    "        # self.DELETE_PATTERN_1 = re.compile(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+')\n",
    "        # 記号\n",
    "        self.DELETE_PATTERN_2 = re.compile(\n",
    "            r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+')\n",
    "        \n",
    "        self.emb_size = self.get_sentence_vec(\"emb\").shape[0]\n",
    "        print(self.emb_size)\n",
    "\n",
    "    def get_sentence_vec(self, sen) -> np.array:\n",
    "        # sen_ = self.DELETE_PATTERN_1.sub(sen)\n",
    "        sen_ = self.DELETE_PATTERN_2.sub(\"\", sen)\n",
    "        sentence_vec = self.nlp(sen_).vector\n",
    "        # sentence_vec = self.sen_model.encode(sen)[0]\n",
    "        return sentence_vec\n",
    "    \n",
    "    def read_json_with_NoErr(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" :\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            if t[\"error_category\"]:\n",
    "                                ec = t[\"error_category\"]\n",
    "                            else:\n",
    "                                ec = [\"No-Err\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "    \n",
    "    def make_error_dict(self, error_types):\n",
    "        error_dict = {}\n",
    "        for e in error_types:\n",
    "            error_dict[e] = len(error_dict)\n",
    "        return error_dict\n",
    "    \n",
    "    def extract_X_y(self, df:pd.DataFrame, error_types, prev_num) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        n = prev_num\n",
    "        # print(did)\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "\n",
    "        # 初期の調整 padding\n",
    "        for i in range(n-1):\n",
    "            sequence_did.append(\n",
    "                np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "            )\n",
    "\n",
    "        # didごとに返却する？\n",
    "        # エラーが発生したら、開始からエラーまでの文脈を入力とする(N=5の固定長でも可能)\n",
    "        # 先にこのベクトル列を作成し，Tensorに変換して， List に保持\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                did = d\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "                # 初期の調整 padding\n",
    "                for i in range(n-1):\n",
    "                    sequence_did.append(\n",
    "                            np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "                        )\n",
    "                # break\n",
    "\n",
    "            # sequence_did.append([u, s])\n",
    "            sequence_did.append(\n",
    "                    np.concatenate(\n",
    "                        [self.get_sentence_vec(u), self.get_sentence_vec(s)]\n",
    "                    )\n",
    "                # [u, s]\n",
    "            )\n",
    "            if e[0] == \"No-Err\":\n",
    "                continue\n",
    "            else:\n",
    "                y_each_error_label = np.zeros(len(error_types))\n",
    "                for e_ in e:\n",
    "                    y_each_error_label[error_dict[e_]] = 1\n",
    "                X_data.append(sequence_did[-n:])\n",
    "                # y_did = np.array(y_each_error_label)\n",
    "                y_data.append(y_each_error_label)\n",
    "        return np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_least_oneClass(clf, X) -> np.array:\n",
    "    y_pred = clf.predict(X)\n",
    "    p = clf.predict_proba(X)\n",
    "    # print(y_pred)\n",
    "    proba = np.array([[p[c][i][1] if (p[c][i].shape[0]!=1) else 0 \n",
    "                     for c in range(len(error_types))] for i in range(len(X))])\n",
    "    # print(proba)\n",
    "  # replace [] to the highest probability label\n",
    "    y_pred2 = np.empty((0, len(error_types)), int)\n",
    "    for y, pr in zip(y_pred, proba):\n",
    "        if  (sum(y) == 0):\n",
    "            ans = np.zeros_like(y)\n",
    "            ans[np.argmax(pr)] = 1\n",
    "        else:\n",
    "            ans = y\n",
    "        y_pred2 = np.append(y_pred2, np.array([ans]), axis=0)\n",
    "    return y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLSTMClassifier:\n",
    "    def __init__(self, embedding_dim, hidden_dim, target_size, batch_size, CUDA=True):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.multi_models = self._make_model()\n",
    "    \n",
    "        if CUDA:\n",
    "            self._model_toCUDA()\n",
    "\n",
    "        self.is_set_target_names = False\n",
    "\n",
    "    def _make_model(self):\n",
    "        multi_models = []\n",
    "        for _ in range(self.target_size):\n",
    "            multi_models.append( LSTMClassifier(self.embedding_dim, self.hidden_dim, 2, self.batch_size) )\n",
    "        return multi_models\n",
    "\n",
    "    def _model_toCUDA(self):\n",
    "        if torch.cuda.is_available():\n",
    "            for model in self.multi_models:\n",
    "                model.cuda()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def set_optimizer(self, lr_=0.01):\n",
    "        self.optimizers = []\n",
    "        for model in self.multi_models:\n",
    "            self.optimizers.append(optim.Adam(model.parameters(), lr=lr_))\n",
    "    \n",
    "    def set_loss_func(self):\n",
    "        self.loss_funcs = []\n",
    "        for _ in self.multi_models:\n",
    "            self.loss_funcs.append(nn.NLLLoss())\n",
    "\n",
    "    def set_target_names(self, target_names):\n",
    "        self.target_names = target_names\n",
    "        self.is_set_target_names = True\n",
    "\n",
    "    def train(self, X, y, epoch=100, loss_border=0):\n",
    "        # データセットを初期化\n",
    "        # datasets = []\n",
    "        # trainloaders = []\n",
    "        \n",
    "        \n",
    "        # for i in range(self.target_size):\n",
    "        #     datasets.append()\n",
    "        #     trainloaders.append()\n",
    "        \n",
    "        # 学習をそれぞれのエラーで\n",
    "        for i, model in enumerate(self.multi_models):\n",
    "            dataset = Datasets(X, y[:, i])\n",
    "            trainloader = torch.utils.data.DataLoader(dataset, batch_size = self.batch_size, shuffle = True, num_workers = 2)\n",
    "            optimizer = self.optimizers[i]\n",
    "            loss_function = self.loss_funcs[i]\n",
    "            if self.is_set_target_names:\n",
    "                print(\"error :\", self.target_names[i], \"\\tstart\")\n",
    "            else:\n",
    "                print(\"error :\", i, \"\\tstart\")\n",
    "            for ep in range(epoch):\n",
    "                all_loss = 0\n",
    "                for data in trainloader:\n",
    "                    X_t = data[0]\n",
    "                    y_t = data[1]\n",
    "                    X_tensor = torch.tensor(X_t, device='cuda:0').float()\n",
    "                    y_tensor = torch.tensor(y_t, dtype=torch.long, device='cuda:0')\n",
    "                    optimizer.zero_grad()\n",
    "                    model.zero_grad()\n",
    "                    \n",
    "                    score = model(X_tensor)\n",
    "                    loss_ = loss_function(score, y_tensor)\n",
    "                    loss_.backward()\n",
    "                    all_loss += loss_.item()\n",
    "                    del score\n",
    "                    del loss_\n",
    "                    optimizer.step()\n",
    "                if (ep+1) % 50 == 0:\n",
    "                    # print(\"model[{0}], \".format(i), \"epoch\", ep+1, \"\\t\" , \"loss\", all_loss)\n",
    "                    print(\"model[{0}] epoch :{1} \\t loss :{2}\".format(i, ep+1, all_loss))\n",
    "                if all_loss <= loss_border:\n",
    "                    print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "                    break\n",
    "            print(\"\")\n",
    "    \n",
    "    def predict(self, X, y, at_least_oneClass=False):\n",
    "        if at_least_oneClass:\n",
    "            return\n",
    "        \n",
    "        # とりあえず argmax で 0 or 1 を獲得\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, device='cuda:0').float()\n",
    "            y_tensor = torch.tensor(y, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "            y_pred = np.array(self.multi_models[0](X_tensor).cpu()).argmax(axis=1).reshape(-1,1)\n",
    "            for i in range(1, self.target_size):\n",
    "                model = self.multi_models[i]\n",
    "                y_pred_each = np.array(model(X_tensor).cpu()).argmax(axis=1).reshape(-1,1)\n",
    "                y_pred = np.concatenate([y_pred, y_pred_each], 1)\n",
    "        \n",
    "        return y_pred\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pre = preprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = './error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "    # List of error types\n",
    "# error_types = ['Ignore question', 'Unclear intention', \n",
    "#             'Wrong information', 'Topic transition error', \n",
    "#             'Lack of information', 'Repetition', \n",
    "#             'Semantic error', 'Self-contradiction', \n",
    "#             'Contradiction', 'Grammatical error', \n",
    "#             'Ignore offer', 'Ignore proposal', \n",
    "#             'Lack of sociality', 'Lack of common sense',\n",
    "#             'Uninterpretable', 'Ignore greeting', \n",
    "#             'No-Err'\n",
    "#             ]\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n",
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "    # df = pre.read_json(path, datalist)\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_ginza_DCM_DIT_IRS.pickle\n",
      "model_ginza.pickle\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = pre.emb_size*2\n",
    "HIDDEN_DIM = pre.emb_size*2\n",
    "# OUTPUT_DIM = len(error_types)-1\n",
    "OUTPUT_DIM = 8\n",
    "# OUTPUT_DIM = 5\n",
    "seq_len = 3\n",
    "mode = \"ginza\"\n",
    "# mode = \"senBERT\"\n",
    "\n",
    "data_path = \"./X_y_data/seq{0}/\".format(seq_len)\n",
    "\n",
    "model_path = \"./models/seq{0}/\".format(seq_len)\n",
    "\n",
    "files = \"_\".join(datalist)\n",
    "data_name = \"data_{0}_{1}.pickle\".format(mode, files)\n",
    "model_name = \"model_{0}.pickle\".format(mode)\n",
    "print(data_name)\n",
    "print(model_name)\n",
    "\n",
    "modelM = DataManager(model_path)\n",
    "# modelM.is_exist(model_name)\n",
    "dataM = DataManager(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ./X_y_data/seq3/data_ginza_DCM_DIT_IRS.pickle\n"
     ]
    }
   ],
   "source": [
    "if dataM.is_exist(data_name):\n",
    "    \n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X_data = DATA_Xy[0]\n",
    "    y_data = DATA_Xy[1]\n",
    "else:\n",
    "    X_data, y_data = pre.extract_X_y(df, error_types, seq_len)\n",
    "    dataM.save_data(data_name, [X_data, y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data, y_data = pre.extract_X_y(df, error_types, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1, 23, 41, 943, "
     ]
    }
   ],
   "source": [
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 17)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 41\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = MultiLSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "multi_model.set_optimizer(lr_=0.01)\n",
    "multi_model.set_loss_func()\n",
    "multi_model.set_target_names(error_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error : Unclear intention \tstart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "ipykernel_launcher:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model[0] epoch :50 \t loss :0.0030111046235106187\n",
      "model[0] epoch :100 \t loss :0.00041632228339949506\n",
      "model[0] epoch :150 \t loss :0.00012157770254361822\n",
      "loss was under border(=0.0001) : train end\n",
      "\n",
      "error : Wrong information \tstart\n",
      "model[1] epoch :50 \t loss :0.010360683882026933\n",
      "model[1] epoch :100 \t loss :0.0015127691585803404\n",
      "model[1] epoch :150 \t loss :0.0004271040334060672\n",
      "model[1] epoch :200 \t loss :0.00017512895749405288\n",
      "loss was under border(=0.0001) : train end\n",
      "\n",
      "error : Ignore question \tstart\n",
      "model[2] epoch :50 \t loss :0.003688056831379072\n",
      "model[2] epoch :100 \t loss :0.0005925267759039343\n",
      "model[2] epoch :150 \t loss :0.000187842361242474\n",
      "loss was under border(=0.0001) : train end\n",
      "\n",
      "error : Topic transition error \tstart\n",
      "model[3] epoch :50 \t loss :0.0035956454848928843\n",
      "model[3] epoch :100 \t loss :0.00038503663472511107\n",
      "model[3] epoch :150 \t loss :0.0001102094017824129\n",
      "loss was under border(=0.0001) : train end\n",
      "\n",
      "error : Lack of information \tstart\n",
      "model[4] epoch :50 \t loss :0.0005309930138537311\n",
      "loss was under border(=0.0001) : train end\n",
      "\n",
      "error : Repetition \tstart\n",
      "model[5] epoch :50 \t loss :0.0018945574393001152\n",
      "model[5] epoch :100 \t loss :0.0002629114569572266\n",
      "loss was under border(=0.0001) : train end\n",
      "\n",
      "error : Contradiction \tstart\n",
      "model[6] epoch :50 \t loss :0.00019497893345032935\n",
      "loss was under border(=0.0001) : train end\n",
      "\n",
      "error : Self-contradiction \tstart\n",
      "loss was under border(=0.0001) : train end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 700\n",
    "loss_border = 0.0001\n",
    "multi_model.train(X_train, y_train, epoch, loss_border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 8)\n",
      "(405, 17)\n",
      "EM: 0.38271604938271603\n"
     ]
    }
   ],
   "source": [
    "y_pred = multi_model.predict(X_test, y_test)\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)\n",
    "print('EM:', metrics.accuracy_score(y_test[:,:OUTPUT_DIM], y_pred[:, :OUTPUT_DIM]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    pred_y = np.array(multi_model.multi_models[0](X_tensor).cpu()).argmax(axis=1).reshape(-1,1)\n",
    "    # print(pred_y[:10])\n",
    "    for i in range(1, multi_model.target_size):\n",
    "        model = multi_model.multi_models[i]\n",
    "        pred_y_each = np.array(model(X_tensor).cpu()).argmax(axis=1).reshape(-1,1)\n",
    "        pred_y = np.concatenate([pred_y, pred_y_each], 1)\n",
    "    print(pred_y[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405, 8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_test[:,:OUTPUT_DIM].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error[Unclear intention]  accuracy: 0.8666666666666667\n",
      "error[Wrong information]  accuracy: 0.6049382716049383\n",
      "error[Ignore question]  accuracy: 0.8641975308641975\n",
      "error[Topic transition error]  accuracy: 0.8518518518518519\n",
      "error[Lack of information]  accuracy: 0.9580246913580247\n",
      "error[Repetition]  accuracy: 0.9580246913580247\n",
      "error[Contradiction]  accuracy: 0.9876543209876543\n",
      "error[Self-contradiction]  accuracy: 0.9901234567901235\n"
     ]
    }
   ],
   "source": [
    "for i in range(OUTPUT_DIM):\n",
    "    print(\"error[{0}]  accuracy: {1}\".format(error_types[i],metrics.accuracy_score(y_test[:, i], y_pred[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ./models/seq3/model_ginza.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM.save_data(model_name, multi_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}