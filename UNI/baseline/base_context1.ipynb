{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = []\n",
    "utt_list = []\n",
    "errors = [\"Unclear intention\", \"Topic transition error\", \"Lack of information\"]\n",
    "# errors = [\"Repetition\", \"Self-contradiction\", \"Contradiction\"]\n",
    "# errors = [\"Repetition\", \"Self-contradiction\", \"Contradiction\"]\n",
    "for conv in convs:\n",
    "    # utt_list_conv = [\"\"]*5\n",
    "    utt_list_conv = []\n",
    "    for i, ut in enumerate( conv ) :\n",
    "        utt_list_conv.append(ut.utt)\n",
    "        # システム発話で，[文脈-形式]のエラー\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "        # if ut.is_system():\n",
    "            # usr_sys.append( [conv[i-1].utt, ut.utt] )\n",
    "            utt_list.append( utt_list_conv[-5:] )\n",
    "            if ut.is_error_included(errors):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1386] 2022-01-24 14:59:42,128 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/model.vec\n",
      "[1386] 2022-01-24 15:00:44,169 Info gensim.utils :KeyedVectors lifecycle event {'msg': 'loaded (351122, 300) matrix of type float32 from ../../corpus/w2v/model.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-01-24T15:00:44.168764', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "# fasttext\n",
    "# https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "w2v_name =  \"dep-ja-300dim\"\n",
    "w2v_name =  \"model.vec\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\"]\n",
    "add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "SYMBOL_w2v = dict(zip(add_keys, add_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/context_symbol.pickle\n"
     ]
    }
   ],
   "source": [
    "symbol_path = \"../models/base/\"\n",
    "symbol_name = \"context_symbol.pickle\"\n",
    "# symbol_name = \"context_symbol_content.pickle\"\n",
    "symbolM = DataManager(symbol_path)\n",
    "symbolM.save_data(symbol_name, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def w2v(word, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    # 形態素が登録されていたとき\n",
    "    \n",
    "    if word in SYMBOL_w2v:\n",
    "        vector = SYMBOL_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOL_w2v[\"[UNK]\"]\n",
    "    return torch.from_numpy(vector)\n",
    "\n",
    "def sentence2formated(sen, w2v_model, SYMBOL_w2v):\n",
    "    normal = sentence2morpheme(sen, sents_span=False)\n",
    "\n",
    "    # 1文だけ\n",
    "    if len(normal) < 2:\n",
    "        formated =  fill_SYMBOL_ONE(normal)[0]\n",
    "    else:\n",
    "        normal_sep = fill_SYMBOL_SEP(normal)\n",
    "        formated =  fill_SYMBOL_ONE( [sum( normal_sep, [] )] )[0]\n",
    "    \n",
    "    return  torch.stack( [w2v(w, w2v_model, SYMBOL_w2v) for w in formated] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\"aa\", \"bb\"] -> [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"]\n",
    "sentence2formated([\"aa\", \"bb\"],  w2v_model, SYMBOL_w2v ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "# def padding_vector(Xseq):\n",
    "#     Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "#     Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "#     Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "#     return Xseq\n",
    "\n",
    "\n",
    "def make_X(utt_list:list, w2v_model, SYMBOL_w2v):\n",
    "    utt_morp_list = []\n",
    "    for utt in tqdm( utt_list) :\n",
    "        # [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"] : 1データ\n",
    "        utt_morp = sentence2formated(utt, w2v_model, SYMBOL_w2v)\n",
    "        utt_morp_list.append(utt_morp)\n",
    "\n",
    "    X = rnn.pad_sequence(utt_morp_list, batch_first=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1349/1349 [00:38<00:00, 35.16it/s]\n"
     ]
    }
   ],
   "source": [
    "X_= make_X(utt_list, w2v_model, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y,  test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, w2v_model, SYMBOL_w2v):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # モデルを2つ定義\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.xtoy_2 = nn.Linear(embedding_dim*3 , hidden_dim)\n",
    "        self.y3toy = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "        self.w2v_model = w2v_model\n",
    "        self.SYMBOL_w2v = SYMBOL_w2v\n",
    "    \n",
    "    def pooling(self, A):\n",
    "        # A : dim3\n",
    "        # pooled = []\n",
    "        b_len = len(A)\n",
    "        f_len = len(A[0][0])\n",
    "        pooled = torch.zeros((b_len, f_len)).cuda()\n",
    "        for i, batch in enumerate( A ):\n",
    "            for j in range(f_len):\n",
    "                # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "                pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "        return pooled\n",
    "\n",
    "    def pooling_2(self, A):\n",
    "        # A : dim3\n",
    "        if len(A.shape) == 2:\n",
    "            A = torch.stack([A])\n",
    "        b_len = len(A)\n",
    "        seq_len = len(A[0])\n",
    "        m = nn.MaxPool1d(seq_len, stride=seq_len)\n",
    "        B = A.permute((0, 2, 1))\n",
    "        return m(B).reshape(b_len, -1)\n",
    "    \n",
    "    def forward(self, e):\n",
    "        \n",
    "        out, hc = self.bilstm(e)\n",
    "        x = torch.cat([ out, e], dim=2 )\n",
    "        y_2 = self.tanh( self.xtoy_2(x) )\n",
    "        y_3 = self.pooling_2(y_2)\n",
    "        y = self.softmax( self.y3toy(y_3) )\n",
    "        return y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 50\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, w2v_model, SYMBOL_w2v)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 50/50 [07:16<00:00,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 0.0013661015964316903\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "from tqdm import tqdm\n",
    "for epoch in tqdm( range(epoch_) ) :  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        # X_tens= data[0].to(torch.int).cuda()\n",
    "        # y_tens = data[1].to(torch.long).cuda()\n",
    "        X_tens= data[0].float().cuda()\n",
    "        y_tens = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        score = model(X_tens)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_tens)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[126  64]\n",
      " [ 48 167]]\n",
      "accuracy =  0.7234567901234568\n",
      "precision =  0.7229437229437229\n",
      "recall =  0.7767441860465116\n",
      "f1 score =  0.7488789237668162\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 矛盾，繰り返し\n",
    "    - 誤検出がやっぱり低い\n",
    "        \n",
    "            confusion matrix = \n",
    "            [[376   6]\n",
    "            [ 21   2]]\n",
    "            accuracy =  0.9333333333333333\n",
    "            precision =  0.25\n",
    "            recall =  0.08695652173913043\n",
    "            f1 score =  0.12903225806451613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/context_form.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/base/\"\n",
    "model_name = \"context_form.pickle\"\n",
    "# model_name = \"context_content.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(60).view(5, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(A):\n",
    "    # A : dim3\n",
    "    # pooled = []\n",
    "    b_len = len(A)\n",
    "    f_len = len(A[0][0])\n",
    "    pooled = torch.zeros((b_len, f_len))\n",
    "    for i, batch in enumerate( A ):\n",
    "        for j in range(f_len):\n",
    "            # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "            pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(8), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = []\n",
    "f_len = len(A[0][0])\n",
    "for i, batch in enumerate( A ):\n",
    "    batch_pooled = []\n",
    "    for j in range(f_len):\n",
    "        batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "    # pooled.append(torch.stack(batch_pooled))\n",
    "    pooled.append(batch_pooled)\n",
    "A_ = pooled \n",
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0][2][0] = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [30,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26, 27],\n",
       "         [28, 29, 30, 31],\n",
       "         [32, 33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50, 51],\n",
       "         [52, 53, 54, 55],\n",
       "         [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = torch.tensor(A_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1][2][0] = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30,  9, 10, 11],\n",
       "        [20, 21, 22, 23],\n",
       "        [32, 33, 34, 35],\n",
       "        [44, 45, 46, 47],\n",
       "        [56, 57, 58, 59]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(60), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
