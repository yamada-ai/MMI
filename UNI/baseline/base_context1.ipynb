{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = []\n",
    "utt_list = []\n",
    "errors = [\"Unclear intention\", \"Topic transition error\", \"Lack of information\"]\n",
    "for conv in convs:\n",
    "    # utt_list_conv = [\"\"]*5\n",
    "    utt_list_conv = []\n",
    "    for i, ut in enumerate( conv ) :\n",
    "        utt_list_conv.append(ut.utt)\n",
    "        # システム発話で，[文脈-形式]のエラー\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "        # if ut.is_system():\n",
    "            # usr_sys.append( [conv[i-1].utt, ut.utt] )\n",
    "            utt_list.append( utt_list_conv[-5:] )\n",
    "            if ut.is_error_included(errors):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "# fasttext\n",
    "# https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "w2v_name =  \"dep-ja-300dim\"\n",
    "w2v_name =  \"model.vec\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\"]\n",
    "add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "SYMBOL_w2v = dict(zip(add_keys, add_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/context_symbol.pickle\n"
     ]
    }
   ],
   "source": [
    "symbol_path = \"../models/base/\"\n",
    "symbol_name = \"context_symbol.pickle\"\n",
    "symbolM = DataManager(symbol_path)\n",
    "symbolM.save_data(symbol_name, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def w2v(word, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    # 形態素が登録されていたとき\n",
    "    \n",
    "    if word in SYMBOL_w2v:\n",
    "        vector = SYMBOL_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOL_w2v[\"[UNK]\"]\n",
    "    return torch.from_numpy(vector)\n",
    "\n",
    "def sentence2formated(sen, w2v_model, SYMBOL_w2v):\n",
    "    normal = sentence2morpheme(sen, sents_span=False)\n",
    "\n",
    "    # 1文だけ\n",
    "    if len(normal) < 2:\n",
    "        formated =  fill_SYMBOL_ONE(normal)[0]\n",
    "    else:\n",
    "        normal_sep = fill_SYMBOL_SEP(normal)\n",
    "        formated =  fill_SYMBOL_ONE( [sum( normal_sep, [] )] )[0]\n",
    "    \n",
    "    return  torch.stack( [w2v(w, w2v_model, SYMBOL_w2v) for w in formated] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\"aa\", \"bb\"] -> [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"]\n",
    "sentence2formated([\"aa\", \"bb\"],  w2v_model, SYMBOL_w2v ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "# def padding_vector(Xseq):\n",
    "#     Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "#     Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "#     Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "#     return Xseq\n",
    "\n",
    "\n",
    "def make_X(utt_list:list, w2v_model, SYMBOL_w2v):\n",
    "    utt_morp_list = []\n",
    "    for utt in tqdm( utt_list) :\n",
    "        # [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"] : 1データ\n",
    "        utt_morp = sentence2formated(utt, w2v_model, SYMBOL_w2v)\n",
    "        utt_morp_list.append(utt_morp)\n",
    "\n",
    "    X = rnn.pad_sequence(utt_morp_list, batch_first=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [01:39<00:00,  9.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X_= make_X(utt_list, w2v_model, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y,  test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, w2v_model, SYMBOL_w2v):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # モデルを2つ定義\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.xtoy_2 = nn.Linear(embedding_dim*3 , hidden_dim)\n",
    "        self.y3toy = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "        self.w2v_model = w2v_model\n",
    "        self.SYMBOL_w2v = SYMBOL_w2v\n",
    "    \n",
    "    def pooling(self, A):\n",
    "        # A : dim3\n",
    "        # pooled = []\n",
    "        b_len = len(A)\n",
    "        f_len = len(A[0][0])\n",
    "        pooled = torch.zeros((b_len, f_len)).cuda()\n",
    "        for i, batch in enumerate( A ):\n",
    "            for j in range(f_len):\n",
    "                # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "                pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "        return pooled\n",
    "\n",
    "    def pooling_2(self, A):\n",
    "        # A : dim3\n",
    "        if len(A.shape) == 2:\n",
    "            A = torch.stack([A])\n",
    "        b_len = len(A)\n",
    "        seq_len = len(A[0])\n",
    "        m = nn.MaxPool1d(seq_len, stride=seq_len)\n",
    "        B = A.permute((0, 2, 1))\n",
    "        return m(B).reshape(b_len, -1)\n",
    "    \n",
    "    def forward(self, e):\n",
    "        \n",
    "        out, hc = self.bilstm(e)\n",
    "        x = torch.cat([ out, e], dim=2 )\n",
    "        y_2 = self.tanh( self.xtoy_2(x) )\n",
    "        y_3 = self.pooling_2(y_2)\n",
    "        y = self.softmax( self.y3toy(y_3) )\n",
    "        return y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 150\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, w2v_model, SYMBOL_w2v)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 33%|███▎      | 50/150 [03:13<06:38,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 0.028828886861447245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 100/150 [06:31<03:15,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 \t loss 0.0007466417991963681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [09:51<00:00,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150 \t loss 0.00015119953923203866\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "from tqdm import tqdm\n",
    "for epoch in tqdm( range(epoch_) ) :  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        # X_tens= data[0].to(torch.int).cuda()\n",
    "        # y_tens = data[1].to(torch.long).cuda()\n",
    "        X_tens= data[0].float().cuda()\n",
    "        y_tens = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        score = model(X_tens)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_tens)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[ 49  57]\n",
      " [ 33 159]]\n",
      "accuracy =  0.697986577181208\n",
      "precision =  0.7361111111111112\n",
      "recall =  0.828125\n",
      "f1 score =  0.7794117647058824\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/context_form.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/base/\"\n",
    "model_name = \"context_form.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(60).view(5, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(A):\n",
    "    # A : dim3\n",
    "    # pooled = []\n",
    "    b_len = len(A)\n",
    "    f_len = len(A[0][0])\n",
    "    pooled = torch.zeros((b_len, f_len))\n",
    "    for i, batch in enumerate( A ):\n",
    "        for j in range(f_len):\n",
    "            # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "            pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(8), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = []\n",
    "f_len = len(A[0][0])\n",
    "for i, batch in enumerate( A ):\n",
    "    batch_pooled = []\n",
    "    for j in range(f_len):\n",
    "        batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "    # pooled.append(torch.stack(batch_pooled))\n",
    "    pooled.append(batch_pooled)\n",
    "A_ = pooled \n",
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0][2][0] = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [30,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26, 27],\n",
       "         [28, 29, 30, 31],\n",
       "         [32, 33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50, 51],\n",
       "         [52, 53, 54, 55],\n",
       "         [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = torch.tensor(A_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1][2][0] = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30,  9, 10, 11],\n",
       "        [20, 21, 22, 23],\n",
       "        [32, 33, 34, 35],\n",
       "        [44, 45, 46, 47],\n",
       "        [56, 57, 58, 59]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(60), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
