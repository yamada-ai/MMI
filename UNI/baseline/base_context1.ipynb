{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = []\n",
    "utt_list = []\n",
    "errors = [\"Unclear intention\", \"Topic transition error\", \"Lack of information\"]\n",
    "for conv in convs:\n",
    "    # utt_list_conv = [\"\"]*5\n",
    "    utt_list_conv = []\n",
    "    for i, ut in enumerate( conv ) :\n",
    "        utt_list_conv.append(ut.utt)\n",
    "        # システム発話で，[文脈-形式]のエラー\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "        # if ut.is_system():\n",
    "            # usr_sys.append( [conv[i-1].utt, ut.utt] )\n",
    "            utt_list.append( utt_list_conv[-5:] )\n",
    "            if ut.is_error_included(errors):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28400] 2021-12-15 16:22:03,214 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/dep-ja-300dim\n",
      "[28400] 2021-12-15 16:22:19,580 Info gensim.utils :KeyedVectors lifecycle event {'msg': 'loaded (93297, 300) matrix of type float32 from ../../corpus/w2v/dep-ja-300dim', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-12-15T16:22:19.579992', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+ \"dep-ja-300dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\"]\n",
    "add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "SYMBOl_w2v = dict(zip(add_keys, add_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def w2v(word, w2v_model:KeyedVectors, SYMBOl_w2v:dict):\n",
    "    # 形態素が登録されていたとき\n",
    "    \n",
    "    if word in SYMBOl_w2v:\n",
    "        vector = SYMBOl_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOl_w2v[\"[UNK]\"]\n",
    "    return torch.from_numpy(vector)\n",
    "\n",
    "def sentence2formated(sen, w2v_model, SYMBOl_w2v):\n",
    "    normal = sentence2morpheme(sen, sents_span=False)\n",
    "\n",
    "    # 1文だけ\n",
    "    if len(normal) < 2:\n",
    "        formated =  fill_SYMBOL_ONE(normal)[0]\n",
    "    else:\n",
    "        normal_sep = fill_SYMBOL_SEP(normal)\n",
    "        formated =  fill_SYMBOL_ONE( [sum( normal_sep, [] )] )[0]\n",
    "    \n",
    "    return  torch.stack( [w2v(w, w2v_model, SYMBOl_w2v) for w in formated] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\"aa\", \"bb\"] -> [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"]\n",
    "sentence2formated([\"aa\", \"bb\"],  w2v_model, SYMBOl_w2v ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "# def padding_vector(Xseq):\n",
    "#     Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "#     Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "#     Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "#     return Xseq\n",
    "\n",
    "\n",
    "def make_X(utt_list:list, w2v_model, SYMBOl_w2v):\n",
    "    utt_morp_list = []\n",
    "    for utt in tqdm( utt_list) :\n",
    "        # [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"] : 1データ\n",
    "        utt_morp = sentence2formated(utt, w2v_model, SYMBOl_w2v)\n",
    "        utt_morp_list.append(utt_morp)\n",
    "\n",
    "    X = rnn.pad_sequence(utt_morp_list, batch_first=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1349/1349 [00:36<00:00, 36.97it/s]\n"
     ]
    }
   ],
   "source": [
    "X_= make_X(utt_list, w2v_model, SYMBOl_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y,  test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, w2v_model, SYMBOl_w2v):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # モデルを2つ定義\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.xtoy_2 = nn.Linear(embedding_dim*3 , hidden_dim)\n",
    "        self.y3toy = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "        self.w2v_model = w2v_model\n",
    "        self.SYMBOL_w2v = SYMBOl_w2v\n",
    "    \n",
    "    def pooling(self, A):\n",
    "        # A : dim3\n",
    "        # pooled = []\n",
    "        b_len = len(A)\n",
    "        f_len = len(A[0][0])\n",
    "        pooled = torch.zeros((b_len, f_len)).cuda()\n",
    "        for i, batch in enumerate( A ):\n",
    "            for j in range(f_len):\n",
    "                # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "                pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, e):\n",
    "        \n",
    "        out, hc = self.bilstm(e)\n",
    "        x = torch.cat([ out, e], dim=2 )\n",
    "        y_2 = self.tanh( self.xtoy_2(x) )\n",
    "        y_3 = self.pooling(y_2)\n",
    "        y = self.softmax( self.y3toy(y_3) )\n",
    "        return y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 300\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, w2v_model, SYMBOl_w2v)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  9%|▉         | 27/300 [6:09:30<62:16:08, 821.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-12ca69434fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_tens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0da83c437269>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0my_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxtoy_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0my_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my3toy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0da83c437269>\u001b[0m in \u001b[0;36mpooling\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mpooled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpooled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "from tqdm import tqdm\n",
    "for epoch in tqdm( range(epoch_) ) :  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        # X_tens= data[0].to(torch.int).cuda()\n",
    "        # y_tens = data[1].to(torch.long).cuda()\n",
    "        X_tens= data[0].float().cuda()\n",
    "        y_tens = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        score = model(X_tens)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_tens)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[118  72]\n",
      " [ 41 174]]\n",
      "accuracy =  0.7209876543209877\n",
      "precision =  0.7073170731707317\n",
      "recall =  0.8093023255813954\n",
      "f1 score =  0.754880694143167\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/context_form.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/base/\"\n",
    "model_name = \"context_form.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(60).view(5, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(A):\n",
    "    # A : dim3\n",
    "    # pooled = []\n",
    "    b_len = len(A)\n",
    "    f_len = len(A[0][0])\n",
    "    pooled = torch.zeros((b_len, f_len))\n",
    "    for i, batch in enumerate( A ):\n",
    "        for j in range(f_len):\n",
    "            # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "            pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26, 27],\n",
       "         [28, 29, 30, 31],\n",
       "         [32, 33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50, 51],\n",
       "         [52, 53, 54, 55],\n",
       "         [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.,  9., 10., 11.],\n",
       "        [20., 21., 22., 23.],\n",
       "        [32., 33., 34., 35.],\n",
       "        [44., 45., 46., 47.],\n",
       "        [56., 57., 58., 59.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(8), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = []\n",
    "f_len = len(A[0][0])\n",
    "for i, batch in enumerate( A ):\n",
    "    batch_pooled = []\n",
    "    for j in range(f_len):\n",
    "        batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "    # pooled.append(torch.stack(batch_pooled))\n",
    "    pooled.append(batch_pooled)\n",
    "A_ = pooled \n",
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0][2][0] = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [30,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26, 27],\n",
       "         [28, 29, 30, 31],\n",
       "         [32, 33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50, 51],\n",
       "         [52, 53, 54, 55],\n",
       "         [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = torch.tensor(A_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1][2][0] = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30,  9, 10, 11],\n",
       "        [20, 21, 22, 23],\n",
       "        [32, 33, 34, 35],\n",
       "        [44, 45, 46, 47],\n",
       "        [56, 57, 58, 59]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(60), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
