{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = []\n",
    "utt_list = []\n",
    "errors = [\"Unclear intention\", \"Topic transition error\", \"Lack of information\"]\n",
    "# errors = [\"Repetition\", \"Self-contradiction\", \"Contradiction\"]\n",
    "# errors = [\"Repetition\", \"Self-contradiction\", \"Contradiction\"]\n",
    "for conv in convs:\n",
    "    # utt_list_conv = [\"\"]*5\n",
    "    utt_list_conv = []\n",
    "    for i, ut in enumerate( conv ) :\n",
    "        utt_list_conv.append(ut.utt)\n",
    "        # システム発話で，[文脈-形式]のエラー\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "        # if ut.is_system():\n",
    "            # usr_sys.append( [conv[i-1].utt, ut.utt] )\n",
    "            utt_list.append( utt_list_conv[-5:] )\n",
    "            if ut.is_error_included(errors):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1386] 2022-01-24 14:59:42,128 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/model.vec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "# fasttext\n",
    "# https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "w2v_name =  \"dep-ja-300dim\"\n",
    "w2v_name =  \"model.vec\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\"]\n",
    "add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "SYMBOL_w2v = dict(zip(add_keys, add_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/context_symbol_content.pickle\n"
     ]
    }
   ],
   "source": [
    "symbol_path = \"../models/base/\"\n",
    "symbol_name = \"context_symbol.pickle\"\n",
    "# symbol_name = \"context_symbol_content.pickle\"\n",
    "symbolM = DataManager(symbol_path)\n",
    "symbolM.save_data(symbol_name, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def w2v(word, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    # 形態素が登録されていたとき\n",
    "    \n",
    "    if word in SYMBOL_w2v:\n",
    "        vector = SYMBOL_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOL_w2v[\"[UNK]\"]\n",
    "    return torch.from_numpy(vector)\n",
    "\n",
    "def sentence2formated(sen, w2v_model, SYMBOL_w2v):\n",
    "    normal = sentence2morpheme(sen, sents_span=False)\n",
    "\n",
    "    # 1文だけ\n",
    "    if len(normal) < 2:\n",
    "        formated =  fill_SYMBOL_ONE(normal)[0]\n",
    "    else:\n",
    "        normal_sep = fill_SYMBOL_SEP(normal)\n",
    "        formated =  fill_SYMBOL_ONE( [sum( normal_sep, [] )] )[0]\n",
    "    \n",
    "    return  torch.stack( [w2v(w, w2v_model, SYMBOL_w2v) for w in formated] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\"aa\", \"bb\"] -> [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"]\n",
    "sentence2formated([\"aa\", \"bb\"],  w2v_model, SYMBOL_w2v ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "# def padding_vector(Xseq):\n",
    "#     Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "#     Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "#     Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "#     return Xseq\n",
    "\n",
    "\n",
    "def make_X(utt_list:list, w2v_model, SYMBOL_w2v):\n",
    "    utt_morp_list = []\n",
    "    for utt in tqdm( utt_list) :\n",
    "        # [\"FOS\", \"aa\", \"[SEP]\", \"bb\", \"EOS\"] : 1データ\n",
    "        utt_morp = sentence2formated(utt, w2v_model, SYMBOL_w2v)\n",
    "        utt_morp_list.append(utt_morp)\n",
    "\n",
    "    X = rnn.pad_sequence(utt_morp_list, batch_first=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1349/1349 [00:36<00:00, 37.35it/s]\n"
     ]
    }
   ],
   "source": [
    "X_= make_X(utt_list, w2v_model, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y,  test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, w2v_model, SYMBOL_w2v):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # モデルを2つ定義\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.xtoy_2 = nn.Linear(embedding_dim*3 , hidden_dim)\n",
    "        self.y3toy = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "        self.w2v_model = w2v_model\n",
    "        self.SYMBOL_w2v = SYMBOL_w2v\n",
    "    \n",
    "    def pooling(self, A):\n",
    "        # A : dim3\n",
    "        # pooled = []\n",
    "        b_len = len(A)\n",
    "        f_len = len(A[0][0])\n",
    "        pooled = torch.zeros((b_len, f_len)).cuda()\n",
    "        for i, batch in enumerate( A ):\n",
    "            for j in range(f_len):\n",
    "                # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "                pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "        return pooled\n",
    "\n",
    "    def pooling_2(self, A):\n",
    "        # A : dim3\n",
    "        if len(A.shape) == 2:\n",
    "            A = torch.stack([A])\n",
    "        b_len = len(A)\n",
    "        seq_len = len(A[0])\n",
    "        m = nn.MaxPool1d(seq_len, stride=seq_len)\n",
    "        B = A.permute((0, 2, 1))\n",
    "        return m(B).reshape(b_len, -1)\n",
    "    \n",
    "    def forward(self, e):\n",
    "        \n",
    "        out, hc = self.bilstm(e)\n",
    "        x = torch.cat([ out, e], dim=2 )\n",
    "        y_2 = self.tanh( self.xtoy_2(x) )\n",
    "        y_3 = self.pooling_2(y_2)\n",
    "        y = self.softmax( self.y3toy(y_3) )\n",
    "        return y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 150\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, w2v_model, SYMBOL_w2v)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 33%|███▎      | 50/150 [06:31<12:45,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 0.0003904735918922597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 100/150 [13:02<06:32,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 \t loss 5.929818937033815e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [19:32<00:00,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150 \t loss 0.002048884025086295\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "from tqdm import tqdm\n",
    "for epoch in tqdm( range(epoch_) ) :  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        # X_tens= data[0].to(torch.int).cuda()\n",
    "        # y_tens = data[1].to(torch.long).cuda()\n",
    "        X_tens= data[0].float().cuda()\n",
    "        y_tens = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        score = model(X_tens)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_tens)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[376   6]\n",
      " [ 21   2]]\n",
      "accuracy =  0.9333333333333333\n",
      "precision =  0.25\n",
      "recall =  0.08695652173913043\n",
      "f1 score =  0.12903225806451613\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 矛盾，繰り返し\n",
    "    - 誤検出がやっぱり低い\n",
    "        \n",
    "            confusion matrix = \n",
    "            [[376   6]\n",
    "            [ 21   2]]\n",
    "            accuracy =  0.9333333333333333\n",
    "            precision =  0.25\n",
    "            recall =  0.08695652173913043\n",
    "            f1 score =  0.12903225806451613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/context_content.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/base/\"\n",
    "model_name = \"context_form.pickle\"\n",
    "# model_name = \"context_content.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(60).view(5, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(A):\n",
    "    # A : dim3\n",
    "    # pooled = []\n",
    "    b_len = len(A)\n",
    "    f_len = len(A[0][0])\n",
    "    pooled = torch.zeros((b_len, f_len))\n",
    "    for i, batch in enumerate( A ):\n",
    "        for j in range(f_len):\n",
    "            # batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "            pooled[i, j] = A[i, torch.argmax(A[i, :, j]), j]\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(8), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = []\n",
    "f_len = len(A[0][0])\n",
    "for i, batch in enumerate( A ):\n",
    "    batch_pooled = []\n",
    "    for j in range(f_len):\n",
    "        batch_pooled.append( A[i, torch.argmax(A[i, :, j]), j] )\n",
    "    # pooled.append(torch.stack(batch_pooled))\n",
    "    pooled.append(batch_pooled)\n",
    "A_ = pooled \n",
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0][2][0] = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [30,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]],\n",
       "\n",
       "        [[24, 25, 26, 27],\n",
       "         [28, 29, 30, 31],\n",
       "         [32, 33, 34, 35]],\n",
       "\n",
       "        [[36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50, 51],\n",
       "         [52, 53, 54, 55],\n",
       "         [56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(20), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = torch.tensor(A_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1][2][0] = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30,  9, 10, 11],\n",
       "        [20, 21, 22, 23],\n",
       "        [32, 33, 34, 35],\n",
       "        [44, 45, 46, 47],\n",
       "        [56, 57, 58, 59]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(30), tensor(9), tensor(10), tensor(11)],\n",
       " [tensor(60), tensor(21), tensor(22), tensor(23)],\n",
       " [tensor(32), tensor(33), tensor(34), tensor(35)],\n",
       " [tensor(44), tensor(45), tensor(46), tensor(47)],\n",
       " [tensor(56), tensor(57), tensor(58), tensor(59)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
