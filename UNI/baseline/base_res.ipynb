{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../corpus/func_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_sys = []\n",
    "y = []\n",
    "utt_list = []\n",
    "errors = [\"Ignore question\", \"Ignore offer\", \"Ignore proposal\", \"Ignore greeting\"]\n",
    "for conv in convs:\n",
    "    for i, ut in enumerate( conv ) :\n",
    "        utt_list.append(ut.utt)\n",
    "        # システム発話で，無視系統のエラー\n",
    "        # if ut.is_system() and ut.is_exist_error():\n",
    "        if ut.is_system():\n",
    "        # if ut.is_system():\n",
    "            usr_sys.append( [conv[i-1].utt, ut.utt] )\n",
    "            if ut.is_error_included(errors):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_vocab_dict(text:str):\n",
    "    vocab_dict = dict()\n",
    "    doc = nlp(text)\n",
    "    print(\"analyzed vocab text\")\n",
    "    vocab_dict[\"[PAD]\"] = 0\n",
    "    for token in tqdm(doc):\n",
    "        # 表層のまま登録してやるぜ\n",
    "        key = token.orth_\n",
    "        if key not in vocab_dict:\n",
    "            vocab_dict[key] = len(vocab_dict)\n",
    "\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    return vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_text = \" \".join(utt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62812/62812 [00:00<00:00, 1700912.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzed vocab text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = make_vocab_dict(vocab_text)\n",
    "vocab_size = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"../X_y_data/base/\"\n",
    "vocab_name = \"vocab_dict.pickle\"\n",
    "vocabM = DataManager(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/base/vocab_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "vocabM.save_data(vocab_name, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/base/vocab_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "vocab_path = \"../X_y_data/base/\"\n",
    "vocab_name = \"vocab_dict.pickle\"\n",
    "vocabM = DataManager(vocab_path)\n",
    "vocab_dict = vocabM.load_data(vocab_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../X_y_data/base/\"\n",
    "data_name = \"response_Xy_ver{0}.pickle\".format(1)\n",
    "dataM = DataManager(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2ids(sentence:str, vocab_dict:dict):\n",
    "    doc = nlp(sentence)\n",
    "    ids = np.zeros(len(doc))\n",
    "    for i, token in enumerate(doc):\n",
    "        key = token.orth_\n",
    "        if key in vocab_dict:\n",
    "            ids[i] = vocab_dict[key]\n",
    "        else:\n",
    "            ids[i] = vocab_dict[\"[UNK]\"]\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "def padding_vector(Xseq):\n",
    "    Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "    Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "    Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "    return Xseq\n",
    "\n",
    "\n",
    "def make_X(usr_sys:list, vocab_dict:dict):\n",
    "    usr_id_list = []\n",
    "    sys_id_list = []\n",
    "    for turn in tqdm( usr_sys ) :\n",
    "        usr_id = sentence2ids(turn[0], vocab_dict)\n",
    "        usr_id_list.append(usr_id)\n",
    "\n",
    "        sys_id = sentence2ids(turn[1], vocab_dict)\n",
    "        sys_id_list.append(sys_id)\n",
    "    \n",
    "    # usr_id_pad = rnn.pad_sequence(torch.Tensor( usr_id_list) , batch_first=True)\n",
    "    # sys_id_pad = rnn.pad_sequence(torch.Tensor( sys_id_list), batch_first=True)\n",
    "    usr_id_pad = padding_vector(usr_id_list)\n",
    "    sys_id_pad = padding_vector(sys_id_list)\n",
    "\n",
    "    usr_pad_len = len(usr_id_pad[0])\n",
    "    sys_pad_len = len(sys_id_pad[0])\n",
    "    # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "    # print(usr_pad_len, sys_pad_len)\n",
    "    X = torch.zeros( (len(usr_sys), usr_pad_len+sys_pad_len) )\n",
    "    for i, (u, s) in enumerate( zip(usr_id_pad, sys_id_pad) ):\n",
    "        # print(i, u, s)\n",
    "        X[i, :usr_pad_len] = u\n",
    "        X[i, usr_pad_len: usr_pad_len+sys_pad_len] = s\n",
    "    return X, usr_pad_len, sys_pad_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1423/1423 [00:32<00:00, 43.94it/s]\n"
     ]
    }
   ],
   "source": [
    "X, upl, spl = make_X(usr_sys, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataM.save_data(data_name, [X, y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim , tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, x, upl, spl):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "\n",
    "        # x : [seq]\n",
    "        usr_ = x[:, :upl]\n",
    "        sys_ = x[:, upl:upl+spl]\n",
    "        emb1 = self.word_embeddings(usr_)\n",
    "        emb2 = self.word_embeddings(sys_)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        _, lstm2_out = self.lstm1(emb2)\n",
    "        # print(hidden_layer)\n",
    "        # bilstm_out = torch.cat([lstm_out[0][0], lstm_out[0][1]], dim=1)\n",
    "        \n",
    "        usr_vec = ( lstm1_out[0][0] + lstm1_out[0][1] )/2 \n",
    "        sys_vec = ( lstm2_out[0][0] + lstm2_out[0][1] )/2\n",
    "\n",
    "        # print(usr_vec.shape, sys_vec.shape)\n",
    "        # print(torch.cat([ usr_vec, sys_vec], dim=1).shape)\n",
    "        tag_space = self.hidden2tag(torch.cat([ usr_vec, sys_vec], dim=1 ))\n",
    "        \n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        # y = self.hidden2tag(bilstm_out)\n",
    "        y =self.softmax(tag_space)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 200\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab_dict)\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-4c1602970ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-4df756d1c41b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, upl, spl)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0memb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm2_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# print(hidden_layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# bilstm_out = torch.cat([lstm_out[0][0], lstm_out[0][1]], dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 662\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        X_t_tensor = data[0].to(torch.int).cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\n",
    "\n",
    "        score = model(X_t_tensor, upl, spl)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0', dtype=torch.int)\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor, upl, spl).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[210  20]\n",
      " [ 10  45]]\n",
      "accuracy =  0.8947368421052632\n",
      "precision =  0.6923076923076923\n",
      "recall =  0.8181818181818182\n",
      "f1 score =  0.7500000000000001\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/base/\"\n",
    "model_name = \"responce_form.pickle\"\n",
    "modelM = DataManager(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/base/responce_form.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
