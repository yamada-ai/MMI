{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from tools import preproc\n",
    "\n",
    "from tools.maneger import DataManager\n",
    "import spacy\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pre = preproc.Preprocessor()\n",
    "nlp = spacy.load('ja_ginza')\n",
    "def div2sentence(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        sentence_list = [str(s)  for s in doc.sents]\n",
    "\n",
    "    elif isinstance(text, list):\n",
    "        sentence_list  = []\n",
    "        docs = list(nlp.pipe(text, disable=['ner']))\n",
    "            # return [ self.get_POS(sen_) for sen_ in sen]\n",
    "        for doc in docs:\n",
    "            sentence_list.extend( [str(s) for s in doc.sents] )\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return sentence_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token_set(texts):\n",
    "    token_set = set()\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            token_set.add(token.text)\n",
    "    return token_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ave_length(texts):\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    ave_length = 0\n",
    "    for doc in docs:\n",
    "        ave_length += len(doc)\n",
    "    ave_length = int(ave_length/len(docs)) + 1\n",
    "    # ave_length = ave_length/len(doc)\n",
    "    return ave_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_sentence(token_list, length):\n",
    "    samples = random.choices(token_list, k=length)\n",
    "    return \"\".join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: /home/yamada/Documents/MMI/UNI\n"
     ]
    }
   ],
   "source": [
    "root = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "print(\"root:\", root)\n",
    "path = root + '/error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./X_y_data/dialogue/\n",
      "data_DNN.pickle\n"
     ]
    }
   ],
   "source": [
    "vec_mode = \"ginza\"\n",
    "data_mode_list = [\"dialogue\", \"wiki\"]\n",
    "data_mode = data_mode_list[0]\n",
    "data_path = \"./X_y_data/{0}/\".format(data_mode)\n",
    "dataM = DataManager(data_path)\n",
    "print(data_path)\n",
    "way = [\"LR\", \"DNN\"]\n",
    "data_name = \"data_{0}.pickle\".format(way[1])\n",
    "print(data_name)\n",
    "\n",
    "model_path = \"./models/{0}/\".format(data_mode)\n",
    "modelM = DataManager(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_list = []\n",
    "for d, u, s, ec in zip(df.did, df.usr, df.sys, df.ec):\n",
    "    usr_list.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = copy.deepcopy(usr_list[:20])\n",
    "text = div2sentence(text)\n",
    "# pprint.pprint(text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['そう', 'です', 'ね', '。'],\n",
       " ['名詞-普通名詞-副詞可能', 'とても', '暑い', 'です', 'から', '。'],\n",
       " ['名詞-普通名詞-副詞可能', 'に', '行き', 'たい', 'と', '思い', 'ます', '。'],\n",
       " ['はい', '。'],\n",
       " ['代名詞', 'は', '名詞-普通名詞-一般', 'に', '行き', 'ます', 'か', '？'],\n",
       " ['何故', 'です', 'か', '？'],\n",
       " ['そう', 'です', 'か', '。'],\n",
       " ['名詞-普通名詞-一般', 'に', '行く', 'の', 'は', 'どう', 'でしょう', 'か', '？'],\n",
       " ['名詞-普通名詞-一般', 'は', 'お', '好き', 'な', 'の', 'です', 'か', '？'],\n",
       " ['名詞-普通名詞-一般',\n",
       "  'の',\n",
       "  '名詞-普通名詞-サ変可能',\n",
       "  'の',\n",
       "  '名詞-普通名詞-一般',\n",
       "  'は',\n",
       "  '名詞-普通名詞-一般',\n",
       "  'です',\n",
       "  'か',\n",
       "  '？'],\n",
       " ['代名詞', 'に', '行く', 'と', 'いい', 'です', 'か', '？'],\n",
       " ['名詞-普通名詞-副詞可能', 'は', 'とても', '暑く', 'なる', 'みたい', 'です', 'ね', '。'],\n",
       " ['涼しく',\n",
       "  'なっ',\n",
       "  'て',\n",
       "  'き',\n",
       "  'たら',\n",
       "  '、',\n",
       "  '名詞-普通名詞-サ変可能',\n",
       "  'に',\n",
       "  '名詞-普通名詞-一般',\n",
       "  'へ',\n",
       "  '行き',\n",
       "  'たい',\n",
       "  'です',\n",
       "  'ね',\n",
       "  '。'],\n",
       " ['美味しい', 'です', 'ね', '。'],\n",
       " ['で', 'も', '高い', 'です', 'ね'],\n",
       " ['名詞-数詞', '名詞-普通名詞-助数詞可能', 'ぐらい', 'か', 'な', '？'],\n",
       " ['名詞-普通名詞-サ変可能', 'が', '分かり', 'ませ', 'ん'],\n",
       " ['名詞-普通名詞-一般', 'が', '好き', 'です'],\n",
       " ['名詞-普通名詞-一般', 'の', '名詞-普通名詞-一般', 'は', '？'],\n",
       " ['好き', 'な', '名詞-普通名詞-一般', '接尾辞-名詞的-一般', 'は', 'い', 'ます', 'か', '？'],\n",
       " ['そう', 'です', 'か'],\n",
       " ['名詞-普通名詞-副詞可能', 'に', '好き', 'な', '名詞-普通名詞-サ変可能', 'は', '？'],\n",
       " ['名詞-普通名詞-一般', 'に', '行き', 'ます', 'か', '？']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec モデル作成\n",
    "from gensim.models import word2vec\n",
    "normalized_list = pre.noun2normal(text)\n",
    "normalized_text = []\n",
    "for nor in normalized_list:\n",
    "    normalized_text.extend(nor)\n",
    "normalized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = word2vec.Word2Vec(normalized_text, vector_size=128, min_count=1, window=5, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "word = \"あ\"\n",
    "if word in wv_model.wv:\n",
    "    wv_model.wv[word]\n",
    "else:\n",
    "    print(\"no\")\n",
    "    print(wv_model.predict_output_word(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}