{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "# from datatools.analyzer import clean_text\n",
    "from error_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm        import Vocabulary\n",
    "# from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.util      import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of text_list : 18674\n"
     ]
    }
   ],
   "source": [
    "def read_file(filename, batch_size=10000) -> list:\n",
    "    f = open(filename, 'r', encoding='UTF-8')\n",
    "    text_list = [  ]\n",
    "    concated = \"\"\n",
    "    for i, line in enumerate( f.readlines() ):\n",
    "        concated += line.rstrip('\\n')\n",
    "        if (i+1) % batch_size == 0:\n",
    "            text_list.append(concated)\n",
    "            concated = \"\"\n",
    "    text_list.append(concated)\n",
    "    f.close()\n",
    "    return text_list\n",
    "\n",
    "filename = \"../../corpus/wiki/wiki_40b_train_normal.txt\"\n",
    "all_list = read_file(filename, batch_size=100)\n",
    "print(\"len of text_list :\", len(all_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18674/18674 [00:03<00:00, 5287.78it/s]\n"
     ]
    }
   ],
   "source": [
    "all_wiki = []\n",
    "for line in tqdm(all_list):\n",
    "    all_wiki += [sen+\"。\" for sen in line.split(\"。\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_utt = all_wiki[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123490"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)\n",
    "conv_utt = []\n",
    "for conv in convs:\n",
    "    for i, ut in enumerate(conv):\n",
    "        if not ut.is_exist_error():\n",
    "            conv_utt.append(clean_text(ut.utt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [09:19<00:00,  6.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def extract_utt_nucc(path):\n",
    "    files = os.listdir(path)\n",
    "    nucc_convs = []\n",
    "    for filename in tqdm(files):\n",
    "        if \".json\" not in filename:\n",
    "            continue\n",
    "        # name = filename.split(\".\")[0]\n",
    "        with open(path+filename, \"r\") as f:\n",
    "            data  = json.load(f)\n",
    "            for conv in data[\"turns\"]:\n",
    "                utt = conv[\"utterance\"]\n",
    "                if len(nlp(utt)) < 2:\n",
    "                    # print(utt)\n",
    "                    continue\n",
    "                nucc_convs.append(clean_text(utt))\n",
    "    return nucc_convs\n",
    "# nuccデータ\n",
    "nucc_path = \"../../corpus/nucc/conv2/\"\n",
    "nucc_convs = extract_utt_nucc(nucc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_utt_ntt():\n",
    "    ntt_path = \"../../corpus/NTT/\"\n",
    "    utt_list = []\n",
    "    for file_ in os.listdir(ntt_path):\n",
    "        if not \"json\" in file_:\n",
    "            continue \n",
    "        with open(ntt_path+file_, \"r\",  encoding=\"utf-8\") as f:\n",
    "            convs = json.load(f)\n",
    "            for did in convs[\"convs\"]:\n",
    "                dids = list( did.keys() )[0]\n",
    "                conv = did[dids]\n",
    "                # conv = did[dids][3::3]\n",
    "                utt_list.extend( [ utt for utt in conv])\n",
    "    \n",
    "    print(len(utt_list))\n",
    "    return utt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141777\n"
     ]
    }
   ],
   "source": [
    "ntt_utt = load_utt_ntt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = conv_utt + nucc_convs + ntt_utt + all_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12536965"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_plain(text):\n",
    "    text_ = neologdn.normalize(text)\n",
    "    text_ = re.sub(r'\\(.*\\)', \"\", text_)\n",
    "    text_ = re.sub(r'\\d+', \"0\", text_)\n",
    "    if \"？？\" in text_:\n",
    "        text_ = text_.replace(\"？？\", \"？\")\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12536965/12536965 [03:56<00:00, 53042.77it/s]\n"
     ]
    }
   ],
   "source": [
    "all_utt = [clean_text_plain(t) for t in tqdm(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_analyzer = Analyzer(tokenizer_)\n",
    "mecab_analyzer = Analyzer(Tokenizer())\n",
    "\n",
    "def last_pos(pos_split, filler=\"*\"):\n",
    "    last = len(pos_split) - 1 \n",
    "    for i, _ in enumerate(pos_split):\n",
    "        if pos_split[last-i] != filler :\n",
    "            return \"[\"+pos_split[last-i]+\"]\"\n",
    "def sentence2normalize_noun_mecab(sentences):\n",
    "    normalize_sen = []\n",
    "    for sen in tqdm(sentences):\n",
    "        df = mecab_analyzer.analyze_with_dataframe(sen)\n",
    "        words = []\n",
    "        if df is None:\n",
    "            continue\n",
    "        for txt, pos in zip(df.surface, df.part_of_speech):\n",
    "            pos_split = pos.split(\"-\")\n",
    "            # if pos_split[0]\n",
    "            # if pos_split[1] != \"サ変接続\" and pos_split[1] != \"非自立\" and pos_split[1] != \"接尾\" and \"名詞\" == pos_split[0]:\n",
    "            if pos_split[0]==\"名詞\":\n",
    "                # words.append(pos)\n",
    "                # print(txt, pos)\n",
    "                last_pos_name = last_pos(pos_split, filler=\"\")\n",
    "                if last_pos_name==\"[サ変可能]\":\n",
    "                    words.append(txt)\n",
    "                else:\n",
    "                    words.append(last_pos_name)\n",
    "            else:\n",
    "                words.append(txt)\n",
    "        normalize_sen.append(words)\n",
    "    return normalize_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 729.27it/s]\n"
     ]
    }
   ],
   "source": [
    "filled_normal = fill_SYMBOL_ONE( sentence2normalize_noun_mecab(all_utt[-30:]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 98421/311503 [01:39<03:39, 970.98it/s] [265] 2022-01-20 01:14:54,141 Info wakame.analyzer :text is empty!\n",
      "100%|██████████| 311503/311503 [05:15<00:00, 988.74it/s] \n"
     ]
    }
   ],
   "source": [
    "normal = sentence2normalize_noun_mecab(all_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311502/311502 [00:03<00:00, 96050.40it/s] \n"
     ]
    }
   ],
   "source": [
    "normal_split = [ list(\"\".join(L)) for L in tqdm(normal) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 4964752/12536965 [00:49<00:39, 192488.33it/s]"
     ]
    }
   ],
   "source": [
    "char_split = [ list(\"\".join(L)) for L in tqdm(all_utt) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_language_model(sentences, N):\n",
    "    vocab = Vocabulary([word for sent in sentences for word in sent])\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sentences]\n",
    "    lm = KneserNeyInterpolated(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "lm = create_language_model(char_split, N=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatools.maneger import DataManager\n",
    "modelM = DataManager(\"../models/utterance/\")\n",
    "model_name = \"KLM_phrase_nucc_ntt_wiki_n={0}_splited_char.pickle\".format(n)\n",
    "# model_name = \"KLM_phrase_nucc_wiki_n={0}_noun2.pickle\".format(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/utterance/KLM_phrase_nucc_ntt_wiki_n=4_splited_unidic.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM.save_data(model_name, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataM = DataManager(\"../X_y_data/utterance/\")\n",
    "# dataM.save_data(\"nucc_ntt_split_char.pickle\", normal_split)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
