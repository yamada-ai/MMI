{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "# from tools.preproc import Preprocessor\n",
    "from tools import preproc\n",
    "# from tools.preproc import Preprocessor\n",
    "from tools.maneger import DataManager\n",
    "import spacy\n",
    "# import importlib\n",
    "pre = preproc.Preprocessor()\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import copy\n",
    "from feature import Feature\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS(texts):\n",
    "    \n",
    "    pos_list = []\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    for doc in docs:\n",
    "        pos_list.append([ token.tag_ for token in doc ])\n",
    "        \n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div2sentence(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        sentence_list = [str(s)  for s in doc.sents]\n",
    "\n",
    "    elif isinstance(text, list):\n",
    "        sentence_list  = []\n",
    "        docs = list(nlp.pipe(text, disable=['ner']))\n",
    "            # return [ self.get_POS(sen_) for sen_ in sen]\n",
    "        for doc in docs:\n",
    "            sentence_list.extend( [str(s) for s in doc.sents] )\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return sentence_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token_set(texts):\n",
    "    token_set = set()\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            token_set.add(token.text)\n",
    "    return token_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ave_length(texts):\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    ave_length = 0\n",
    "    for doc in docs:\n",
    "        ave_length += len(doc)\n",
    "    ave_length = int(ave_length/len(docs)) + 1\n",
    "    # ave_length = ave_length/len(doc)\n",
    "    return ave_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_sentence(token_list, length):\n",
    "    samples = random.choices(token_list, k=length)\n",
    "    return \"\".join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature(sentence_list):\n",
    "    features = []\n",
    "    docs = list(nlp.pipe(sentence_list, disable=['ner']))\n",
    "    # 名詞\n",
    "    # for doc in docs:\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yamada/Documents/MMI/UNI'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = root + '/error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./X_y_data/dialogue/\n",
      "data_LR.pickle\n"
     ]
    }
   ],
   "source": [
    "vec_mode = \"ginza\"\n",
    "data_mode_list = [\"dialogue\", \"wiki\"]\n",
    "data_mode = data_mode_list[0]\n",
    "data_path = \"./X_y_data/{0}/\".format(data_mode)\n",
    "dataM = DataManager(data_path)\n",
    "print(data_path)\n",
    "way = [\"LR\", \"DNN\"]\n",
    "data_name = \"data_{0}.pickle\".format(way[0])\n",
    "print(data_name)\n",
    "\n",
    "model_path = \"./models/{0}/\".format(data_mode)\n",
    "modelM = DataManager(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataM.is_exist(data_name):\n",
    "    \n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X_data = DATA_Xy[0]\n",
    "    y_data = DATA_Xy[1]\n",
    "# else:\n",
    "#     X_data, y_data = pre.extract_X_y(df, error_types, seq_len)\n",
    "#     dataM.save_data(data_name, [X_data, y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユーザ発話のみ取得し，操作\n",
    "\n",
    "usr_list = []\n",
    "for d, u, s, ec in zip(df.did, df.usr, df.sys, df.ec):\n",
    "    usr_list.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = copy.deepcopy(usr_list[:20])\n",
    "text = copy.deepcopy(usr_list)\n",
    "text = div2sentence(text)\n",
    "# pprint.pprint(text[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_set = make_token_set(text)\n",
    "token_list = list(token_set)\n",
    "ave_length = get_ave_length(text)\n",
    "\n",
    "correct_len = len(text)\n",
    "X_str = copy.deepcopy(text)\n",
    "y = [1]*correct_len\n",
    "for _ in range(correct_len):\n",
    "    X_str.append( make_random_sentence(token_list, ave_length) )\n",
    "    y.append(0)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X_str, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "F = Feature()\n",
    "F.make_features(X_train_str)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "for i, x_t_str in enumerate( X_train_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_train.append(x)\n",
    "for i, x_t_str in enumerate( X_test_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_test.append(x)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3458, 19450)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=10000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 自体難し蒸し暑いフィギュア少ない注目どこ木村\n",
      "1 : 初めて聞いた名前\n",
      "0 : 日本人カレーパン忍れる花火凍らHEROざわ\n",
      "0 : か朝日寒くまどろっこしく來未休みルーム佐村\n",
      "1 : 私は1位じゃないことに驚きました\n",
      "1 : 私は喜多方ラーメンが好きです\n",
      "1 : 入社前の裁判は日本テレビですよ。\n",
      "1 : ミラノ版なんてあるんだね\n",
      "0 : 寝苦しい仲良く時代劇時頃たく秋何事\n",
      "0 : 金髪天下一品は横切るこまめ楽しく元気付い\n"
     ]
    }
   ],
   "source": [
    "for y_p, x_s in zip(y_pred[:10], X_test_str[:10]):\n",
    "    print(\"{0} : {1}\".format(y_p, x_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[748   2]\n",
      " [ 16 716]]\n",
      "accuracy =  0.9878542510121457\n",
      "precision =  0.9972144846796658\n",
      "recall =  0.9781420765027322\n",
      "f1 score =  0.9875862068965517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"これはペンです\", \"F1スコアは、適合率と再現率の調和平均で定義されます\", \"鉛筆は下書きを描くのです\"]\n",
    "XX = []\n",
    "for t in test:\n",
    "    XX.append(F.featurization(t))\n",
    "pred =  lr.predict(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}