{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../tools/')\n",
    "from maneger import DataManager\n",
    "# from tools import preproc\n",
    "from preproc import Preprocessor\n",
    "# from utterance.feature import Featur\n",
    "import spacy\n",
    "# import importlib\n",
    "pre = Preprocessor()\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import copy\n",
    "from feature import Feature\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "nlp = spacy.load('ja_ginza')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_POS(texts):\n",
    "    \n",
    "    pos_list = []\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    for doc in docs:\n",
    "        pos_list.append([ token.tag_ for token in doc ])\n",
    "        \n",
    "    return pos_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def div2sentence(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        sentence_list = [str(s)  for s in doc.sents]\n",
    "\n",
    "    elif isinstance(text, list):\n",
    "        sentence_list  = []\n",
    "        docs = list(nlp.pipe(text, disable=['ner']))\n",
    "            # return [ self.get_POS(sen_) for sen_ in sen]\n",
    "        for doc in docs:\n",
    "            sentence_list.extend( [str(s) for s in doc.sents] )\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return sentence_list "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def make_token_set(texts):\n",
    "    token_set = set()\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            token_set.add(token.text)\n",
    "    return token_set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_ave_length(texts):\n",
    "    docs = list(nlp.pipe(texts, disable=['ner']))\n",
    "    ave_length = 0\n",
    "    for doc in docs:\n",
    "        ave_length += len(doc)\n",
    "    ave_length = int(ave_length/len(docs)) + 1\n",
    "    # ave_length = ave_length/len(doc)\n",
    "    return ave_length"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def make_random_sentence(token_list, length):\n",
    "    samples = random.choices(token_list, k=length)\n",
    "    return \"\".join(samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def make_feature(sentence_list):\n",
    "    features = []\n",
    "    docs = list(nlp.pipe(sentence_list, disable=['ner']))\n",
    "    # 名詞\n",
    "    # for doc in docs:\n",
    "    \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "root = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "root"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/yamada/Documents/MMI/UNI'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "path = root + '/error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "print(df.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "vec_mode = \"ginza\"\n",
    "data_mode_list = [\"dialogue\", \"wiki\"]\n",
    "data_mode = data_mode_list[0]\n",
    "data_path = \"./X_y_data/{0}/\".format(data_mode)\n",
    "dataM = DataManager(data_path)\n",
    "print(data_path)\n",
    "way = [\"LR\", \"DNN\"]\n",
    "data_name = \"data_{0}.pickle\".format(way[0])\n",
    "print(data_name)\n",
    "\n",
    "model_path = \"./models/{0}/\".format(data_mode)\n",
    "modelM = DataManager(model_path)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./X_y_data/dialogue/\n",
      "data_LR.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "if dataM.is_exist(data_name):\n",
    "    \n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X_data = DATA_Xy[0]\n",
    "    y_data = DATA_Xy[1]\n",
    "else:\n",
    "    X_data, y_data = pre.extract_X_y(df, error_types, seq_len)\n",
    "    dataM.save_data(data_name, [X_data, y_data])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'seq_len' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-49e3bcc4bf51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_Xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq_len' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# ユーザ発話のみ取得し，操作\n",
    "\n",
    "usr_list = []\n",
    "for d, u, s, ec in zip(df.did, df.usr, df.sys, df.ec):\n",
    "    usr_list.append(u)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "# text = copy.deepcopy(usr_list[:20])\n",
    "text = copy.deepcopy(usr_list)\n",
    "text = div2sentence(text)\n",
    "# pprint.pprint(text[:20])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\n",
    "token_set = make_token_set(text)\n",
    "token_list = list(token_set)\n",
    "ave_length = get_ave_length(text)\n",
    "\n",
    "correct_len = len(text)\n",
    "X_str = copy.deepcopy(text)\n",
    "y = [1]*correct_len\n",
    "for _ in range(correct_len):\n",
    "    X_str.append( make_random_sentence(token_list, ave_length) )\n",
    "    y.append(0)\n",
    "y = np.array(y)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\n",
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X_str, y, test_size=0.30, random_state=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "F = Feature()\n",
    "F.set_preprocessor(Preprocessor())\n",
    "F.make_features(X_train_str)\n",
    "\n",
    "F_path = \"../X_y_data/utterance/\"\n",
    "F_name = \"grammer.pickle\"\n",
    "featureM = DataManager(F_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "featureM.save_data(F_name, F)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success save : ../X_y_data/utterance/grammer.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "for i, x_t_str in enumerate( X_train_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_train.append(x)\n",
    "for i, x_t_str in enumerate( X_test_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_test.append(x)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=10000)\n",
    "lr.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "y_pred = lr.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "for y_p, x_s in zip(y_pred[:10], X_test_str[:10]):\n",
    "    print(\"{0} : {1}\".format(y_p, x_s))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 : 朝ニッカウヰスキーでしょうそこお気に入りシンガポールって家\n",
      "1 : 初めて聞いた名前\n",
      "0 : ブラック教わり頑張りこっち」、カッコイイお\n",
      "0 : 危険ジーニストエコ日本食椅子持ち味技術メロン\n",
      "1 : 私は1位じゃないことに驚きました\n",
      "1 : 私は喜多方ラーメンが好きです\n",
      "1 : 入社前の裁判は日本テレビですよ。\n",
      "1 : ミラノ版なんてあるんだね\n",
      "0 : 同じくネイマール国際賢くスイス大リーグかっこいい歴史\n",
      "0 : 反応ダイエットラーメン適度褒めヒカル矢部うどん\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "confusion matrix = \n",
      " [[734  16]\n",
      " [  4 728]]\n",
      "accuracy =  0.9865047233468286\n",
      "precision =  0.978494623655914\n",
      "recall =  0.994535519125683\n",
      "f1 score =  0.986449864498645\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "test = [\"これはペンです\", \"F1スコアは、適合率と再現率の調和平均で定義されます\", \"鉛筆は下書きを描くのです\"]\n",
    "XX = []\n",
    "for t in test:\n",
    "    XX.append(F.featurization(t))\n",
    "pred =  lr.predict(XX)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "pred"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "model_path = \"../models/utterance/\"\n",
    "model_name = \"grammer_M.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "grammer_M.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "modelM.save_data(model_name, lr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success save : ../models/utterance/grammer_M.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}