{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertJapaneseTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BertProofreader:\n",
    "    def __init__(self, pretrained_model: str, cache_dir: str = None):\n",
    "\n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model, cache_dir=cache_dir)\n",
    "\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = BertForMaskedLM.from_pretrained(pretrained_model, cache_dir=cache_dir)\n",
    "        self.model.to('cuda')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def mask_prediction(self, sentence: str) -> torch.Tensor:\n",
    "        # 特殊Tokenの追加\n",
    "        sentence = f'[CLS]{sentence}[SEP]'\n",
    "\n",
    "        tokenized_text = self.tokenizer.tokenize(sentence)\n",
    "\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens], device='cuda')\n",
    "\n",
    "        # [MASK]に対応するindexを取得\n",
    "        mask_index = self.tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "\n",
    "        # 1単語ずつ[MASK]に置き換えたTensorを作る\n",
    "        repeat_num = tokens_tensor.shape[1] - 2\n",
    "        tokens_tensor = tokens_tensor.repeat(repeat_num, 1)\n",
    "        for i in range(repeat_num):\n",
    "            tokens_tensor[i, i + 1] = mask_index\n",
    "\n",
    "        # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tokens_tensor, token_type_ids=None)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        return tokenized_text, predictions\n",
    "\n",
    "    def check_topk(self, sentence: str, topk: int = 10):\n",
    "        \"\"\"\n",
    "        [MASK]に対して予測された単語のTop Kに元の単語が含まれていればTrueと判定\n",
    "        \"\"\"\n",
    "\n",
    "        tokens, predictions = self.mask_prediction(sentence)\n",
    "\n",
    "        pred_sort = torch.argsort(predictions, dim=2, descending=True)\n",
    "        pred_top_k = pred_sort[:, :, :topk]  # 上位Xのindex取得\n",
    "\n",
    "        judges = []\n",
    "        for i in range(len(tokens) - 2):\n",
    "            pred_top_k_word = self.tokenizer.convert_ids_to_tokens(pred_top_k[i][i + 1])\n",
    "            judges.append(tokens[i + 1] in pred_top_k_word)\n",
    "            logger.info(f'{tokens[i + 1]}: {judges[-1]}')\n",
    "            logger.debug(f'top k word={pred_top_k_word}')\n",
    "\n",
    "        return all(judges)\n",
    "\n",
    "    def check_threshold(self, sentence: str, threshold: float = 0.01):\n",
    "        \"\"\"\n",
    "        [MASK]に対して予測された単語のスコアが閾値以上の単語群に、元の単語が含まれていればTrueと判定\n",
    "        \"\"\"\n",
    "        tokens, predictions = self.mask_prediction(sentence)\n",
    "\n",
    "        predictions = predictions.softmax(dim=2)\n",
    "\n",
    "        judges = []\n",
    "        for i in range(len(tokens) - 2):\n",
    "            indices = (predictions[i][i + 1] >= threshold).nonzero()\n",
    "            pred_top_word = self.tokenizer.convert_ids_to_tokens(indices)\n",
    "            judges.append(tokens[i + 1] in pred_top_word)\n",
    "            logger.info(f'{tokens[i + 1]}: {judges[-1]}')\n",
    "\n",
    "        return all(judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# from models.bert_proofreader import BertProofreader\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "PRETRAINED_MODEL = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "proofreader = BertProofreader(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:今: True\n",
      "INFO:__main__:彼女: True\n",
      "INFO:__main__:が: True\n",
      "INFO:__main__:い: True\n",
      "INFO:__main__:ない: True\n",
      "INFO:__main__:ん: True\n",
      "INFO:__main__:##で: True\n",
      "INFO:__main__:、: True\n",
      "INFO:__main__:私: True\n",
      "INFO:__main__:も: True\n",
      "INFO:__main__:是非: False\n",
      "INFO:__main__:上げ: False\n",
      "INFO:__main__:たい: True\n",
      "INFO:__main__:です: True\n",
      "INFO:__main__:。: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proofreader.check_topk('今彼女がいないんで、私も是非上げたいです。', topk=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:元気: True\n",
      "INFO:__main__:は: True\n",
      "INFO:__main__:良い: True\n",
      "INFO:__main__:ん: True\n",
      "INFO:__main__:です: True\n",
      "INFO:__main__:か: True\n",
      "INFO:__main__:?: True\n",
      "INFO:__main__:##?: True\n",
      "INFO:__main__:元気: True\n",
      "INFO:__main__:です: True\n",
      "INFO:__main__:か: True\n",
      "INFO:__main__:は: True\n",
      "INFO:__main__:元気: True\n",
      "INFO:__main__:か: True\n",
      "INFO:__main__:.: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proofreader.check_topk('元気は良いんですか？？元気ですかは元気か．', topk=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
