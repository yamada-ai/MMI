{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "# from datatools.analyzer import clean_text\n",
    "from error_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm        import Vocabulary\n",
    "# from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.util      import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_utt = []\n",
    "for conv in convs:\n",
    "    for i, ut in enumerate(conv):\n",
    "        if not ut.is_exist_error():\n",
    "            conv_utt.append(clean_text(ut.utt))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def extract_utt_nucc(path):\n",
    "    files = os.listdir(path)\n",
    "    nucc_convs = []\n",
    "    for filename in tqdm(files):\n",
    "        if \".json\" not in filename:\n",
    "            continue\n",
    "        # name = filename.split(\".\")[0]\n",
    "        with open(path+filename, \"r\") as f:\n",
    "            data  = json.load(f)\n",
    "            for conv in data[\"turns\"]:\n",
    "                utt = conv[\"utterance\"]\n",
    "                if len(nlp(utt)) < 2:\n",
    "                    # print(utt)\n",
    "                    continue\n",
    "                nucc_convs.append(clean_text(utt))\n",
    "    return nucc_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [08:48<00:00,  5.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# nuccデータ\n",
    "nucc_path = \"../../corpus/nucc/conv2/\"\n",
    "nucc_convs = extract_utt_nucc(nucc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode phrase\n",
    "\n",
    "# phrase_data = \"../../corpus/gogakuru/phrases.csv\"\n",
    "# df = pd.read_csv(phrase_data)\n",
    "# corpus = list( df[\"phrase\"].values ) + nucc_convs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_utt_ntt():\n",
    "    ntt_path = \"../../corpus/NTT/\"\n",
    "\n",
    "    # with open(ntt_path+\"empathetic.json\", \"r\") as f:\n",
    "    #     empathetic = json.load(f)\n",
    "    # with open(ntt_path+\"persona.json\", \"r\") as f:\n",
    "    #     persona = json.load(f)\n",
    "\n",
    "    utt_list = []\n",
    "\n",
    "    for file_ in os.listdir(ntt_path):\n",
    "        if not \"json\" in file_:\n",
    "            continue \n",
    "        with open(ntt_path+file_, \"r\",  encoding=\"utf-8\") as f:\n",
    "            convs = json.load(f)\n",
    "            for did in convs[\"convs\"]:\n",
    "                dids = list( did.keys() )[0]\n",
    "                conv = did[dids]\n",
    "                # conv = did[dids][3::3]\n",
    "                utt_list.extend( [ clean_text(utt)  for utt in conv])\n",
    "    \n",
    "    print(len(utt_list))\n",
    "    return utt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141777\n"
     ]
    }
   ],
   "source": [
    "ntt_utt = load_utt_ntt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = conv_utt + nucc_convs + ntt_utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus:188013, conv:2851, nucc:43385, ntt:141777\n"
     ]
    }
   ],
   "source": [
    "print(\"corpus:{0}, conv:{1}, nucc:{2}, ntt:{3}\".format(len(corpus), len(conv_utt), len(nucc_convs), len(ntt_utt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled_normal = fill_SYMBOL( sentence2normalize_nv(corpus) )\n",
    "# filled_normal = fill_SYMBOL(sentence2normalize_independent(corpus) )\n",
    "filled_normal = fill_SYMBOL( sentence2normalize_noun(corpus) )\n",
    "# filled_normal  = fill_SYMBOL( sentence2morpheme(corpus) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_language_model(sentences, N):\n",
    "    vocab = Vocabulary([word for sent in sentences for word in sent])\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sentences]\n",
    "    lm = KneserNeyInterpolated(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "lm = create_language_model(filled_normal, N=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_list = []\n",
    "# for word in lm.context_counts(lm.vocab.lookup(context)): # 文脈に続く単語一覧の取得\n",
    "#     prob_list.append((word, lm.score(word, context))) # 単語のその出現する確率を格納\n",
    "\n",
    "# prob_list.sort(key=lambda x: x[1], reverse=True) # 出現確率順にソート\n",
    "# for word, prob in prob_list:\n",
    "#     print('\\t{:s}: {:f}'.format(word, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pprint\n",
    "def sentence2score(sentence, l, N):\n",
    "    # filled = fill_SYMBOL( sentence2normalize_nv(sentence) )\n",
    "    filled = fill_SYMBOL( sentence2normalize_noun(sentence) )\n",
    "    # filled = fill_SYMBOL( sentence2morpheme(sentence) )\n",
    "    # filled = fill_SYMBOL( sentence2normalize_independent(sentence) )\n",
    "    filled_pos = fill_SYMBOL( sentence2pos(sentence) )\n",
    "    print(filled)\n",
    "    print(filled_pos)\n",
    "    ngram_text = []\n",
    "    ngram_pos = []\n",
    "\n",
    "    function_score = 0\n",
    "    # デフォルトで1\n",
    "    function_num = 1\n",
    "\n",
    "    for L, P in zip(filled,filled_pos):\n",
    "        for i in range(len(L)-N+1):\n",
    "            # print(L[i:i+N])\n",
    "            ngram_text.append(L[i:i+N])\n",
    "            ngram_pos.append(P[i:i+N])\n",
    "    # pprint.pprint(ngram_text)\n",
    "    all_score = 0\n",
    "    for ngram, pgram in zip(ngram_text, ngram_pos):\n",
    "        context = (ngram[:-1])\n",
    "        context_pos = pgram[:-1]\n",
    "        # print(context)\n",
    "        # for word in lm.context_counts(lm.vocab.lookup(context)): # 文脈に続く単語一覧の取得\n",
    "            \n",
    "        score = lm.score(ngram[-1], context) + 1e-10\n",
    "        \n",
    "        log_score = math.log2(score)\n",
    "        if \"助動詞\" in context_pos[1] or \"助詞\" in context_pos[1] or \"助動詞\" in context_pos[0] or \"助詞\" in context_pos[0]:\n",
    "            print(\"\\tcontext : {0}| ->\".format(context), log_score)\n",
    "            function_score += log_score\n",
    "            function_num += 1\n",
    "    print(all_score/len(ngram_text))\n",
    "    # return function_score/function_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FOS', 'FOS', '名詞-普通名詞-一般', 'です', 'い', 'ます', 'EOS', 'EOS']]\n",
      "[['FOS', 'FOS', '名詞-普通名詞-一般', '助動詞', '動詞-非自立可能', '助動詞', 'EOS', 'EOS']]\n",
      "\tcontext : ['名詞-普通名詞-一般', 'です', 'い']| -> -0.15199125175478748\n",
      "\tcontext : ['です', 'い', 'ます']| -> -17.046528996520383\n",
      "\tcontext : ['い', 'ます', 'EOS']| -> -0.0009431763729459997\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# n=4\n",
    "sentence = \"最近とても暑いですから。\"\n",
    "sentence = \"ご存知ですいます\"\n",
    "# sentence = \"はい、そうですよ\"\n",
    "sentence2score(sentence, lm, N=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FOS', 'FOS', 'はい', '、', 'そう', 'です', 'よ', '。', 'EOS', 'EOS']]\n",
      "[['FOS', 'FOS', '感動詞-一般', '補助記号-読点', '副詞', '助動詞', '助詞-終助詞', '補助記号-句点', 'EOS', 'EOS']]\n",
      "\tcontext : ['そう', 'です', 'よ']| -> -3.4860704153263287\n",
      "\tcontext : ['です', 'よ', '。']| -> -0.015371320551957559\n",
      "\tcontext : ['よ', '。', 'EOS']| -> -5.082264860742065e-06\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"はい、そうですよ。\"\n",
    "sentence2score(sentence, lm, N=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled = fill_SYMBOL( sentence2normalize_noun(sentence) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FOS', 'FOS', 'はい', '、'], ['FOS', 'はい', '、', 'そう'], ['はい', '、', 'そう', 'です'], ['、', 'そう', 'です', 'よ'], ['そう', 'です', 'よ', '。'], ['です', 'よ', '。', 'EOS'], ['よ', '。', 'EOS', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "ngram_text = []\n",
    "for L in filled:\n",
    "    for i in range(len(L)-n+1):\n",
    "            # print(L[i:i+N])\n",
    "        ngram_text.append(L[i:i+n])\n",
    "print(ngram_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9443869758379804"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.perplexity(ngram_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatools.maneger import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelM = DataManager(\"../models/utterance/\")\n",
    "# model_name = \"KLM_phrase_n={0}.pickle\".format(n)\n",
    "# model_name = \"KLM_phrase_nucc_n={0}.pickle\".format(n)\n",
    "# model_name = \"KLM_phrase_nucc_n={0}_orth.pickle\".format(n)\n",
    "model_name = \"KLM_phrase_nucc_n={0}_noun2.pickle\".format(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/utterance/KLM_phrase_nucc_n=4_noun2.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelM.save_data(model_name, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/utterance/KLM_phrase_nucc_n=4_noun2.pickle\n"
     ]
    }
   ],
   "source": [
    "lm = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = ['Grammatical error', \"Uninterpretable\"]\n",
    "\n",
    "# y = []\n",
    "# n=3\n",
    "# y_pred = []\n",
    "# for conv in convs:\n",
    "#     for ut in conv:\n",
    "#         if not ut.is_system():\n",
    "#             continue\n",
    "#         # エラーなら1\n",
    "#         if ut.is_error_included(errors):\n",
    "#             # print(ut)\n",
    "#             y.append(1)\n",
    "#         else:\n",
    "#             y.append(0)\n",
    "#         #LM 判定\n",
    "#         # エラーなら1\n",
    "#         if sentence2score(ut.utt, lm, N=n) < -5.6:\n",
    "#             y_pred.append(1)\n",
    "#         else:\n",
    "#             y_pred.append(0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "# print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "# print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "\n",
    "# print('EM:', metrics.accuracy_score(y, y_pred))\n",
    "# print('F-measure: ', metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conv in convs:\n",
    "#     for ut in conv:\n",
    "#         if sentence2score(ut.utt, lm, N=3) < -5.5:\n",
    "#             # print(ut.utt)\n",
    "#             pass\n",
    "#         else:\n",
    "#             # print(ut.utt)\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-bac5893d322b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sklearn_confusion_matrix.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true=y, y_pred=y_pred)\n",
    "sns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues')\n",
    "plt.savefig('sklearn_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 951263 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
