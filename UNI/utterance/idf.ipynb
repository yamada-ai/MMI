{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "# from janome.tokenizer import Tokenizer\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = \"../../corpus/wiki/all_files/\"\n",
    "\n",
    "files = os.listdir(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20609.91"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[:10:2]\n",
    "skip=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 601204/1030496 [09:52<07:03, 1014.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-20439b39efd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# test_corpus.append( clean_text( text )  )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# test_corpus.append( text  )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmecab_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# documents.append (normalized_span(text) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MMI/UNI/datatools/analyzer.py\u001b[0m in \u001b[0;36mmecab_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmecab_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwakati\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfill_SYMBOL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/wakame/tokenizer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, wakati)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwakati\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurface\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/wakame/tokenizer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwakati\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurface\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "test_corpus = []\n",
    "from tqdm import tqdm\n",
    "for txt in tqdm( files[::2] ):\n",
    "    with open(file_path+txt, \"r\") as f:\n",
    "        text = f.readline()\n",
    "        if \"。\" not in text:\n",
    "            continue\n",
    "        # 再度掛けると結構消える\n",
    "        # test_corpus.append( clean_text( text )  )\n",
    "        # test_corpus.append( text  )\n",
    "        documents.append( mecab_tokenize(text) )\n",
    "        # documents.append (normalized_span(text) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353974"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121190/121190 [5:28:25<00:00,  6.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# documents = rhetoricasl_and_words(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../../corpus/wiki/idf_corpus_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "document_path = \"../../corpus/wiki/\"\n",
    "document_name = \"idf_corpus_v2.pickle\"\n",
    "documentM = DataManager(document_path)\n",
    "documentM.save_data(document_name, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[567] 2021-12-08 19:39:33,304 Info gensim.corpora.dictionary :adding document #0 to Dictionary(0 unique tokens: [])\n",
      "[567] 2021-12-08 19:39:36,087 Info gensim.corpora.dictionary :adding document #10000 to Dictionary(181649 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:38,773 Info gensim.corpora.dictionary :adding document #20000 to Dictionary(279290 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:41,282 Info gensim.corpora.dictionary :adding document #30000 to Dictionary(357307 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:43,898 Info gensim.corpora.dictionary :adding document #40000 to Dictionary(425192 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:46,285 Info gensim.corpora.dictionary :adding document #50000 to Dictionary(484167 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:48,911 Info gensim.corpora.dictionary :adding document #60000 to Dictionary(538575 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:51,379 Info gensim.corpora.dictionary :adding document #70000 to Dictionary(589441 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:54,052 Info gensim.corpora.dictionary :adding document #80000 to Dictionary(637343 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:56,518 Info gensim.corpora.dictionary :adding document #90000 to Dictionary(681832 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:39:58,941 Info gensim.corpora.dictionary :adding document #100000 to Dictionary(723407 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:01,354 Info gensim.corpora.dictionary :adding document #110000 to Dictionary(763697 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:03,781 Info gensim.corpora.dictionary :adding document #120000 to Dictionary(802654 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:06,383 Info gensim.corpora.dictionary :adding document #130000 to Dictionary(841452 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:08,897 Info gensim.corpora.dictionary :adding document #140000 to Dictionary(877540 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:11,499 Info gensim.corpora.dictionary :adding document #150000 to Dictionary(913107 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:14,033 Info gensim.corpora.dictionary :adding document #160000 to Dictionary(947144 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:16,514 Info gensim.corpora.dictionary :adding document #170000 to Dictionary(980321 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:18,979 Info gensim.corpora.dictionary :adding document #180000 to Dictionary(1011713 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:21,368 Info gensim.corpora.dictionary :adding document #190000 to Dictionary(1042471 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:23,745 Info gensim.corpora.dictionary :adding document #200000 to Dictionary(1072676 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:26,161 Info gensim.corpora.dictionary :adding document #210000 to Dictionary(1102883 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:28,585 Info gensim.corpora.dictionary :adding document #220000 to Dictionary(1131696 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:30,965 Info gensim.corpora.dictionary :adding document #230000 to Dictionary(1159633 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:33,338 Info gensim.corpora.dictionary :adding document #240000 to Dictionary(1186320 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:35,686 Info gensim.corpora.dictionary :adding document #250000 to Dictionary(1212634 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:38,091 Info gensim.corpora.dictionary :adding document #260000 to Dictionary(1239415 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:40,509 Info gensim.corpora.dictionary :adding document #270000 to Dictionary(1265397 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:43,006 Info gensim.corpora.dictionary :adding document #280000 to Dictionary(1292027 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:45,594 Info gensim.corpora.dictionary :adding document #290000 to Dictionary(1317900 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:47,991 Info gensim.corpora.dictionary :adding document #300000 to Dictionary(1342163 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:50,317 Info gensim.corpora.dictionary :adding document #310000 to Dictionary(1366496 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:52,704 Info gensim.corpora.dictionary :adding document #320000 to Dictionary(1390331 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:55,049 Info gensim.corpora.dictionary :adding document #330000 to Dictionary(1413151 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:57,436 Info gensim.corpora.dictionary :adding document #340000 to Dictionary(1436521 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:40:59,911 Info gensim.corpora.dictionary :adding document #350000 to Dictionary(1458808 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...)\n",
      "[567] 2021-12-08 19:41:00,906 Info gensim.corpora.dictionary :built Dictionary(1468211 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...) from 353974 documents (total 163125928 corpus positions)\n",
      "[567] 2021-12-08 19:41:00,908 Info gensim.utils :Dictionary lifecycle event {'msg': \"built Dictionary(1468211 unique tokens: ['、', '。', 'の', 'は', 'アメリカ合衆国']...) from 353974 documents (total 163125928 corpus positions)\", 'datetime': '2021-12-08T19:41:00.907589', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "dictionary_ = corpora.Dictionary(documents)\n",
    "corpus = list(map(dictionary_.doc2bow,documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[567] 2021-12-08 19:42:02,229 Info gensim.models.tfidfmodel :collecting document frequencies\n",
      "[567] 2021-12-08 19:42:02,230 Info gensim.models.tfidfmodel :PROGRESS: processing document #0\n",
      "[567] 2021-12-08 19:42:02,451 Info gensim.models.tfidfmodel :PROGRESS: processing document #10000\n",
      "[567] 2021-12-08 19:42:02,681 Info gensim.models.tfidfmodel :PROGRESS: processing document #20000\n",
      "[567] 2021-12-08 19:42:02,927 Info gensim.models.tfidfmodel :PROGRESS: processing document #30000\n",
      "[567] 2021-12-08 19:42:03,174 Info gensim.models.tfidfmodel :PROGRESS: processing document #40000\n",
      "[567] 2021-12-08 19:42:03,415 Info gensim.models.tfidfmodel :PROGRESS: processing document #50000\n",
      "[567] 2021-12-08 19:42:03,667 Info gensim.models.tfidfmodel :PROGRESS: processing document #60000\n",
      "[567] 2021-12-08 19:42:03,917 Info gensim.models.tfidfmodel :PROGRESS: processing document #70000\n",
      "[567] 2021-12-08 19:42:04,174 Info gensim.models.tfidfmodel :PROGRESS: processing document #80000\n",
      "[567] 2021-12-08 19:42:04,421 Info gensim.models.tfidfmodel :PROGRESS: processing document #90000\n",
      "[567] 2021-12-08 19:42:04,677 Info gensim.models.tfidfmodel :PROGRESS: processing document #100000\n",
      "[567] 2021-12-08 19:42:04,934 Info gensim.models.tfidfmodel :PROGRESS: processing document #110000\n",
      "[567] 2021-12-08 19:42:05,188 Info gensim.models.tfidfmodel :PROGRESS: processing document #120000\n",
      "[567] 2021-12-08 19:42:05,445 Info gensim.models.tfidfmodel :PROGRESS: processing document #130000\n",
      "[567] 2021-12-08 19:42:05,699 Info gensim.models.tfidfmodel :PROGRESS: processing document #140000\n",
      "[567] 2021-12-08 19:42:05,961 Info gensim.models.tfidfmodel :PROGRESS: processing document #150000\n",
      "[567] 2021-12-08 19:42:06,219 Info gensim.models.tfidfmodel :PROGRESS: processing document #160000\n",
      "[567] 2021-12-08 19:42:06,479 Info gensim.models.tfidfmodel :PROGRESS: processing document #170000\n",
      "[567] 2021-12-08 19:42:06,736 Info gensim.models.tfidfmodel :PROGRESS: processing document #180000\n",
      "[567] 2021-12-08 19:42:06,998 Info gensim.models.tfidfmodel :PROGRESS: processing document #190000\n",
      "[567] 2021-12-08 19:42:07,256 Info gensim.models.tfidfmodel :PROGRESS: processing document #200000\n",
      "[567] 2021-12-08 19:42:07,517 Info gensim.models.tfidfmodel :PROGRESS: processing document #210000\n",
      "[567] 2021-12-08 19:42:07,777 Info gensim.models.tfidfmodel :PROGRESS: processing document #220000\n",
      "[567] 2021-12-08 19:42:08,040 Info gensim.models.tfidfmodel :PROGRESS: processing document #230000\n",
      "[567] 2021-12-08 19:42:08,299 Info gensim.models.tfidfmodel :PROGRESS: processing document #240000\n",
      "[567] 2021-12-08 19:42:08,552 Info gensim.models.tfidfmodel :PROGRESS: processing document #250000\n",
      "[567] 2021-12-08 19:42:08,811 Info gensim.models.tfidfmodel :PROGRESS: processing document #260000\n",
      "[567] 2021-12-08 19:42:09,091 Info gensim.models.tfidfmodel :PROGRESS: processing document #270000\n",
      "[567] 2021-12-08 19:42:09,445 Info gensim.models.tfidfmodel :PROGRESS: processing document #280000\n",
      "[567] 2021-12-08 19:42:09,765 Info gensim.models.tfidfmodel :PROGRESS: processing document #290000\n",
      "[567] 2021-12-08 19:42:10,114 Info gensim.models.tfidfmodel :PROGRESS: processing document #300000\n",
      "[567] 2021-12-08 19:42:10,427 Info gensim.models.tfidfmodel :PROGRESS: processing document #310000\n",
      "[567] 2021-12-08 19:42:10,714 Info gensim.models.tfidfmodel :PROGRESS: processing document #320000\n",
      "[567] 2021-12-08 19:42:11,014 Info gensim.models.tfidfmodel :PROGRESS: processing document #330000\n",
      "[567] 2021-12-08 19:42:11,303 Info gensim.models.tfidfmodel :PROGRESS: processing document #340000\n",
      "[567] 2021-12-08 19:42:11,590 Info gensim.models.tfidfmodel :PROGRESS: processing document #350000\n",
      "[567] 2021-12-08 19:42:13,970 Info gensim.utils :TfidfModel lifecycle event {'msg': 'calculated IDF weights for 353974 documents and 1468211 features (55063900 matrix non-zeros)', 'datetime': '2021-12-08T19:42:13.970679', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'initialize'}\n"
     ]
    }
   ],
   "source": [
    "test_model = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = test_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = test_model.idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict = dict()\n",
    "for id_ in idfs.keys():\n",
    "    # print( dictionary_[id_], idfs[id_] )\n",
    "    idf_dict[dictionary_[id_]] = idfs[id_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1468211"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set を取っていない\n",
    "\n",
    "        の 0.9714308478032291\n",
    "        スワン 6.643856189774725\n",
    "        定理 6.643856189774725\n",
    "        神 6.643856189774725\n",
    "        神道流 6.643856189774725\n",
    "        道流 6.643856189774725\n",
    "        オーウェンス 6.643856189774725\n",
    "        ケビン 6.643856189774725\n",
    "        ケビン・オーウェンス 6.643856189774725\n",
    "        ・ 1.4739311883324122\n",
    "        , 4.058893689053568\n",
    "        . 1.4739311883324122\n",
    "        0 1.217591435072627\n",
    "        0,0丁 6.643856189774725\n",
    "        0丁以上 6.643856189774725\n",
    "        0年 2.1844245711374275\n",
    "        0年0月 3.3219280948873626\n",
    "        0年0月0日 2.736965594166206\n",
    "        0年頃 5.643856189774724\n",
    "        0日 5.058893689053569\n",
    "        0月 5.058893689053569\n",
    "        0月0日 4.321928094887363\n",
    "        、 1.1202942337177118\n",
    "        。 1.1202942337177118\n",
    "        「 2.321928094887362"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../../corpus/wiki/idf_wiki_v2.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(idf_dict,  f,  ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_path = \"../../corpus/wiki/idf_wiki_v2.json\"\n",
    "with open(idf_path, \"r\") as f:\n",
    "    idf_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idf = sorted(idf_dict.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1468211"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ジェニファー・ランドン', 18.433283870200963),\n",
       " ('ローカルシート', 18.433283870200963),\n",
       " ('ローカル・ボイド', 18.433283870200963),\n",
       " ('泊母子殺人事件', 18.433283870200963),\n",
       " ('サブナック', 18.433283870200963),\n",
       " ('サブナッケ', 18.433283870200963),\n",
       " ('サルマク', 18.433283870200963),\n",
       " ('サヴノック', 18.433283870200963),\n",
       " ('ケッツァーナケッツァーナ', 18.433283870200963),\n",
       " ('ナリー・ライアン', 18.433283870200963),\n",
       " ('バーシ・ブラック', 18.433283870200963),\n",
       " ('フォートシャーマン', 18.433283870200963),\n",
       " ('ヴィルヘルム・ファレイ', 18.433283870200963),\n",
       " ('school of lock! university', 18.433283870200963),\n",
       " ('とーや', 18.433283870200963),\n",
       " ('やしろ教頭', 18.433283870200963),\n",
       " ('やましげ校長', 18.433283870200963),\n",
       " ('よしだ教頭', 18.433283870200963),\n",
       " ('グラン・プレミオ・ブルーノ・ベゲッリ', 18.433283870200963),\n",
       " ('ブルーノ・ベゲッリ', 18.433283870200963),\n",
       " ('めいりょう', 18.433283870200963),\n",
       " ('ビズカード', 18.433283870200963),\n",
       " ('計画配送', 18.433283870200963),\n",
       " ('玫胡', 18.433283870200963),\n",
       " ('グラセナッピアグラセナッピア', 18.433283870200963),\n",
       " ('グラセナップ', 18.433283870200963),\n",
       " ('auenbach', 18.433283870200963),\n",
       " ('hunde', 18.433283870200963),\n",
       " ('krieh', 18.433283870200963),\n",
       " ('kränk', 18.433283870200963)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_idf[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_values = [idf[1]  for idf in sorted_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15594197768310883,\n",
       " 0.05223950009537809,\n",
       " 0.031479370274679966,\n",
       " 0.0055168175786554415,\n",
       " 0.0003464768721207745]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_values[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25パーセント点 5.340630805533431\n",
      "75パーセント点 16.433283870200963\n",
      "四分位範囲 11.092653064667532\n"
     ]
    }
   ],
   "source": [
    "q75, q25 = np.percentile(idf_values, [25, 0.05])\n",
    "iqr = q75 - q25\n",
    "print(\"25パーセント点\", q25)\n",
    "print(\"75パーセント点\", q75)\n",
    "print(\"四分位範囲\", iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鬼\t名詞,普通名詞,一般,,,,オニ,鬼,鬼,オニ,鬼,オニ,和,\"\",\"\",\"\",\"\",\"\",\"\",体,オニ,オニ,オニ,オニ,\"2\",\"C3\",\"\",1408208140902912,5123\n",
      "滅\t名詞,普通名詞,一般,,,\n",
      "の\t助詞,格助詞,,,,,ノ,の,の,ノ,の,ノ,和,\"\",\"\",\"\",\"\",\"\",\"\",格助,ノ,ノ,ノ,ノ,\"\",\"名詞%F1\",\"\",7968444268028416,28989\n",
      "刃\t名詞,普通名詞,一般,,,,ハ,刃,刃,ハ,刃,ハ,和,\"ハ濁\",\"基本形\",\"\",\"\",\"\",\"\",体,ハ,ハ,ハ,ハ,\"1\",\"C3\",\"\",8060803244761600,29325\n",
      "も\t助詞,係助詞,,,,,モ,も,も,モ,も,モ,和,\"\",\"\",\"\",\"\",\"\",\"\",係助,モ,モ,モ,モ,\"\",\"動詞%F2@-1,形容詞%F4@-2,名詞%F1\",\"\",10324972564259328,37562\n",
      "いい\t形容詞,非自立可能,,,形容詞,終止形-一般,ヨイ,良い,いい,イー,いい,イー,和,\"\",\"\",\"\",\"\",\"\",\"\",相,イイ,イイ,イイ,イイ,\"1\",\"C3\",\"\",10716948459561643,38988\n",
      "けれど\t助詞,接続助詞,,,,,ケレド,けれど,けれど,ケレド,けれど,ケレド,和,\"\",\"\",\"\",\"\",\"\",\"\",接助,ケレド,ケレド,ケレド,ケレド,\"\",\"動詞%F2@0,形容詞%F2@-1\",\"\",3089095131800064,11238\n",
      "、\t補助記号,読点,,,,,,、,、,,、,,記号,\"\",\"\",\"\",\"\",\"\",\"\",補助,,,,,\"\",\"\",\"\",6605693395456,24\n",
      "約束\t名詞,普通名詞,サ変可能,,,,ヤクソク,約束,約束,ヤクソク,約束,ヤクソク,漢,\"\",\"\",\"\",\"\",\"\",\"\",体,ヤクソク,ヤクソク,ヤクソク,ヤクソク,\"0\",\"C2\",\"\",10495396866564608,38182\n",
      "の\t助詞,格助詞,,,,,ノ,の,の,ノ,の,ノ,和,\"\",\"\",\"\",\"\",\"\",\"\",格助,ノ,ノ,ノ,ノ,\"\",\"名詞%F1\",\"\",7968444268028416,28989\n",
      "ネバー\t名詞,普通名詞,一般,,,,ネバー,ネバー-never,ネバー,ネバー,ネバー,ネバー,外,\"\",\"\",\"\",\"\",\"\",\"\",体,ネバー,ネバー,ネバー,ネバー,\"1\",\"C1\",\"\",22987498225541632,83628\n",
      "ランド\t名詞,普通名詞,一般,,,,ランド,ランド-land,ランド,ランド,ランド,ランド,外,\"\",\"\",\"\",\"\",\"\",\"\",体,ランド,ランド,ランド,ランド,\"1\",\"C1\",\"\",10950044924649984,39836\n",
      "も\t助詞,係助詞,,,,,モ,も,も,モ,も,モ,和,\"\",\"\",\"\",\"\",\"\",\"\",係助,モ,モ,モ,モ,\"\",\"動詞%F2@-1,形容詞%F4@-2,名詞%F1\",\"\",10324972564259328,37562\n",
      "ね\t助詞,終助詞,,,,,ネ,ね,ね,ネ,ね,ネ,和,\"\",\"\",\"\",\"\",\"\",\"\",終助,ネ,ネ,ネ,ネ,\"\",\"動詞%F1,名詞%F1,形容詞%F1\",\"\",7903847959896576,28754\n",
      "EOS\n",
      "\n",
      "鬼滅の刃\tキメツノヤイバ\tキメツノヤイバ\t鬼滅の刃\t名詞-固有名詞-一般\t\t\n",
      "も\tモ\tモ\tも\t助詞-係助詞\t\t\n",
      "いい\tイー\tヨイ\t良い\t形容詞-非自立可能\t形容詞\t終止形-一般\n",
      "けれど\tケレド\tケレド\tけれど\t助詞-接続助詞\t\t\n",
      "、\t\t\t、\t補助記号-読点\t\t\n",
      "約束のネバーランド\tヤクソクノネバーランド\tヤクソクノネバーランド\t約束のネバーランド\t名詞-固有名詞-一般\t\t\n",
      "も\tモ\tモ\tも\t助詞-係助詞\t\t\n",
      "ね\tネ\tネ\tね\t助詞-終助詞\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "tagger1 = MeCab.Tagger()\n",
    "dicdir = '-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-unidic-neologd'\n",
    "tagger2 = MeCab.Tagger(dicdir)\n",
    "\n",
    "sample_txt = '鬼滅の刃もいいけれど、約束のネバーランドもね'\n",
    "print(tagger1.parse(sample_txt))\n",
    "print(tagger2.parse(sample_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = '鬼滅の刃もいいけれど、約束のネバーランドもね'\n",
    "\n",
    "# 基本的な使い方\n",
    "tokenizer_ = Tokenizer(use_neologd=True)\n",
    "token_filters = [POSKeepFilter('名詞')]\n",
    "analyzer = Analyzer(tokenizer_, token_filters=token_filters)\n",
    "df = analyzer.analyze_with_dataframe(sample_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surface</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>infl_type</th>\n",
       "      <th>infl_form</th>\n",
       "      <th>base_form</th>\n",
       "      <th>reading</th>\n",
       "      <th>phonetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鬼滅の刃</td>\n",
       "      <td>名詞,固有名詞,一般,*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>鬼滅の刃</td>\n",
       "      <td>キメツノヤイバ</td>\n",
       "      <td>キメツノヤイバ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>約束のネバーランド</td>\n",
       "      <td>名詞,固有名詞,一般,*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>約束のネバーランド</td>\n",
       "      <td>ヤクソクノネバーランド</td>\n",
       "      <td>ヤクソクノネバーランド</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     surface part_of_speech infl_type infl_form  base_form      reading  \\\n",
       "0       鬼滅の刃   名詞,固有名詞,一般,*         *         *       鬼滅の刃      キメツノヤイバ   \n",
       "1  約束のネバーランド   名詞,固有名詞,一般,*         *         *  約束のネバーランド  ヤクソクノネバーランド   \n",
       "\n",
       "      phonetic  \n",
       "0      キメツノヤイバ  \n",
       "1  ヤクソクノネバーランド  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['鬼滅の刃', 'も', 'いい', 'けれど', '、', '約束のネバーランド', 'も', 'ね']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_tokenize(sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
