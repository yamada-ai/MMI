{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(CaseModel, self).__init__()    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hid1= embedding_dim*2\n",
    "        self.hid2 = embedding_dim//2\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, self.hid1)\n",
    "        self.fc2 = nn.Linear(self.hid1, self.hid2)\n",
    "        self.hidden2tag = nn.Linear(self.hid2, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.hidden2tag( y )\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "border=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/utterance/group_border=0.8_2.pickle\n"
     ]
    }
   ],
   "source": [
    "dict_path = \"../models/utterance/\"\n",
    "dict_name = \"group_border={0}_2.pickle\".format(border)\n",
    "dictM = DataManager(dict_path)\n",
    "group_dict = dictM.load_data(dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word(word):\n",
    "    word = clean_text(word)\n",
    "    found_list = []\n",
    "    for group_key in group_dict.keys():\n",
    "        if word in group_dict[group_key]:\n",
    "            found_list.append(group_key)\n",
    "    return found_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2777] 2022-01-06 14:04:02,775 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/model.vec\n",
      "[2777] 2022-01-06 14:05:05,971 Info gensim.utils :KeyedVectors lifecycle event {'msg': 'loaded (351122, 300) matrix of type float32 from ../../corpus/w2v/model.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-01-06T14:05:05.971525', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "# fasttext\n",
    "# https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "# w2v_name =  \"dep-ja-300dim\"\n",
    "w2v_name =  \"model.vec\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "group2index = dict(zip( group_dict.keys(), range(len(group_dict.keys())) ))\n",
    "index2group = dict(zip(range(len(group_dict.keys())), group_dict.keys() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/utterance/case_frame_0.8_2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/utterance/\"\n",
    "model_name = \"case_frame_{0}_2.pickle\".format(border)\n",
    "modelM = DataManager(model_path)\n",
    "cmodel = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_word(word):\n",
    "    word = clean_text(word)\n",
    "    if word not in w2v_model:\n",
    "        return \"\"\n",
    "    with torch.no_grad():\n",
    "        vector = torch.tensor([w2v_model[word]]).cuda()\n",
    "        pred = np.array(cmodel(vector).cpu()).argmax()\n",
    "    \n",
    "    return index2group[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_triple(case_frame:dict, V, C, Noun):\n",
    "    # 動詞の確認\n",
    "    if V not in case_frame:\n",
    "        case_frame[V] = dict()\n",
    "\n",
    "    # 格の確認\n",
    "    if C not in case_frame[V]:\n",
    "        case_frame[V][C] = set()\n",
    "    \n",
    "    # 名詞の登録\n",
    "    case_frame[V][C].add(Noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_noun(token):\n",
    "    base = token.lemma_\n",
    "    for child in token.children:\n",
    "        if child.dep_ == \"compound\":\n",
    "           base = child.lemma_ + base\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_RDF_triple(text):\n",
    "    text = clean_text(text)\n",
    "    doc = nlp(text)\n",
    "    triple_list = []\n",
    "    for i, token in enumerate( doc ):\n",
    "        if token.pos_ in [\"VERB\", \"ADJ\"]:\n",
    "        # if token.pos_==\"VERB\":\n",
    "        # if is_declinable(token):\n",
    "            for c in token.children:\n",
    "                if c.dep_ in [\"nsubj\", \"obj\", \"obl\"]:\n",
    "                    noun = c.lemma_\n",
    "                    for c2 in c.children:\n",
    "                        # if c2.dep_ == \"case\" and c2.orth_ in case_set:\n",
    "                        if c2.dep_ == \"case\":\n",
    "                            case = c2.orth_\n",
    "                            if case == \"は\":\n",
    "                                case = \"が\"\n",
    "                            triple_list.append( (token.lemma_, case, noun) )\n",
    "    return triple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Semantic error\"\n",
    "sys_utt = []\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        # if ut.is_system() and ut.is_exist_error():\n",
    "        if not ut.is_exist_error():\n",
    "            sys_utt.append(ut.utt)\n",
    "            if ut.is_error_included(error):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5016/5016 [00:00<00:00, 1263353.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../corpus/NTT/persona.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    convs = json.load(f)\n",
    "all_utt = []\n",
    "for did in tqdm( convs[\"convs\"] ) :\n",
    "    dids = list( did.keys() )[0]\n",
    "    all_utt += did[dids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63167"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_utt = sys_utt + all_utt\n",
    "# len(all_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61781/61781 [19:26<00:00, 52.98it/s]\n"
     ]
    }
   ],
   "source": [
    "case_frame = dict()\n",
    "for utt in tqdm( all_utt ):\n",
    "    is_valid = True\n",
    "\n",
    "    triples = extract_RDF_triple(utt)\n",
    "    if len(triples)>0:\n",
    "        for triple in triples:\n",
    "            V = triple[0]\n",
    "            C = triple[1]\n",
    "            noun = triple[2]\n",
    "            group_ = search_word(noun)\n",
    "            if len(group_) > 0:\n",
    "                for group in group_:\n",
    "                    register_triple(case_frame, V, C, group)\n",
    "            else:\n",
    "                group = classify_word(noun)\n",
    "                register_triple(case_frame, V, C, group)\n",
    "\n",
    "            register_triple(case_frame, V, C, noun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_frame_name = \"case_frame_V2_ntt_uni.pickle\"\n",
    "data_path = \"../X_y_data/utterance/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/utterance/case_frame_V2_ntt.pickle\n"
     ]
    }
   ],
   "source": [
    "dataM = DataManager(data_path)\n",
    "dataM.save_data(case_frame_name, case_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_triple(V, C, N):\n",
    "    if N in case_frame[V][C]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# True : 用法は問題ない\n",
    "# False : 問題あり\n",
    "def judge_triple(triple):\n",
    "    V = triple[0]\n",
    "    C = triple[1]\n",
    "    noun = triple[2]\n",
    "\n",
    "    # Vが登録されていないならパスしておくかな(一旦)\n",
    "    if V not in case_frame:\n",
    "        # print(\"not registered V :\", V)\n",
    "        return True\n",
    "    \n",
    "    # C が登録されていないなら，アウト\n",
    "    if C not in case_frame[V]:\n",
    "        # print(\"not registered V, C :\", V, C)\n",
    "        return False\n",
    "    \n",
    "    # そのままの名詞で検索 -> 発見なら正例\n",
    "    if search_triple(V, C, noun):\n",
    "        return True\n",
    "\n",
    "    # 名詞がどこかのグループに属する\n",
    "    group_ = search_word(noun)\n",
    "    if len(group_) > 0:\n",
    "        # 1つでも引っかかればOK\n",
    "        for group in group_:\n",
    "            if search_triple(V, C, group):\n",
    "                return True\n",
    "        return False\n",
    "    # グループを推定\n",
    "    else:\n",
    "        group = classify_word(noun) \n",
    "        return search_triple(V, C, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)\n",
    "\n",
    "error = \"Semantic error\"\n",
    "sys_utt = []\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "            sys_utt.append(ut.utt)\n",
    "            if ut.is_error_included(error):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1386/1386 [00:22<00:00, 60.61it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for utt in tqdm(sys_utt):\n",
    "    is_valid = True\n",
    "\n",
    "    triples = extract_RDF_triple(utt)\n",
    "    for triple in triples:\n",
    "        if not judge_triple(triple):\n",
    "            is_valid = False\n",
    "            break\n",
    "    if is_valid:\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[836 542]\n",
      " [  6   2]]\n",
      "accuracy =  0.6046176046176046\n",
      "precision =  0.003676470588235294\n",
      "recall =  0.25\n",
      "f1 score =  0.007246376811594203\n"
     ]
    }
   ],
   "source": [
    "score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習データのみでやってみた\n",
    "\n",
    "        confusion matrix = \n",
    "        [[716 662]\n",
    "        [  1   7]]\n",
    "        accuracy =  0.5216450216450217\n",
    "        precision =  0.01046337817638266\n",
    "        recall =  0.875\n",
    "        f1 score =  0.020679468242245203\n",
    "\n",
    "    - 再現率は良い！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_triple(triple):\n",
    "    V = triple[0]\n",
    "    C = triple[1]\n",
    "    noun = triple[2]\n",
    "\n",
    "    # Vが登録されていないならパスしておくかな(一旦)\n",
    "    if V not in case_frame:\n",
    "        # print(\"not registered V :\", V)\n",
    "        return True\n",
    "    \n",
    "    # C が登録されていないなら，アウト\n",
    "    if C not in case_frame[V]:\n",
    "        # print(\"not registered V, C :\", V, C)\n",
    "        return False\n",
    "    \n",
    "    # そのままの名詞で検索 -> 発見なら正例\n",
    "    if search_triple(V, C, noun):\n",
    "        print(\"\\tfound\", V, C, noun)\n",
    "        return True\n",
    "\n",
    "    # 名詞がどこかのグループに属する\n",
    "    group_ = search_word(noun)\n",
    "    if len(group_) > 0:\n",
    "        # 1つでも引っかかればOK\n",
    "        for group in group_:\n",
    "            if search_triple(V, C, group):\n",
    "                return True\n",
    "        return False\n",
    "    # グループを推定\n",
    "    else:\n",
    "        group = classify_word(noun) \n",
    "        return search_triple(V, C, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元気ですかは元気です\n",
      "--------\n",
      "好きだを見ますよねー\n",
      "('見る', 'を', '好き')\n",
      "False\n",
      "--------\n",
      "病院は治療を受けましょう\n",
      "('受ける', 'が', '病院')\n",
      "True\n",
      "('受ける', 'を', '治療')\n",
      "True\n",
      "--------\n",
      "好きだは好きですか。お寿司はエンガワが好きですね\n",
      "('好き', 'が', '縁側')\n",
      "True\n",
      "--------\n",
      "時期から資格を取りますねぇ\n",
      "('取る', 'から', '時期')\n",
      "False\n",
      "('取る', 'を', '資格')\n",
      "\tfound 取る を 資格\n",
      "True\n",
      "--------\n",
      "手を貯金に出しますねぇ\n",
      "('出す', 'を', '手')\n",
      "\tfound 出す を 手\n",
      "True\n",
      "('出す', 'に', '貯金')\n",
      "True\n",
      "--------\n",
      "ところで、テレビでテレビあるって言ってましたが、テレビは民主党支持が多いですね\n",
      "('ある', 'で', 'テレビ')\n",
      "\tfound ある で テレビ\n",
      "True\n",
      "('言う', 'で', '所')\n",
      "\tfound 言う で 所\n",
      "True\n",
      "('多い', 'が', '支持')\n",
      "True\n",
      "--------\n",
      "旬ですねぇ。自分もオリンピック書いたし。\n",
      "('書く', 'も', '自分')\n",
      "\tfound 書く も 自分\n",
      "True\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for t, p, utt in zip(y, y_pred, sys_utt):\n",
    "    if t==1:\n",
    "        print(utt)\n",
    "        triples = extract_RDF_triple(utt)\n",
    "        for triple in triples:\n",
    "            print(triple)\n",
    "            print( judge_triple(triple) )\n",
    "        print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
