{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from datatools.maneger import DataManager\n",
    "# from datatools.analyzer import clean_text\n",
    "from error_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm        import Vocabulary\n",
    "# from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.util      import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/utterance/KLM_phrase_nucc_ntt_wiki_n=3_splited_unidic.pickle\n"
     ]
    }
   ],
   "source": [
    "from datatools.maneger import DataManager\n",
    "n=3\n",
    "modelM = DataManager(\"../models/utterance/\")\n",
    "model_name = \"KLM_phrase_wiki_n={0}_splited.pickle\".format(n)\n",
    "model_name = \"KLM_phrase_nucc_ntt_n={0}_splited.pickle\".format(n)\n",
    "model_name = \"KLM_phrase_nucc_ntt_wiki_n={0}_splited.pickle\".format(n)\n",
    "model_name = \"KLM_phrase_nucc_ntt_wiki_n={0}_splited_unidic.pickle\".format(n)\n",
    "lm = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)\n",
    "\n",
    "error = \"Grammatical error\"\n",
    "errors = ['Grammatical error', \"Uninterpretable\"]\n",
    "sys_utt = []\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "            if not ut.utt[-1] in [\"？\", \"！\", \"。\", \"!\"]:\n",
    "                sys_utt.append( clean_text( ut.utt+\"。\" ))\n",
    "                # sys_utt.append(ut.utt)\n",
    "            else:   \n",
    "                sys_utt.append(clean_text( ut.utt ))\n",
    "            if ut.is_error_included(errors):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_analyzer = Analyzer(tokenizer_)\n",
    "mecab_analyzer = Analyzer(Tokenizer())\n",
    "\n",
    "def last_pos(pos_split):\n",
    "    last = len(pos_split) - 1 \n",
    "    for i, _ in enumerate(pos_split):\n",
    "        if pos_split[last-i] != \"*\":\n",
    "            return \"[\"+pos_split[last-i]+\"]\"\n",
    "# def sentence2normalize_noun_mecab(sentences):\n",
    "#     normalize_sen = []\n",
    "#     if isinstance(sentences, str):\n",
    "#         sentences = [sentences]\n",
    "#     for sen in sentences:\n",
    "#         df = mecab_analyzer.analyze_with_dataframe(sen)\n",
    "#         words = []\n",
    "#         for txt, pos in zip(df.surface, df.part_of_speech):\n",
    "#             pos_split = pos.split(\"-\")\n",
    "#             # if pos_split[0]\n",
    "#             # if pos_split[1] != \"サ変接続\" and pos_split[1] != \"非自立\" and pos_split[1] != \"接尾\" and \"名詞\" == pos_split[0]:\n",
    "#             if pos_split[1] == \"固有名詞\" or pos_split[1] == \"一般\":\n",
    "#                 # words.append(pos)\n",
    "#                 words.append(last_pos(pos_split))\n",
    "#             else:\n",
    "#                 words.append(txt)\n",
    "#         normalize_sen.append(words)\n",
    "#     return normalize_sen\n",
    "\n",
    "def sentence2normalize_noun_mecab(sentences):\n",
    "    normalize_sen = []\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]\n",
    "    for sen in sentences:\n",
    "        df = mecab_analyzer.analyze_with_dataframe(sen)\n",
    "        words = []\n",
    "        if df is None:\n",
    "            continue\n",
    "        for txt, pos in zip(df.surface, df.part_of_speech):\n",
    "            pos_split = pos.split(\"-\")\n",
    "            # if pos_split[0]\n",
    "            # if pos_split[1] != \"サ変接続\" and pos_split[1] != \"非自立\" and pos_split[1] != \"接尾\" and \"名詞\" == pos_split[0]:\n",
    "            print(pos_split)\n",
    "            if pos_split[0]==\"名詞\" and ( pos_split[1] == \"固有名詞\" or pos_split[1] == \"一般\"):\n",
    "                # words.append(pos)\n",
    "                # print(txt, pos)\n",
    "                words.append(last_pos(pos_split))\n",
    "            else:\n",
    "                words.append(txt)\n",
    "        normalize_sen.append(words)\n",
    "    return normalize_sen\n",
    "\n",
    "\n",
    "mecab_analyzer = Analyzer(Tokenizer())\n",
    "\n",
    "def last_pos(pos_split, filler=\"*\"):\n",
    "    last = len(pos_split) - 1 \n",
    "    for i, _ in enumerate(pos_split):\n",
    "        if pos_split[last-i] != filler :\n",
    "            return \"[\"+pos_split[last-i]+\"]\"\n",
    "def sentence2normalize_noun_mecab(sentences):\n",
    "    normalize_sen = []\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]\n",
    "    for sen in sentences:\n",
    "        df = mecab_analyzer.analyze_with_dataframe(sen)\n",
    "        words = []\n",
    "        if df is None:\n",
    "            continue\n",
    "        for txt, pos in zip(df.surface, df.part_of_speech):\n",
    "            pos_split = pos.split(\"-\")\n",
    "            # if pos_split[0]\n",
    "            # if pos_split[1] != \"サ変接続\" and pos_split[1] != \"非自立\" and pos_split[1] != \"接尾\" and \"名詞\" == pos_split[0]:\n",
    "            if pos_split[0]==\"名詞\":\n",
    "                # words.append(pos)\n",
    "                # print(txt, pos)\n",
    "                last_pos_name = last_pos(pos_split, filler=\"\")\n",
    "                if last_pos_name==\"[サ変可能]\":\n",
    "                    words.append(txt)\n",
    "                else:\n",
    "                    words.append(last_pos_name)\n",
    "            else:\n",
    "                words.append(txt)\n",
    "        normalize_sen.append(words)\n",
    "    return normalize_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pprint\n",
    "\n",
    "\n",
    "def sentence2score(sentence, lm, N, is_print=False):\n",
    "    # filled = fill_SYMBOL( sentence2normalize_independent(sentence) )\n",
    "    # filled = fill_SYMBOL_ONE( sentence2normalize_noun_mecab(sentence) )\n",
    "    filled = sentence2normalize_noun_mecab(sentence)\n",
    "    normal_split = [ list(\"\".join(L)) for L in filled ]\n",
    "    # filled = fill_SYMBOL( sentence2morpheme(sentence) )\n",
    "    # filled_pos = fill_SYMBOL_ONE( sentence2pos_mecab(sentence) )\n",
    "    # print(filled)\n",
    "    # print(filled_pos)\n",
    "    ngram_text = []\n",
    "    ngram_pos = []\n",
    "    # for L, P in zip(filled):\n",
    "    for L in normal_split:\n",
    "        for i in range(len(L)-N+1):\n",
    "            # print(L[i:i+N])\n",
    "            ngram_text.append(L[i:i+N])\n",
    "            # ngram_pos.append(P[i:i+N])\n",
    "    # pprint.pprint(ngram_text)\n",
    "    all_score = 0\n",
    "    function_score = 0\n",
    "    # デフォルトで1\n",
    "    function_num = 1\n",
    "\n",
    "    under = 1 / (1000*len( lm.vocab.counts ) )\n",
    "\n",
    "    min_score = 0\n",
    "\n",
    "    # for ngram, pgram in zip(ngram_text, ngram_pos):\n",
    "    for ngram in ngram_text:\n",
    "        context = (ngram[:-1])\n",
    "        # context_pos = pgram[:-1]\n",
    "        # print(context)\n",
    "        # for word in lm.context_counts(lm.vocab.lookup(context)): # 文脈に続く単語一覧の取得\n",
    "            \n",
    "        score = lm.score(ngram[-1], context) + under\n",
    "        log_score = math.log2(score)\n",
    "        if is_print:\n",
    "            print(\"context : {0}|{1} ->\".format(context, ngram[-1:]), log_score)\n",
    "        if log_score < min_score:\n",
    "            min_score = log_score\n",
    "\n",
    "        # if \"助動詞\" in context_pos[1] or \"助詞\" in context_pos[1] or \"助動詞\" in context_pos[0] or \"助詞\" in context_pos[0]:\n",
    "        #     # print(\"\\tcontext : {0}| ->\".format(context), log_score)\n",
    "        #     function_score += log_score\n",
    "        #     function_num += 1\n",
    "\n",
    "        all_score += log_score\n",
    "    # print(all_score/len(ngram_text))\n",
    "    # return all_score/len(ngram_text)\n",
    "    return  min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 480.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context : ['熱', '中']|['症'] -> -1.8407896861385864\n",
      "context : ['中', '症']|['に'] -> -0.3576363069212646\n",
      "context : ['症', 'に']|['['] -> -2.0228209876331347\n",
      "context : ['に', '[']|['一'] -> -0.4191514837509937\n",
      "context : ['[', '一']|['般'] -> 3.433652350548337e-07\n",
      "context : ['一', '般']|[']'] -> 3.433652350548337e-07\n",
      "context : ['般', ']']|['を'] -> -3.7795003803728964\n",
      "context : [']', 'を']|['つ'] -> -6.351459545586087\n",
      "context : ['を', 'つ']|['け'] -> -0.5926746717796643\n",
      "context : ['つ', 'け']|['か'] -> -8.711270979131637\n",
      "context : ['け', 'か']|['?'] -> -3.628992130214225\n",
      "context : ['か', '?']|['?'] -> -7.329583962284409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.711270979131637"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"熱中症に気をつけか??\"\n",
    "# sentence = \"最近とても暑いですから。\"\n",
    "# sentence = \"仲間由紀恵は怖い\"\n",
    "sentence2score(clean_text(sentence), lm, N=n, is_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 293.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['熱中', '症', 'に', '[一般]', 'を', 'つけ', 'か', '?', '?']]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2normalize_noun_mecab(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1386/1386 [00:02<00:00, 472.16it/s]\n"
     ]
    }
   ],
   "source": [
    "y_scores = []\n",
    "for utt in tqdm(sys_utt):\n",
    "    y_scores.append(sentence2score(utt, lm, N=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_plain(test, pred):\n",
    "    return f1_score(y_true=test, y_pred=pred)\n",
    "\n",
    "def recall_score_plain(test, pred):\n",
    "    return recall_score(y_true=test, y_pred=pred)\n",
    "\n",
    "def precision_score_plain(test, pred):\n",
    "    return precision_score(y_true=test, y_pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [00:00<00:01, 607.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLM_phrase_nucc_ntt_wiki_n=3_splited_unidic.pickle\n",
      "f1 score:0.010471204188481674, border:-5.0\n",
      "f1 score:0.010510510510510511, border:-5.1\n",
      "f1 score:0.01051840721262209, border:-5.2\n",
      "f1 score:0.010534236267870579, border:-5.3\n",
      "f1 score:0.01061410159211524, border:-5.4\n",
      "f1 score:0.010630220197418374, border:-5.5\n",
      "f1 score:0.010654490106544902, border:-5.6\n",
      "f1 score:0.010687022900763359, border:-5.7\n",
      "f1 score:0.010760953112990006, border:-5.8\n",
      "f1 score:0.01078582434514638, border:-5.9\n",
      "f1 score:0.010819165378670788, border:-6.0\n",
      "f1 score:0.010920436817472701, border:-6.1\n",
      "f1 score:0.010989010989010988, border:-6.2\n",
      "f1 score:0.011014948859166013, border:-6.3\n",
      "f1 score:0.011182108626198084, border:-6.4\n",
      "f1 score:0.011217948717948718, border:-6.5\n",
      "f1 score:0.011235955056179775, border:-6.6\n",
      "f1 score:0.011336032388663968, border:-6.7\n",
      "f1 score:0.01138211382113821, border:-6.8\n",
      "f1 score:0.011494252873563216, border:-6.9\n",
      "f1 score:0.011527377521613832, border:-12.100000000000001\n",
      "f1 score:0.011661807580174927, border:-12.2\n",
      "f1 score:0.011782032400589101, border:-12.3\n",
      "f1 score:0.01183431952662722, border:-12.4\n",
      "f1 score:0.011922503725782416, border:-12.5\n",
      "f1 score:0.0121765601217656, border:-12.600000000000001\n",
      "f1 score:0.012326656394453005, border:-12.7\n",
      "f1 score:0.012461059190031154, border:-12.8\n",
      "f1 score:0.012618296529968456, border:-12.9\n",
      "f1 score:0.012759170653907496, border:-13.0\n",
      "f1 score:0.012903225806451615, border:-13.1\n",
      "f1 score:0.012944983818770227, border:-13.200000000000001\n",
      "f1 score:0.0130718954248366, border:-13.3\n",
      "f1 score:0.013201320132013203, border:-13.4\n",
      "f1 score:0.013377926421404682, border:-13.5\n",
      "f1 score:0.01342281879194631, border:-13.6\n",
      "f1 score:0.013468013468013467, border:-13.700000000000001\n",
      "f1 score:0.013559322033898305, border:-13.8\n",
      "f1 score:0.01360544217687075, border:-13.9\n",
      "f1 score:0.013769363166953527, border:-14.0\n",
      "f1 score:0.013913043478260868, border:-14.1\n",
      "f1 score:0.014035087719298248, border:-14.200000000000001\n",
      "f1 score:0.014059753954305799, border:-14.3\n",
      "f1 score:0.014109347442680775, border:-14.5\n",
      "f1 score:0.014159292035398232, border:-14.600000000000001\n",
      "f1 score:0.014336917562724013, border:-14.700000000000001\n",
      "f1 score:0.015122873345935728, border:-14.9\n",
      "f1 score:0.015180265654648958, border:-15.0\n",
      "f1 score:0.015296367112810707, border:-15.100000000000001\n",
      "f1 score:0.015325670498084292, border:-15.200000000000001\n",
      "f1 score:0.015444015444015443, border:-15.3\n",
      "f1 score:0.015594541910331385, border:-15.4\n",
      "f1 score:0.015810276679841896, border:-15.5\n",
      "f1 score:0.016129032258064516, border:-15.600000000000001\n",
      "f1 score:0.0163265306122449, border:-15.700000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 178/1000 [00:00<00:01, 543.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:0.01663201663201663, border:-15.8\n",
      "f1 score:0.01702127659574468, border:-15.9\n",
      "f1 score:0.017582417582417582, border:-16.0\n",
      "f1 score:0.017937219730941704, border:-16.1\n",
      "f1 score:0.018475750577367205, border:-16.200000000000003\n",
      "f1 score:0.01900237529691211, border:-16.3\n",
      "f1 score:0.01904761904761905, border:-17.200000000000003\n",
      "f1 score:0.019801980198019802, border:-17.3\n",
      "f1 score:0.020477815699658702, border:-17.4\n",
      "f1 score:0.021428571428571432, border:-17.5\n",
      "f1 score:0.022641509433962263, border:-17.6\n",
      "f1 score:0.022988505747126436, border:-18.5\n",
      "f1 score:0.024999999999999998, border:-18.6\n",
      "f1 score:0.026666666666666665, border:-18.700000000000003\n",
      "f1 score:0.028169014084507043, border:-18.8\n",
      "f1 score:0.03571428571428571, border:-19.8\n",
      "f1 score:0.0392156862745098, border:-20.0\n",
      "f1 score:0.04, border:-20.1\n",
      "f1 score:0.043478260869565216, border:-20.200000000000003\n",
      "f1 score:0.05714285714285715, border:-20.3\n",
      "f1 score:0.06451612903225806, border:-20.4\n",
      "f1 score:0.06666666666666667, border:-20.5\n",
      "f1 score:0.07692307692307693, border:-20.6\n",
      "f1 score:0.09090909090909091, border:-20.700000000000003\n",
      "f1 score:0.10526315789473685, border:-20.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 663.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "base = -5\n",
    "epoch = 1000\n",
    "# y_pred = np.zeros(len(y)) + 1\n",
    "y_pred = np.zeros(len(y))\n",
    "max_f1 = 0\n",
    "\n",
    "print(model_name)\n",
    "for e in tqdm(range(epoch)):\n",
    "    y_pred = np.zeros(len(y))\n",
    "\n",
    "    for i, score_ in enumerate(y_scores):\n",
    "        # border 未満をエラーでとする\n",
    "        border = (base - 0.1*e)\n",
    "        if score_ < border :\n",
    "            y_pred[i] = 1\n",
    "    f1 = f1_score_plain(y, y_pred)\n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        print(\"f1 score:{0}, border:{1}\".format(max_f1, border))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = np.zeros(len(y))\n",
    "\n",
    "for i, score_ in enumerate(y_scores):\n",
    "    if score_ < -17 :\n",
    "        y_pred2[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1052  327]\n",
      " [   4    3]]\n",
      "accuracy =  0.7611832611832612\n",
      "precision =  0.00909090909090909\n",
      "recall =  0.42857142857142855\n",
      "f1 score =  0.01780415430267062\n"
     ]
    }
   ],
   "source": [
    "score(y, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "熱中症に気をつけか??\n",
      "熱中症に気をつけか??\n",
      "withha。\n"
     ]
    }
   ],
   "source": [
    "for p, y_ , utt in zip(y_pred2, y, sys_utt):\n",
    "    if y_==1 and p==0:\n",
    "        print(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
