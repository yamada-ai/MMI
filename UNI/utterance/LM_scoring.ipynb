{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm        import Vocabulary\n",
    "# from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.util      import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)\n",
    "\n",
    "error = \"Grammatical error\"\n",
    "errors = ['Grammatical error', \"Uninterpretable\"]\n",
    "sys_utt = []\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "            if not ut.utt[-1] in [\"？\", \"！\", \"。\", \"!\"]:\n",
    "                # sys_utt.append(ut.utt+\"。\")\n",
    "                sys_utt.append(ut.utt)\n",
    "            else:   \n",
    "                sys_utt.append(ut.utt)\n",
    "            if ut.is_error_included(errors):\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of y:1349, error 'Grammatical error' counts:1345\n"
     ]
    }
   ],
   "source": [
    "print(\"len of y:{0}, error '{1}' counts:{2}\".format(len(y), error, y.count(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/utterance/KLM_phrase_nucc_n=3_noun.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM = DataManager(\"../models/utterance/\")\n",
    "# model_name = \"KLM_nucc.pickle\"\n",
    "# model_name = \"KLM_phrase.pickle\"\n",
    "n = 3\n",
    "model_name = \"KLM_phrase_n={0}.pickle\".format(n)\n",
    "# model_name = \"KLM_phrase_n={0}_noun.pickle\".format(n)\n",
    "model_name = \"KLM_phrase_nucc_n={0}.pickle\".format(n)\n",
    "model_name = \"KLM_phrase_nucc_n={0}_noun.pickle\".format(n)\n",
    "lm = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pprint\n",
    "def sentence2score(sentence, l, N):\n",
    "    # filled = fill_SYMBOL( sentence2normalize_independent(sentence) )\n",
    "    filled = fill_SYMBOL( sentence2normalize_noun(sentence) )\n",
    "    filled_pos = fill_SYMBOL( sentence2pos(sentence) )\n",
    "    # print(filled)\n",
    "    # print(filled_pos)\n",
    "    ngram_text = []\n",
    "    ngram_pos = []\n",
    "    for L, P in zip(filled,filled_pos):\n",
    "        for i in range(len(L)-N+1):\n",
    "            # print(L[i:i+N])\n",
    "            ngram_text.append(L[i:i+N])\n",
    "            ngram_pos.append(P[i:i+N])\n",
    "    # pprint.pprint(ngram_text)\n",
    "    all_score = 0\n",
    "    function_score = 0\n",
    "    # デフォルトで1\n",
    "    function_num = 1\n",
    "    for ngram, pgram in zip(ngram_text, ngram_pos):\n",
    "        context = (ngram[:-1])\n",
    "        context_pos = pgram[:-1]\n",
    "        # print(context)\n",
    "        # for word in lm.context_counts(lm.vocab.lookup(context)): # 文脈に続く単語一覧の取得\n",
    "            \n",
    "        score = lm.score(ngram[-1], context) + 1e-4\n",
    "        log_score = math.log2(score)\n",
    "        # print(\"context : {0}|{1} ->\".format(context, ngram[-1:]), log_score)\n",
    "\n",
    "        if \"助動詞\" in context_pos[1] or \"助詞\" in context_pos[1] or \"助動詞\" in context_pos[0] or \"助詞\" in context_pos[0]:\n",
    "            # print(\"\\tcontext : {0}| ->\".format(context), log_score)\n",
    "            function_score += log_score\n",
    "            function_num += 1\n",
    "\n",
    "        all_score += log_score\n",
    "    # print(all_score/len(ngram_text))\n",
    "    # return all_score/len(ngram_text)\n",
    "    return function_score/function_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.6100002077774005"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentence = \"最近とても暑いですから。\"\n",
    "# sentence = \"魚はおいしいんですか？？海は素晴らしいですね。\"\n",
    "# sentence = \"魚はおいんですか？？海は素晴らしいですね。\"\n",
    "sentence2score(sentence, lm, N=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ご存知ですいます。:-5.243564695392782\n",
      "魚はおいんですか？？海は素晴らしいですね。:-6.645624200019985\n",
      "もちろんですってのが元気ですかにいくないですよ。:-6.351135445510097\n",
      "熱中症に気をつけか？？:-6.889447202555353\n"
     ]
    }
   ],
   "source": [
    "error_samples = [['ご存知ですいます。'],\n",
    " ['魚はおいんですか？？海は素晴らしいですね。'],\n",
    " ['もちろんですってのが元気ですかにいくないですよ。'],\n",
    " ['熱中症に気をつけか？？']]\n",
    "\n",
    "for s in error_samples:\n",
    "    print(\"{0}:{1}\".format(s[0], sentence2score(s[0], lm, N=n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N=4 gram の全品詞，全単語\n",
    "        \n",
    "        ご存知ですいます:-8.056322000232084\n",
    "        魚はおいんですか？？海は素晴らしいですね:-3.628491577687402\n",
    "        もちろんですってのが元気ですかにいくないですよ:-8.338289348723574\n",
    "        熱中症に気をつけか？？:-3.\n",
    "\n",
    "- N=4 で助詞，助動詞の前後\n",
    "\n",
    "        ご存知ですいます:-4.654287917265429\n",
    "        魚はおいんですか？？海は素晴らしいですね:-3.56763363850194\n",
    "        もちろんですってのが元気ですかにいくないですよ:-7.488756600321924\n",
    "        熱中症に気をつけか？？:-3.9569609593484123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N=4 gram の全品詞，全単語\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoreを入れておこう\n",
    "y_scores = []\n",
    "for utt in sys_utt:\n",
    "    y_scores.append(sentence2score(utt, lm, N=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLM_phrase_nucc_n=3_noun.pickle\n",
      "confusion matrix = \n",
      " [[   4    0]\n",
      " [ 126 1219]]\n",
      "accuracy =  0.9065974796145293\n",
      "precision =  1.0\n",
      "recall =  0.9063197026022305\n",
      "f1 score =  0.9508580343213728\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "border = -6.8\n",
    "epoch = 1\n",
    "y_pred = np.zeros(len(y)) + 1\n",
    "max_precision = 0\n",
    "\n",
    "print(model_name)\n",
    "for e in range(epoch):\n",
    "    # y_pred = np.zeros(len(y))\n",
    "    y_pred = np.zeros(len(y)) + 1\n",
    "    for i, score in enumerate(y_scores):\n",
    "        # border 未満をエラーでとする\n",
    "        if score < border + 0.01*e :\n",
    "            y_pred[i] = 0\n",
    "            # print(sys_utt[i])\n",
    "        # precision = metrics.precision_score(y, y_pred)\n",
    "\n",
    "    # print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "    # print(border + 0.01*e)\n",
    "    # print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "    # print()\n",
    "\n",
    "\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_pred).count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- corpus をゴガクルにした場合\n",
    "\n",
    "        confusion matrix = \n",
    "        [[1300   45]\n",
    "        [   2    2]]\n",
    "        confusion matrix = \n",
    "        [[1206  139]\n",
    "        [   2    2]]\n",
    "        confusion matrix = \n",
    "        [[961 384]\n",
    "        [  1   3]]\n",
    "        confusion matrix = \n",
    "        [[363 982]\n",
    "        [  0   4]]\n",
    "        confusion matrix = \n",
    "        [[  86 1259]\n",
    "        [   0    4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 助詞，助動詞の直後のみ\n",
    "\n",
    "        confusion matrix = \n",
    "        [[888 457]\n",
    "        [  3   1]]\n",
    "        confusion matrix = \n",
    "        [[1135  210]\n",
    "        [   4    0]]\n",
    "        confusion matrix = \n",
    "        [[1282   63]\n",
    "        [   4    0]]\n",
    "        confusion matrix = \n",
    "        [[1329   16]\n",
    "        [   4    0]]\n",
    "        confusion matrix = \n",
    "        [[1343    2]\n",
    "        [   4    0]]\n",
    "\n",
    "- 名詞を正規化\n",
    "\n",
    "        confusion matrix = \n",
    "        [[1175  170]\n",
    "        [   1    3]]\n",
    "        confusion matrix = \n",
    "        [[1127  218]\n",
    "        [   0    4]]\n",
    "        confusion matrix = \n",
    "        [[1041  304]\n",
    "        [   0    4]]\n",
    "        confusion matrix = \n",
    "        [[921 424]\n",
    "        [  0   4]]\n",
    "        confusion matrix = \n",
    "        [[765 580]\n",
    "        [  0   4]]\n",
    "        confusion matrix = \n",
    "        [[568 777]\n",
    "        [  0   4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KLM_phrase_nucc_n=3_noun.pickle / return func\n",
    "\n",
    "        confusion matrix = \n",
    "        [[1233  112]\n",
    "        [   2    2]]\n",
    "        confusion matrix = \n",
    "        [[1195  150]\n",
    "        [   0    4]]\n",
    "        confusion matrix = \n",
    "\n",
    "- border = -6.7 / KLM_phrase_nucc_n=3_noun.pickle\n",
    "\n",
    "        confusion matrix = \n",
    "        [[   4    0]\n",
    "        [ 126 1219]]\n",
    "        accuracy =  0.9065974796145293\n",
    "        precision =  1.0\n",
    "        recall =  0.9063197026022305\n",
    "        f1 score =  0.9508580343213728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
