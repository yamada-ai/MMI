{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "# from datatools.analyzer import clean_text\n",
    "from error_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm        import Vocabulary\n",
    "# from nltk.lm.models import MLE\n",
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.util      import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_utt = []\n",
    "for conv in convs:\n",
    "    for i, ut in enumerate(conv):\n",
    "        if not ut.is_exist_error():\n",
    "            conv_utt.append(clean_text(ut.utt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_utt_ntt():\n",
    "    ntt_path = \"../../corpus/NTT/\"\n",
    "    utt_list = []\n",
    "    for file_ in os.listdir(ntt_path):\n",
    "        if not \"json\" in file_:\n",
    "            continue \n",
    "        with open(ntt_path+file_, \"r\",  encoding=\"utf-8\") as f:\n",
    "            convs = json.load(f)\n",
    "            for did in convs[\"convs\"]:\n",
    "                dids = list( did.keys() )[0]\n",
    "                conv = did[dids]\n",
    "                # utt_list.extend( [ clean_text(utt)  for utt in conv])\n",
    "                utt_list.extend( [utt  for utt in conv])\n",
    "    print(len(utt_list))\n",
    "    return utt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141777\n"
     ]
    }
   ],
   "source": [
    "ntt_utt = load_utt_ntt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntt_span_utt = [str(doc) for doc in sentence2docs(ntt_utt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, batch_size=10000) -> list:\n",
    "    f = open(filename, 'r', encoding='UTF-8')\n",
    "    text_list = [  ]\n",
    "    concated = \"\"\n",
    "    for i, line in enumerate( f.readlines() ):\n",
    "        concated += line.rstrip('\\n')\n",
    "        if (i+1) % batch_size == 0:\n",
    "            text_list.append(concated)\n",
    "            concated = \"\"\n",
    "    text_list.append(concated)\n",
    "    f.close()\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of text_list : 18674\n"
     ]
    }
   ],
   "source": [
    "filename = \"../../corpus/wiki/wiki_40b_train_normal.txt\"\n",
    "all_list = read_file(filename, batch_size=100)\n",
    "print(\"len of text_list :\", len(all_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18674/18674 [00:02<00:00, 8556.43it/s]\n"
     ]
    }
   ],
   "source": [
    "all_wiki = []\n",
    "for line in tqdm(all_list):\n",
    "    all_wiki += [sen+\"。\" for sen in line.split(\"。\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_analyzer = Analyzer(tokenizer_)\n",
    "# 一文ずつしか送られて来ない読み\n",
    "def sentence2normalize_noun_mecab(sentences):\n",
    "    normalize_sen = []\n",
    "    for sen in sentences:\n",
    "        df = mecab_analyzer.analyze_with_dataframe(sen)\n",
    "        words = []\n",
    "        for txt, pos in zip(df.surface, df.part_of_speech):\n",
    "            # pos_split = pos.split(\"-\")\n",
    "            # if pos_split[0]\n",
    "            if \"名詞\" in pos:\n",
    "                words.append(pos)\n",
    "            else:\n",
    "                words.append(txt)\n",
    "        normalize_sen.append(words)\n",
    "    return normalize_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_utt = ntt_span_utt + all_wiki[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_normal = fill_SYMBOL_ONE( sentence2normalize_noun_mecab(all_utt) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_language_model(sentences, N):\n",
    "    vocab = Vocabulary([word for sent in sentences for word in sent])\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sentences]\n",
    "    lm = KneserNeyInterpolated(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "lm = create_language_model(filled_normal, N=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatools.maneger import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelM = DataManager(\"../models/utterance/\")\n",
    "# model_name = \"KLM_phrase_n={0}.pickle\".format(n)\n",
    "# model_name = \"KLM_phrase_nucc_n={0}.pickle\".format(n)\n",
    "# model_name = \"KLM_phrase_nucc_n={0}_orth.pickle\".format(n)\n",
    "model_name = \"KLM_phrase_nucc_wiki_n={0}_noun1.pickle\".format(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/utterance/KLM_phrase_nucc_wiki_n=3_noun1.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM.save_data(model_name, lm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
