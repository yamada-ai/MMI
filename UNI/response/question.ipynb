{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from tools.maneger import DataManager\n",
    "from tools.preproc import Preprocessor\n",
    "# from utterance.feature import Feature\n",
    "from feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pre = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/response/typeClassify_F.dill\n"
     ]
    }
   ],
   "source": [
    "F_path = \"../X_y_data/response/\"\n",
    "F_name = \"typeClassify_F.dill\"\n",
    "featureM = DataManager(F_path, format_=\"dill\")\n",
    "\n",
    "Fe = featureM.load_data(F_name)\n",
    "Fe.set_preprocessor(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response/typeClassify_M.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response/\"\n",
    "model_name = \"typeClassify_M.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "LR = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True, bidirectional=True )\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        bilstm_out = torch.cat([hidden_layer[0][0], hidden_layer[0][1]], dim=1)\n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        y = self.hidden2tag(bilstm_out)\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question1_M.pickle\n",
      "success load : ../models/response/question1_M.pickle\n"
     ]
    }
   ],
   "source": [
    "Model_path = \"../models/response/\"\n",
    "Model_name = \"question1_M.pickle\"\n",
    "ModelM = DataManager(model_path)\n",
    "print(Model_name)\n",
    "Model = ModelM.load_data(Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "# datalist = ['DCM']\n",
    "    # List of error types\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    "    'Ignore question', 'Topic transition error', \n",
    "    'Lack of information', 'Repetition', \n",
    "    'Contradiction', 'Self-contradiction',\n",
    "    'Lack of common sense', 'Semantic error',\n",
    "    'Grammatical error', 'Ignore proposal', \n",
    "    'Ignore offer', 'Lack of sociality', \n",
    "    'Uninterpretable', 'Ignore greeting', \n",
    "    'No-Err']\n",
    "df = pre.read_json_with_NoErr(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_str_y(df, errors):\n",
    "    X_str = []\n",
    "    y = []\n",
    "    y = np.zeros(len(df))\n",
    "    for i, (u, s, e) in enumerate(zip(df.usr, df.sys, df.ec)):\n",
    "        # sentence_vectors = Nmodel.encode([u, s])\n",
    "        X_str.append([u, s])\n",
    "        for err in errors:\n",
    "            if err in e:\n",
    "                y[i] = 1\n",
    "\n",
    "    return X_str, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [\"Ignore question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(768, 1536, batch_first=True, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=3072, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str, y = make_X_str_y(df, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(X_str, Fe: Feature, lr, Model, lr_mode=\"proba\", mode=\"dif\"):\n",
    "    X = []\n",
    "    doc_vectors = []\n",
    "    # doc_vectors = []\n",
    "    for conv in X_str:\n",
    "        sentence_vectors = Nmodel.encode(conv)\n",
    "        doc_vectors.append(sentence_vectors)\n",
    "        diff = sentence_vectors[0] - sentence_vectors[1]\n",
    "        if np.linalg.norm(diff) == 0:\n",
    "            diff_norm = diff\n",
    "        else:\n",
    "            diff_norm = diff/np.linalg.norm(diff)\n",
    "        # ユーザ発話の特徴\n",
    "        f = Fe.featurization(conv[0])\n",
    "        if lr_mode == \"proba\":\n",
    "            p = lr.predict_proba(f.reshape(1,-1))\n",
    "            is_ques = p[0]\n",
    "        elif lr_mode == \"pred\":\n",
    "            p = lr.predict(f.reshape(1,-1))\n",
    "            if p == 0 or p == 1 :\n",
    "                is_ques = np.array([1])\n",
    "            else:\n",
    "                is_ques = np.array([0])\n",
    "        \n",
    "        X.append( np.concatenate([diff_norm, is_ques]) )\n",
    "    \n",
    "    X = np.array(X)\n",
    "    # 後ろ向き発話具合\n",
    "    with torch.no_grad():\n",
    "        # for i  in range\n",
    "        # back_x = torch.tensor( doc_vectors , device='cuda:0').float()\n",
    "        back_x = torch.tensor( doc_vectors).float()\n",
    "        back_y = np.array(Model(back_x)).argmax(axis=1)\n",
    "    \n",
    "    X = np.block([X, back_y.reshape(-1, 1)])\n",
    "    del doc_vectors\n",
    "    return np.asarray(np.nan_to_num(X), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = make_X(X_str, Fe, LR, Model, lr_mode=\"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_D1.pickle\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../X_y_data/response/\"\n",
    "data_name = \"question_D1.pickle\"\n",
    "dataM = DataManager(data_path)\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataM.is_exist(data_name):\n",
    "#     DATA_Xy = dataM.load_data(data_name)\n",
    "#     X = DATA_Xy[0]\n",
    "#     y = DATA_Xy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/response/question_D1.pickle\n"
     ]
    }
   ],
   "source": [
    "dataM.save_data(data_name, [X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='sag')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=1000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[328  18]\n",
      " [ 22  32]]\n",
      "accuracy =  0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix = \n",
    " [[332  14]\n",
    " [ 33  21]]\n",
    "accuracy =  0.8825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrent_n: 32\n",
      "all_tp_one: 54\n",
      "rate c: 0.5925925925925926\n",
      "rate bad: 0.865\n"
     ]
    }
   ],
   "source": [
    "correct_n = 0\n",
    "all_tp_one = 0\n",
    "for t, n in zip(y_test, y_pred):\n",
    "    if t == 1:\n",
    "        all_tp_one += 1\n",
    "        if n == 1:\n",
    "            correct_n += 1\n",
    "print(\"corrent_n:\", correct_n)\n",
    "print(\"all_tp_one:\", all_tp_one)\n",
    "print(\"rate c:\", correct_n/all_tp_one)\n",
    "print(\"rate bad:\",  (len(y_pred)-all_tp_one)/len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_X2(X_str, Fe: Feature, lr , lr_mode=\"proba\"):\n",
    "#     X = []\n",
    "#     # doc_vectors = []\n",
    "#     for conv in X_str:\n",
    "#         sentence_vectors = Nmodel.encode(conv)\n",
    "#         doc_vectors.append(sentence_vectors)\n",
    "#         diff = sentence_vectors[0] - sentence_vectors[1]\n",
    "#         if np.linalg.norm(diff) == 0:\n",
    "#             diff_norm = diff\n",
    "#         else:\n",
    "#             diff_norm = diff/np.linalg.norm(diff)\n",
    "#         # ユーザ発話の特徴\n",
    "#         f = Fe.featurization(conv[0])\n",
    "#         if lr_mode == \"proba\":\n",
    "#             is_ques = lr.predict_proba(f.reshape(1,-1))\n",
    "#         elif lr_mode == \"pred\":\n",
    "#             is_ques = lr.predict(f.reshape(1,-1))\n",
    "        \n",
    "#         X.append( np.concatenate([diff_norm, is_ques]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "leng = len(df)\n",
    "y_rule = np.zeros(leng)\n",
    "users_f =[ Fe.featurization(t) for t in X_str[:leng]]\n",
    "is_ques = LR.predict(users_f)\n",
    "vec_test = [ Nmodel.encode(t) for t in X_str[:leng]]\n",
    "with torch.no_grad():\n",
    "    back_x = torch.tensor( vec_test).float()\n",
    "    back_y = np.array(Model(back_x)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (q, b) in enumerate(zip(is_ques, back_y)):\n",
    "    if (q==0 or q == 1) and b == 1:\n",
    "        y_rule[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1544  150]\n",
      " [ 124  182]]\n",
      "accuracy =  0.863\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y[:leng], y_pred=y_rule))\n",
    "print('accuracy = ', accuracy_score(y_true=y[:leng], y_pred=y_rule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix = \n",
    " [[1510  184]\n",
    " [ 149  157]]\n",
    "accuracy =  0.8335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrent_n: 182\n",
      "all_tp_one: 306\n",
      "rate c: 0.5947712418300654\n",
      "rate bad: 0.847\n"
     ]
    }
   ],
   "source": [
    "correct_n = 0\n",
    "all_tp_one = 0\n",
    "for t, n in zip(y[:leng], y_rule):\n",
    "    if t == 1:\n",
    "        all_tp_one += 1\n",
    "        if n == 1:\n",
    "            correct_n += 1\n",
    "print(\"corrent_n:\", correct_n)\n",
    "print(\"all_tp_one:\", all_tp_one)\n",
    "print(\"rate c:\", correct_n/all_tp_one)\n",
    "print(\"rate bad:\",  (len(y_rule)-all_tp_one)/len(y_rule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corrent_n: 157\n",
    "all_tp_one: 306\n",
    "rate c: 0.5130718954248366\n",
    "rate bad: 0.235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
