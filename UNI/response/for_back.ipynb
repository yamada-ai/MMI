{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "from feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "def read_conv(path:str, datalist:list):\n",
    "    convs = []\n",
    "    for p in datalist:\n",
    "        datapath = Path(path + p + '/')\n",
    "        for file_ in datapath.glob(\"*.json\"):\n",
    "            conv = []\n",
    "            with open(file_, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                did = json_data[\"did\"]\n",
    "                for t in json_data[\"turns\"]:\n",
    "                    sp = t[\"speaker\"]\n",
    "                    utt = t[\"utterance\"]\n",
    "                    errors = t[\"error_category\"]\n",
    "                    type_ = t[\"type\"]\n",
    "                    one = Utterance(did, sp, utt, errors, type_)\n",
    "                    conv.append(one)\n",
    "            convs.append(conv)\n",
    "    return convs      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向き発話以外は後ろ向きと考える\n",
    "def extract_response(convs, type_):\n",
    "    type_utt = []\n",
    "    forward = []\n",
    "    backward = []\n",
    "    plain = []\n",
    "    for conv in convs:\n",
    "        for i, ut in enumerate(conv):\n",
    "            if i == len(conv)-1:\n",
    "                continue\n",
    "            # システムの質問に対する応答\n",
    "            # if ut.is_system() and ut.is_type_included(type_) and not ut.is_exist_error():\n",
    "            #     if not ut.is_utt_level_error():\n",
    "            #         if conv[i+1].is_exist_type():\n",
    "            #             print(\"後ろ+前\", conv[i+1], conv[i+1].did)\n",
    "            #         else:\n",
    "            #             print(\"後ろ\", conv[i+1])\n",
    "\n",
    "            # システムの場合\\\n",
    "            \n",
    "            if ut.is_type_included(\"挨拶\"):\n",
    "                continue\n",
    "            utt = clean_text( ut.utt )\n",
    "            utt = utt.replace(\"\\u3000\", \" \")\n",
    "            utt = utt.replace(\"？？\", \"？\")\n",
    "            if ut.is_system():\n",
    "                # 発話レベルのエラーではなく\n",
    "                if not ut.is_utt_level_error():\n",
    "                    # 前向きである\n",
    "                    if not ut.is_exist_error():\n",
    "                        if ut.is_exist_type():\n",
    "                        # # 後ろ+前ではない\n",
    "                        # if not conv[i-1].is_exist_type():\n",
    "                            forward.append(utt)\n",
    "                        else:\n",
    "                            plain.append(utt)\n",
    "                            pass\n",
    "                        \n",
    "            # ユーザ発話\n",
    "            else:\n",
    "                # 前向き\n",
    "                if ut.is_exist_type():\n",
    "                    if conv[i-1].is_exist_type():\n",
    "                        forward.append(utt)\n",
    "                # ユーザで，後ろ向き\n",
    "                else:\n",
    "                    # 直前の発話は\n",
    "                    if conv[i-1].is_exist_type():\n",
    "                        backward.append(utt)\n",
    "                    else:\n",
    "                        backward.append(utt)\n",
    "\n",
    "            \n",
    "            # システムの前向き \n",
    "            # if ut.is_system() and not ut.is_utt_level_error():\n",
    "            #     if ut.is_exist_type():\n",
    "            #         print(\"前向き\", ut)\n",
    "            #         # 後ろ+前\n",
    "            #         if conv[i+1].is_exist_type():\n",
    "            #             print(\"後ろ+前\", conv[i+1], conv[i+1].did)\n",
    "            #         # 後ろ\n",
    "            #         else:\n",
    "            #             print(\"後ろ\", conv[i+1])\n",
    "            #         print()\n",
    "    \n",
    "            #     else:\n",
    "            #         if not ut.is_exist_error():\n",
    "            #             print(\"not type\", conv[i+1])\n",
    "            #             print()\n",
    "                    \n",
    "    return forward, backward, plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward, backward, plain = extract_response(convs, \"提案\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (forward+backward)\n",
    "y = np.concatenate( [ np.zeros(len(forward)), np.zeros(len(backward))+1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "F = Feature()\n",
    "F.set_preprocessor(Preprocessor())\n",
    "F.make_features(X_train_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/response/forback1.pickle\n"
     ]
    }
   ],
   "source": [
    "F_path = \"../X_y_data/response/\"\n",
    "F_name = \"forback1.pickle\"\n",
    "featureM = DataManager(F_path)\n",
    "featureM.save_data(F_name, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20642\n"
     ]
    }
   ],
   "source": [
    "print(F.feature_num)\n",
    "X_train = []\n",
    "X_test = []\n",
    "for i, x_t_str in enumerate( X_train_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_train.append(x)\n",
    "for i, x_t_str in enumerate( X_test_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_test.append(x)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=10000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = lr.predict(X_test)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 : それは私の質問の答えではないですよ？\n",
      "1.0 : 倖田來未さんはアナウンサーではないですよ！\n",
      "1.0 : 海はいいですよね。\n",
      "1.0 : ヒルトンですね\n",
      "1.0 : そうかも知れませんね。\n",
      "1.0 : そうだね。適切に水分をと取らないとね。\n",
      "1.0 : お遍路さんは、歩くのが辛そうですね。\n",
      "1.0 : 知らなかった。お詳しいんですね。\n",
      "1.0 : 犬です\n",
      "1.0 : ああ。周りの女子の話か。そう？\n",
      "1.0 : 役立つというか達成感がいいわ\n",
      "0.0 : では、あなたの趣味は何ですか？\n",
      "1.0 : それ知ってます、私も見に行きました。\n",
      "1.0 : こんにちは。すみません、二人とも知りません。\n",
      "1.0 : 危険ですよ。水分補給をしてくださいね。\n",
      "1.0 : そうかもしれないですね。今日は暑いですね。\n",
      "1.0 : 関西方面へ行かれることがありましたら。ぜひ関西のメロンパンの皮焼いちゃいましたを御覧になってみてください。\n",
      "1.0 : 他にもシューマイ等色々名物がありますよ。\n",
      "1.0 : 楽しいですね。\n",
      "1.0 : 物知りですね\n",
      "1.0 : 涼しくなってきたら、一緒に山へ行きたいですね。\n",
      "1.0 : 維持費が結構かかりますよ。\n",
      "1.0 : たまにだね\n",
      "1.0 : 行くかどうかどうかは時と場合によりますね\n",
      "1.0 : こんばんは、夏といえば海ですね。\n",
      "1.0 : ニュージーランドが楽しいと思います。\n",
      "1.0 : こんにちは。怖いですよね熱中症。毎日蒸し暑いですからね。\n",
      "1.0 : どんな御菓子でも、薄塩味は外しませんよね\n",
      "1.0 : 恋愛こそケースバイケースな判断力が必要となりますしね\n",
      "1.0 : そうなんですね。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for y_p, x_s in zip(y_pred[:30], X_test_str[:30]):\n",
    "    print(\"{0} : {1}\".format(y_p, x_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[ 47  14]\n",
      " [ 10 403]]\n",
      "accuracy =  0.9493670886075949\n",
      "precision =  0.9664268585131894\n",
      "recall =  0.9757869249394673\n",
      "f1 score =  0.9710843373493975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forback_clf.pickle\n",
      "success save : ../models/response/forback_clf.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response/\"\n",
    "model_name = \"forback_clf.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)\n",
    "modelM.save_data(model_name, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
