{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# 多分もう使わない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature import Feature\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2data(path, max_ = 300):\n",
    "    cols = [\"text\", \"label\", \"subLabel\"]\n",
    "    df = pd.DataFrame(index=[], columns=cols)\n",
    "    files = os.listdir(path)\n",
    "    for cop in files:\n",
    "        if \".\" not in cop:\n",
    "            continue\n",
    "        with open(path+cop, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "            mode = cop.split(\".\")[0]\n",
    "            for i, data in enumerate(json_data[mode]):\n",
    "                if i >= max_:\n",
    "                    break\n",
    "                text = data[\"data\"]\n",
    "                label = data[\"label\"][0]\n",
    "                subLabel = \"\"\n",
    "                df = df.append(pd.DataFrame([text, label, subLabel], index = cols).T)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'YN': 0, 'WH': 1, 'NOT': 2}\n"
     ]
    }
   ],
   "source": [
    "# label_list = \"YN WH please plain\".split()\n",
    "label_list = \"YN WH NOT\".split()\n",
    "label_dict = dict( zip(label_list, range(len(label_list))) )\n",
    "print(label_dict)\n",
    "def extract_X_y(df):\n",
    "    X = []\n",
    "    y = []\n",
    "    for te, la in zip(df.text, df.label):\n",
    "        X.append(te)\n",
    "        if la not in label_dict:\n",
    "            y.append(0)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../corpus/question/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_root = \"../../corpus\"\n",
    "# name = \"question/short\"\n",
    "name = \"question\"\n",
    "data_path = \"/\".join([corpus_root, name]) + \"/\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json2data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = extract_X_y(df)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "F = Feature()\n",
    "F.make_features(X_train_str)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "for i, x_t_str in enumerate( X_train_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_train.append(x)\n",
    "for i, x_t_str in enumerate( X_test_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_test.append(x)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='sag')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=1000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'YN': 0, 'WH': 1, 'NOT': 2}\n",
      "0.7851776097250244 : きみはテレビゲームが好きかい？\n",
      "0.11460969125396304 : 申し訳ないけど、知りませんね。\n",
      "0.027222053865762916 : 朝食のサンドイッチを作ってあげるよ。\n",
      "0.021518936000066543 : はい。ペパローニピザに、シーフードパスタ、それにサラダをお願いします。\n",
      "0.4088095887224571 : いっしょに来ますか？\n",
      "0.5033779592808952 : ちょっとアドバイスをもらえませんか。\n",
      "0.053665666840919016 : 「今朝は駅まで乗せてもらえるかな？」とケンは父親に聞きました。「遅れそうなんだ」\n",
      "0.532502696628346 : これをSNSに載せてもいい？\n",
      "0.03593148304770703 : あまねは、モニカにはひそかに好きな人がいるのを知らないんだ。\n",
      "0.0754320261568931 : エマ・スミスさんをお願いできますか？\n",
      "0.9117604926698546 : 僕が9時ごろ送ったメールは読んだかい？\n",
      "0.3500339835439612 : もっと強く引っ張らないと。\n",
      "0.758447412364085 : 誕生日に何が欲しい？\n",
      "0.1330649963516219 : このロボットを直してくれませんか？\n",
      "0.09599947869350231 : 雨が降るときには、ぼくは家の中で座って本を読むのが大好きなんだよ。\n",
      "0.1056432556522079 : あなたのホストファミリーにあげてね。\n",
      "0.05284078872971196 : すみませんが。銀行への行き方を教えていただけませんか？\n",
      "0.20237836512678653 : ちょっと聞き取れなかった。\n",
      "0.9885450244517617 : 君は何か買ったのかい？\n",
      "0.9156332066280782 : 目標に達したのかい？\n",
      "0.47550940443763545 : 先に行かせてもらってもよろしいですか？\n",
      "0.12742294216270608 : 私のコンピューターをモニターにつないでくれませんか？\n",
      "0.6381296671913905 : いっしょに行ってもいいかな？\n",
      "0.6703881437913295 : 1人で行くほうがいいの？\n",
      "0.9496351970655051 : 罪悪感があるのですか？\n",
      "0.3080784110339082 : 2人で腹を割って話し合うことが必要ね、スタートを切るために。\n",
      "0.17389233446557667 : 約束します。\n",
      "0.4010078043592989 : 乗らなきゃいけないか？\n",
      "0.17769051311994535 : ぼくたちはスカートを探しているんです。\n",
      "0.26534003089814234 : もしもし、浅香律子でございます。\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)\n",
    "for y_p, x_s in zip(y_log[:30], X_test_str[:30]):\n",
    "    print(\"{0} : {1}\".format(y_p[1], x_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[159  16]\n",
      " [ 29  98]]\n",
      "accuracy =  0.8509933774834437\n",
      "precision =  0.8596491228070176\n",
      "recall =  0.7716535433070866\n",
      "f1 score =  0.8132780082987552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/response/is_question_F.dill\n"
     ]
    }
   ],
   "source": [
    "F_path = \"../X_y_data/response/\"\n",
    "F_name = \"is_question_F.dill\"\n",
    "featureM = DataManager(F_path, format_=\"dill\")\n",
    "featureM.save_data(F_name, F)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_question_M.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response/\"\n",
    "model_name = \"is_question_M.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/response/is_question_M.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM.save_data(model_name, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
