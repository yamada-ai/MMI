{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import select\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaConfig\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        # self.model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "        # self.model = SentenceTransformer(self.model_path, show_progress_bar=False)\n",
    "\n",
    "        # 半角全角英数字\n",
    "        # self.DELETE_PATTERN_1 = re.compile(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+')\n",
    "        # 記号\n",
    "        self.DELETE_PATTERN_2 = re.compile(\n",
    "            r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+')\n",
    "        \n",
    "        self.emb_size = self.get_sentence_vec(\"emb\").shape[0]\n",
    "        print(self.emb_size)\n",
    "\n",
    "    def get_sentence_vec(self, sen) -> np.array:\n",
    "        # sen_ = self.DELETE_PATTERN_1.sub(sen)\n",
    "        sen_ = self.DELETE_PATTERN_2.sub(\"\", sen)\n",
    "        sentence_vec = self.nlp(sen_).vector\n",
    "        # sentence_vec = self.model.encode(sen)[0]\n",
    "        return sentence_vec\n",
    "\n",
    "    def read_json(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "        # datalist = ['DCM', 'DIT', 'IRS']\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            print(datapath)\n",
    "            # print(list(datapath.glob(\"*.json\")))\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" and t[\"error_category\"] != None:\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            ec = t[\"error_category\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "\n",
    "    def read_json_with_NoErr(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" :\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            if t[\"error_category\"]:\n",
    "                                ec = t[\"error_category\"]\n",
    "                            else:\n",
    "                                ec = [\"No-Err\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "    \n",
    "    def feature_extraction(self, df:pd.DataFrame) -> np.array:\n",
    "        return np.array([np.concatenate([self.get_sentence_vec(u), self.get_sentence_vec(s)]) for u,s in zip(df.usr, df.sys)])\n",
    "    \n",
    "    def feature_extraction_context2(self, df:pd.DataFrame) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        feature = []\n",
    "        did = 0\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                u_prev_vec = self.get_sentence_vec(u)\n",
    "                s_prev_vec = self.get_sentence_vec(s)\n",
    "                did = d\n",
    "                if e[0] != \"No-Err\":\n",
    "                    each = np.array(\n",
    "                        [np.concatenate(\n",
    "                            [np.zeros(self.emb_size),\n",
    "                            np.zeros(self.emb_size), \n",
    "                            u_prev_vec, \n",
    "                            s_prev_vec]\n",
    "                        )]\n",
    "                    ) \n",
    "                    feature.append(each[0])\n",
    "            else:\n",
    "                # エラーである\n",
    "                if e[0] != \"No-Err\":\n",
    "                    u_vec = self.get_sentence_vec(u)\n",
    "                    s_vec = self.get_sentence_vec(s)\n",
    "                    each = np.array(\n",
    "                        [np.concatenate(\n",
    "                            [u_vec,\n",
    "                            s_vec, \n",
    "                            u_prev_vec, \n",
    "                            s_prev_vec]\n",
    "                        )]\n",
    "                    )\n",
    "                    feature.append(each[0])\n",
    "                    u_prev_vec = u_vec\n",
    "                    s_prev_vec = s_vec\n",
    "                # エラーではない\n",
    "                else:    \n",
    "                    u_prev_vec = self.get_sentence_vec(u)\n",
    "                    s_prev_vec = self.get_sentence_vec(s)\n",
    "        return np.array(feature)\n",
    "    \n",
    "    def extract_y(self, df:pd.DataFrame, error_types) -> np.array:\n",
    "        y = []\n",
    "        for ec in df.ec:\n",
    "            if ec[0] == \"No-Err\":\n",
    "                continue\n",
    "            y_each_err = np.zeros(len(error_types))\n",
    "            for i, err in enumerate( error_types ):\n",
    "                if err in ec:\n",
    "                    y_each_err[i] = 1\n",
    "            y.append(y_each_err)\n",
    "        return np.array(y)\n",
    "\n",
    "    def make_error_dict(self, error_types):\n",
    "        error_dict = {}\n",
    "        for e in error_types:\n",
    "            error_dict[e] = len(error_dict)\n",
    "        return error_dict\n",
    "\n",
    "    def div_did_error(self, df:pd.DataFrame, error_types) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        \n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            # did で学習データを分割してみる？\n",
    "            y_one_conv = np.zeros(len(error_types))\n",
    "            \n",
    "            if did != d:\n",
    "                did = d\n",
    "                # 登録用データ修正\n",
    "                sequence_did = np.array(sequence_did)\n",
    "                y_did = np.array(y_did)\n",
    "\n",
    "                # training_data.append([sequence_did, y_did])\n",
    "                X_data.append(sequence_did)\n",
    "                y_data.append(y_did)\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "\n",
    "            for e_ in e:\n",
    "                y_one_conv[error_dict[e_]] = 1\n",
    "\n",
    "            sequence_did.append(\n",
    "                np.concatenate(\n",
    "                    [self.nlp(u).vector,\n",
    "                    self.nlp(s).vector]\n",
    "                )\n",
    "            )\n",
    "            y_did.append(y_one_conv)\n",
    "\n",
    "        sequence_did = np.array(sequence_did)\n",
    "        y_did = np.array(y_did)\n",
    "        X_data.append(sequence_did)\n",
    "        y_data.append(y_did)\n",
    "        return np.array(X_data), np.array( y_data )\n",
    "\n",
    "    # 頑張って学習データを新たに分割\n",
    "    def extract_X_y(self, df:pd.DataFrame, error_types, prev_num) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        n = prev_num\n",
    "        # print(did)\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "\n",
    "        # 初期の調整 padding\n",
    "        for i in range(n-1):\n",
    "            sequence_did.append(\n",
    "                    np.concatenate( [np.zeros(300), np.zeros(300)])\n",
    "                )\n",
    "\n",
    "        # didごとに返却する？\n",
    "        # エラーが発生したら、開始からエラーまでの文脈を入力とする(N=5の固定長でも可能)\n",
    "        # 先にこのベクトル列を作成し，Tensorに変換して， List に保持\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                did = d\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "                # 初期の調整 padding\n",
    "                for i in range(n-1):\n",
    "                    sequence_did.append(\n",
    "                            np.concatenate( [np.zeros(300), np.zeros(300)])\n",
    "                        )\n",
    "                # break\n",
    "\n",
    "            # sequence_did.append([u, s])\n",
    "            sequence_did.append(\n",
    "                    np.concatenate([self.nlp(u).vector, self.nlp(s).vector])\n",
    "                # [u, s]\n",
    "            )\n",
    "            if e[0] == \"No-Err\":\n",
    "                continue\n",
    "            else:\n",
    "                y_each_error_label = np.zeros(len(error_types))\n",
    "                for e_ in e:\n",
    "                    y_each_error_label[error_dict[e_]] = 1\n",
    "                X_data.append(sequence_did[-n:])\n",
    "                # y_did = np.array(y_each_error_label)\n",
    "                y_data.append(y_each_error_label)\n",
    "        return np.array(X_data), np.array(y_data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_least_oneClass(clf, X) -> np.array:\n",
    "    y_pred = clf.predict(X)\n",
    "    p = clf.predict_proba(X)\n",
    "    # print(y_pred)\n",
    "    proba = np.array([[p[c][i][1] if (p[c][i].shape[0]!=1) else 0 \n",
    "                     for c in range(len(error_types))] for i in range(len(X))])\n",
    "    # print(proba)\n",
    "  # replace [] to the highest probability label\n",
    "    y_pred2 = np.empty((0, len(error_types)), int)\n",
    "    for y, pr in zip(y_pred, proba):\n",
    "        if  (sum(y) == 0):\n",
    "            ans = np.zeros_like(y)\n",
    "            ans[np.argmax(pr)] = 1\n",
    "        else:\n",
    "            ans = y\n",
    "        y_pred2 = np.append(y_pred2, np.array([ans]), axis=0)\n",
    "    return y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # インプットの単語をベクトル化するために使う\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, self.lstm_out = self.lstm(x.view(seq_len, batch_size, -1) )\n",
    "        y = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
    "        y = self.softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pre = preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(670, 5)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 600\n",
    "HIDDEN_DIM = 600\n",
    "\n",
    "path = './error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM']\n",
    "    # List of error types\n",
    "error_types = ['Ignore question', 'Unclear intention', \n",
    "            'Wrong information', 'Topic transition error', \n",
    "            'Lack of information', 'Repetition', \n",
    "            'Semantic error', 'Self-contradiction', \n",
    "            'Contradiction', 'Grammatical error', \n",
    "            'Ignore offer', 'Ignore proposal', \n",
    "            'Lack of sociality', 'Lack of common sense',\n",
    "            'Uninterpretable', 'Ignore greeting', \n",
    "            'No-Err'\n",
    "            ]\n",
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "    # df = pre.read_json(path, datalist)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, 2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = pre.extract_X_y(df, error_types, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "y_train_ = y_train[:, 0]\n",
    "y_test_ = y_test[:, 0]\n",
    "y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Mydatasets(X_train, y_train_)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 51, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([51, 3, 600])\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not the same dtype, found input tensor with Double and parameter tensor with Float",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-03857e8a6289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-5b0ad9ed96c0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 662\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not the same dtype, found input tensor with Double and parameter tensor with Float"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(500):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0')\n",
    "        y_t_tensor = torch.tensor(data[1], dtype=torch.long, device='cuda:0')\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        print(X_t_tensor.shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        all_loss += loss_.item()\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([306])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "y_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}