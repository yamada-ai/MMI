{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import select\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaConfig\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import losss\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        # self.model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "        # self.model = SentenceTransformer(self.model_path, show_progress_bar=False)\n",
    "\n",
    "        # 半角全角英数字\n",
    "        # self.DELETE_PATTERN_1 = re.compile(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+')\n",
    "        # 記号\n",
    "        self.DELETE_PATTERN_2 = re.compile(\n",
    "            r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+')\n",
    "        \n",
    "        self.emb_size = self.get_sentence_vec(\"emb\").shape[0]\n",
    "        print(self.emb_size)\n",
    "\n",
    "    def get_sentence_vec(self, sen) -> np.array:\n",
    "        # sen_ = self.DELETE_PATTERN_1.sub(sen)\n",
    "        sen_ = self.DELETE_PATTERN_2.sub(\"\", sen)\n",
    "        sentence_vec = self.nlp(sen_).vector\n",
    "        # sentence_vec = self.model.encode(sen)[0]\n",
    "        return sentence_vec\n",
    "\n",
    "    def read_json(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "        # datalist = ['DCM', 'DIT', 'IRS']\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            print(datapath)\n",
    "            # print(list(datapath.glob(\"*.json\")))\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" and t[\"error_category\"] != None:\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            ec = t[\"error_category\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "\n",
    "    def read_json_with_NoErr(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" :\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            if t[\"error_category\"]:\n",
    "                                ec = t[\"error_category\"]\n",
    "                            else:\n",
    "                                ec = [\"No-Err\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "    \n",
    "    def feature_extraction(self, df:pd.DataFrame) -> np.array:\n",
    "        return np.array([np.concatenate([self.get_sentence_vec(u), self.get_sentence_vec(s)]) for u,s in zip(df.usr, df.sys)])\n",
    "    \n",
    "    def feature_extraction_context2(self, df:pd.DataFrame) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        feature = []\n",
    "        did = 0\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                u_prev_vec = self.get_sentence_vec(u)\n",
    "                s_prev_vec = self.get_sentence_vec(s)\n",
    "                did = d\n",
    "                if e[0] != \"No-Err\":\n",
    "                    each = np.array(\n",
    "                        [np.concatenate(\n",
    "                            [np.zeros(self.emb_size),\n",
    "                            np.zeros(self.emb_size), \n",
    "                            u_prev_vec, \n",
    "                            s_prev_vec]\n",
    "                        )]\n",
    "                    ) \n",
    "                    feature.append(each[0])\n",
    "            else:\n",
    "                # エラーである\n",
    "                if e[0] != \"No-Err\":\n",
    "                    u_vec = self.get_sentence_vec(u)\n",
    "                    s_vec = self.get_sentence_vec(s)\n",
    "                    each = np.array(\n",
    "                        [np.concatenate(\n",
    "                            [u_vec,\n",
    "                            s_vec, \n",
    "                            u_prev_vec, \n",
    "                            s_prev_vec]\n",
    "                        )]\n",
    "                    )\n",
    "                    feature.append(each[0])\n",
    "                    u_prev_vec = u_vec\n",
    "                    s_prev_vec = s_vec\n",
    "                # エラーではない\n",
    "                else:    \n",
    "                    u_prev_vec = self.get_sentence_vec(u)\n",
    "                    s_prev_vec = self.get_sentence_vec(s)\n",
    "        return np.array(feature)\n",
    "    \n",
    "    def extract_y(self, df:pd.DataFrame, error_types) -> np.array:\n",
    "        y = []\n",
    "        for ec in df.ec:\n",
    "            if ec[0] == \"No-Err\":\n",
    "                continue\n",
    "            y_each_err = np.zeros(len(error_types))\n",
    "            for i, err in enumerate( error_types ):\n",
    "                if err in ec:\n",
    "                    y_each_err[i] = 1\n",
    "            y.append(y_each_err)\n",
    "        return np.array(y)\n",
    "\n",
    "    def make_error_dict(self, error_types):\n",
    "        error_dict = {}\n",
    "        for e in error_types:\n",
    "            error_dict[e] = len(error_dict)\n",
    "        return error_dict\n",
    "\n",
    "    def div_did_error(self, df:pd.DataFrame, error_types) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        \n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            # did で学習データを分割してみる？\n",
    "            y_one_conv = np.zeros(len(error_types))\n",
    "            \n",
    "            if did != d:\n",
    "                did = d\n",
    "                # 登録用データ修正\n",
    "                sequence_did = np.array(sequence_did)\n",
    "                y_did = np.array(y_did)\n",
    "\n",
    "                # training_data.append([sequence_did, y_did])\n",
    "                X_data.append(sequence_did)\n",
    "                y_data.append(y_did)\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "\n",
    "            for e_ in e:\n",
    "                y_one_conv[error_dict[e_]] = 1\n",
    "\n",
    "            sequence_did.append(\n",
    "                np.concatenate(\n",
    "                    [self.nlp(u).vector,\n",
    "                    self.nlp(s).vector]\n",
    "                )\n",
    "            )\n",
    "            y_did.append(y_one_conv)\n",
    "\n",
    "        sequence_did = np.array(sequence_did)\n",
    "        y_did = np.array(y_did)\n",
    "        X_data.append(sequence_did)\n",
    "        y_data.append(y_did)\n",
    "        return np.array(X_data), np.array( y_data )\n",
    "\n",
    "    # 頑張って学習データを新たに分割\n",
    "    def extract_X_y(self, df:pd.DataFrame, error_types) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        # print(did)\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "\n",
    "        # didごとに返却する？\n",
    "        # エラーが発生したら、開始からエラーまでの文脈を入力とする(N=5の固定長でも可能)\n",
    "        # 先にこのベクトル列を作成し，Tensorに変換して， List に保持\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                did = d\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "                # break\n",
    "\n",
    "            # sequence_did.append([u, s])\n",
    "            sequence_did.append(\n",
    "                    np.concatenate([self.nlp(u).vector, self.nlp(s).vector])\n",
    "                # [u, s]\n",
    "            )\n",
    "            if e[0] == \"No-Err\":\n",
    "                continue\n",
    "            else:\n",
    "                y_each_error_label = np.zeros(len(error_types))\n",
    "                for e_ in e:\n",
    "                    y_each_error_label[error_dict[e_]] = 1\n",
    "                X_data.append(sequence_did[-5:])\n",
    "                # y_did = np.array(y_each_error_label)\n",
    "                y_data.append(y_each_error_label)\n",
    "        return X_data, torch.tensor(y_data, device='cuda:0', dtype=torch.long)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_least_oneClass(clf, X) -> np.array:\n",
    "    y_pred = clf.predict(X)\n",
    "    p = clf.predict_proba(X)\n",
    "    # print(y_pred)\n",
    "    proba = np.array([[p[c][i][1] if (p[c][i].shape[0]!=1) else 0 \n",
    "                     for c in range(len(error_types))] for i in range(len(X))])\n",
    "    # print(proba)\n",
    "  # replace [] to the highest probability label\n",
    "    y_pred2 = np.empty((0, len(error_types)), int)\n",
    "    for y, pr in zip(y_pred, proba):\n",
    "        if  (sum(y) == 0):\n",
    "            ans = np.zeros_like(y)\n",
    "            ans[np.argmax(pr)] = 1\n",
    "        else:\n",
    "            ans = y\n",
    "        y_pred2 = np.append(y_pred2, np.array([ans]), axis=0)\n",
    "    return y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # インプットの単語をベクトル化するために使う\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, lstm_out = self.lstm(x.view(len(x), 1, -1 ) )\n",
    "        y = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
    "        y = self.softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "(670, 5)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 600\n",
    "HIDDEN_DIM = 600\n",
    "pre = preprocessor()\n",
    "path = './error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM']\n",
    "    # List of error types\n",
    "error_types = ['Ignore question', 'Unclear intention', \n",
    "            'Wrong information', 'Topic transition error', \n",
    "            'Lack of information', 'Repetition', \n",
    "            'Semantic error', 'Self-contradiction', \n",
    "            'Contradiction', 'Grammatical error', \n",
    "            'Ignore offer', 'Ignore proposal', \n",
    "            'Lack of sociality', 'Lack of common sense',\n",
    "            'Uninterpretable', 'Ignore greeting', \n",
    "            'No-Err'\n",
    "            ]\n",
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "    # df = pre.read_json(path, datalist)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, 2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = pre.extract_X_y(df, error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ = y_train[:, 0]\n",
    "y_test_ = y_test[:, 0]\n",
    "y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 2083.577615232178\n",
      "epoch 100 \t loss 2036.9021016129323\n",
      "epoch 150 \t loss 1693.3125752017486\n",
      "epoch 200 \t loss 1877.6715104081877\n",
      "epoch 250 \t loss 2061.6088132960635\n",
      "epoch 300 \t loss 2283.664155610385\n",
      "epoch 350 \t loss 1926.3038743625834\n",
      "epoch 400 \t loss 2297.4894335748195\n",
      "epoch 450 \t loss 1759.7459834949573\n",
      "epoch 500 \t loss 2016.7090647841178\n",
      "epoch 550 \t loss 1950.9299698612458\n",
      "epoch 600 \t loss 2044.207030028284\n",
      "epoch 650 \t loss 2172.046194552974\n",
      "epoch 700 \t loss 1865.262272161953\n",
      "epoch 750 \t loss 1681.8945366197627\n",
      "epoch 800 \t loss 1946.7060477366686\n",
      "epoch 850 \t loss 2024.0394287732865\n",
      "epoch 900 \t loss 2206.128927666453\n",
      "epoch 950 \t loss 2148.0223680699164\n",
      "epoch 1000 \t loss 2152.413977678666\n",
      "epoch 1050 \t loss 2148.1493997904436\n",
      "epoch 1100 \t loss 2183.3519965110727\n",
      "epoch 1150 \t loss 2104.244491664486\n",
      "epoch 1200 \t loss 1718.215649185662\n",
      "epoch 1250 \t loss 2043.7107574853198\n",
      "epoch 1300 \t loss 1745.0383913763249\n",
      "epoch 1350 \t loss 1737.0150554496095\n",
      "epoch 1400 \t loss 2149.1884760869552\n",
      "epoch 1450 \t loss 1852.5622119740794\n",
      "epoch 1500 \t loss 1953.3787605608761\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1500):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for X_t, y_t in zip(X_train, y_train_):\n",
    "        X_t_tensor = torch.tensor(X_t, device='cuda:0')\n",
    "        y_t_tensor = torch.tensor([y_t], dtype=torch.long, device='cuda:0')\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        all_loss += loss_.item()\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101.25090551376343,\n",
       " 41.07545018196103,\n",
       " 15.189394047018084,\n",
       " 10.21215343437143,\n",
       " 4.580461127799907,\n",
       " 0.32040360709549986,\n",
       " 9.82208001560138,\n",
       " 0.05771276634180822,\n",
       " 0.49403877534132334,\n",
       " 0.05411070529589779,\n",
       " 0.0573089870149488,\n",
       " 0.10201472786684462,\n",
       " 0.011147599252126383,\n",
       " 0.00490452447820644,\n",
       " 0.002775311950244941,\n",
       " 0.0019663393031805754,\n",
       " 0.001685113791609183,\n",
       " 0.0010825724493770394,\n",
       " 0.0007133898452593712,\n",
       " 0.0006413141563825775,\n",
       " 0.0005916316404181998,\n",
       " 0.0005594628164544702,\n",
       " 0.0005364675635064486,\n",
       " 0.0005193101678742096,\n",
       " 0.0005060845141997561,\n",
       " 0.0004954800606356002,\n",
       " 0.0004865435548708774,\n",
       " 0.0004787984653376043,\n",
       " 0.0004718873678939417,\n",
       " 0.00046509514868375845,\n",
       " 0.00045854107884224504,\n",
       " 0.0004515099717536941,\n",
       " 0.0004434058973856736,\n",
       " 0.0004320832958910614,\n",
       " 0.00041611161577748135,\n",
       " 0.000398113377741538,\n",
       " 0.00038404881706810556,\n",
       " 0.00037403714668471366,\n",
       " 0.0003661710652522743,\n",
       " 0.00035997376835439354,\n",
       " 0.000354610885551665,\n",
       " 0.00034996308750123717,\n",
       " 0.00034567284819786437,\n",
       " 0.00034185936965513974,\n",
       " 0.0003382842114660889,\n",
       " 0.00033506661930005066,\n",
       " 0.0003318489543744363,\n",
       " 0.00032898885183385573,\n",
       " 0.00032648628621245734,\n",
       " 0.0003238644821976777,\n",
       " 0.00032160022237803787,\n",
       " 0.00031945514638209715,\n",
       " 0.0003174292251060251,\n",
       " 0.00031528411636827514,\n",
       " 0.00029657224149559624,\n",
       " 0.00030491732104565017,\n",
       " 0.00031147378831519745,\n",
       " 0.00031004356060293503,\n",
       " 0.00030849408722133376,\n",
       " 0.0003073021798627451,\n",
       " 0.00030611027614213526,\n",
       " 0.0003051567473448813,\n",
       " 0.0003040840310859494,\n",
       " 0.000303249697026331,\n",
       " 0.0003022961609531194,\n",
       " 0.00030146181961754337,\n",
       " 0.0003007466657436453,\n",
       " 0.00029991232440806925,\n",
       " 0.0002991971850860864,\n",
       " 0.0002984820239362307,\n",
       " 0.0002977668773382902,\n",
       " 0.0002970517234643921,\n",
       " 0.0002962173748528585,\n",
       " 0.00029562141571659595,\n",
       " 0.0002950254565803334,\n",
       " 0.00029431030998239294,\n",
       " 0.00029359514883253723,\n",
       " 0.00029287998768268153,\n",
       " 0.000292164822894847,\n",
       " 0.00029144967629690655,\n",
       " 0.00029073451514705084,\n",
       " 0.0002901385487348307,\n",
       " 0.0002894233839469962,\n",
       " 0.00028882741753477603,\n",
       " 0.00028811225638492033,\n",
       " 0.0002873970952350646,\n",
       " 0.0002866819231712725,\n",
       " 0.00028608595675905235,\n",
       " 0.00028537079560919665,\n",
       " 0.0002846556089934893,\n",
       " 0.00028405964258126915,\n",
       " 0.00028334448143141344,\n",
       " 0.0002826293093676213,\n",
       " 0.000281914140941808,\n",
       " 0.0002813181599776726,\n",
       " 0.00028048379317624494,\n",
       " 0.0002798878267640248,\n",
       " 0.00027917265470023267,\n",
       " 0.0002785766737360973,\n",
       " 0.0002779807000479195,\n",
       " 0.00027714632233255543,\n",
       " 0.00027655035228235647,\n",
       " 0.0002757159891189076,\n",
       " 0.0002751199972408358,\n",
       " 0.00027452403082861565,\n",
       " 0.0002738088442129083,\n",
       " 0.000273093675787095,\n",
       " 0.00027237848917138763,\n",
       " 0.00027178251548320986,\n",
       " 0.0002710673288675025,\n",
       " 0.00027023296570405364,\n",
       " 0.00026963697382598184,\n",
       " 0.0002689218054001685,\n",
       " 0.0002684450191736687,\n",
       " 0.0002677298471098766,\n",
       " 0.0002668954693945125,\n",
       " 0.0002662994920683559,\n",
       " 0.00026558430909062736,\n",
       " 0.00026486913702683523,\n",
       " 0.00026427314878674224,\n",
       " 0.0002635579621710349,\n",
       " 0.00026284278283128515,\n",
       " 0.0002621275998535566,\n",
       " 0.0002615316079754848,\n",
       " 0.0002608164395496715,\n",
       " 0.0002599820581963286,\n",
       " 0.0002593860663182568,\n",
       " 0.00025867089789244346,\n",
       " 0.0002579557040007785,\n",
       " 0.0002573597230366431,\n",
       " 0.0002566445436968934,\n",
       " 0.000255929357081186,\n",
       " 0.0002552141741034575,\n",
       " 0.0002546181822253857,\n",
       " 0.00025402221581316553,\n",
       " 0.00025330702555947937,\n",
       " 0.0002527110409573652,\n",
       " 0.0002519958470657002,\n",
       " 0.0002512806786398869,\n",
       " 0.00025056548474822193,\n",
       " 0.00024996950014610775,\n",
       " 0.0002492543098924216,\n",
       " 0.00024853912327671424,\n",
       " 0.0002478239475749433,\n",
       " 0.0002472279629728291,\n",
       " 0.00024651276908116415,\n",
       " 0.00024591678084107116,\n",
       " 0.00024532078896299936,\n",
       " 0.0002446055950713344,\n",
       " 0.00024389041209360585,\n",
       " 0.0002431752254778985,\n",
       " 0.00024257924815174192,\n",
       " 0.00024186405789805576,\n",
       " 0.0002411488712823484,\n",
       " 0.0002405528794042766,\n",
       " 0.00023983768551261164,\n",
       " 0.0002391225025348831,\n",
       " 0.00023840730864321813,\n",
       " 0.00023781132404110394,\n",
       " 0.00023709613378741778,\n",
       " 0.00023650014190934598,\n",
       " 0.00023590415003127418,\n",
       " 0.00023518895613960922,\n",
       " 0.00023447377316188067,\n",
       " 0.00023399698693538085,\n",
       " 0.00023328179304371588,\n",
       " 0.00023256660642800853,\n",
       " 0.00023197061454993673,\n",
       " 0.00023137461903388612,\n",
       " 0.00023065943241817877,\n",
       " 0.00023006344054010697,\n",
       " 0.00022934823937248439,\n",
       " 0.00022863305639475584,\n",
       " 0.00022803706451668404,\n",
       " 0.00022744106536265463,\n",
       " 0.00022672587147098966,\n",
       " 0.00022612989050685428,\n",
       " 0.0002254146893392317,\n",
       " 0.00022469949544756673,\n",
       " 0.00022422269830713049,\n",
       " 0.00022350751532940194,\n",
       " 0.00022279231416177936,\n",
       " 0.00022231552793527953,\n",
       " 0.00022160034131957218,\n",
       " 0.00022100433488958515,\n",
       " 0.00022040834301151335,\n",
       " 0.0002196931527578272,\n",
       " 0.0002189779515902046,\n",
       " 0.00021850116172572598,\n",
       " 0.000217785967834061,\n",
       " 0.0002171899686800316,\n",
       " 0.00021647477478836663,\n",
       " 0.0002158787719963584,\n",
       " 0.00021528278739424422,\n",
       " 0.00021468679187819362,\n",
       " 0.00021409080000012182,\n",
       " 0.0002134947935701348,\n",
       " 0.00021277959967846982,\n",
       " 0.0002121836005244404,\n",
       " 0.0002115876086463686,\n",
       " 0.000210991613130318,\n",
       " 0.00021027641923865303,\n",
       " 0.000209680412808666,\n",
       " 0.0002090844209305942,\n",
       " 0.0002084884254145436,\n",
       " 0.00020801162099814974,\n",
       " 0.00020729643438244238,\n",
       " 0.00020670043522841297,\n",
       " 0.00020610443607438356,\n",
       " 0.00020550844055833295,\n",
       " 0.00020491243412834592,\n",
       " 0.00020419724023668095,\n",
       " 0.00020360124108265154,\n",
       " 0.0002031244403042365,\n",
       " 0.00020252844115020707,\n",
       " 0.00020193244199617766,\n",
       " 0.00020133644284214824,\n",
       " 0.00020062124895048328,\n",
       " 0.00020014444453408942,\n",
       " 0.00019942925064242445,\n",
       " 0.0001989524462260306,\n",
       " 0.00019835645070997998,\n",
       " 0.00019787964993156493,\n",
       " 0.00019716444876394235,\n",
       " 0.00019656844960991293,\n",
       " 0.00019597245045588352,\n",
       " 0.0001953764513018541,\n",
       " 0.00019478044487186708,\n",
       " 0.00019430365136940964,\n",
       " 0.0001937076449394226,\n",
       " 0.0001931116457853932,\n",
       " 0.00019251564663136378,\n",
       " 0.00019191964747733437,\n",
       " 0.00019132364104734734,\n",
       " 0.00019072764189331792,\n",
       " 0.00019025084111490287,\n",
       " 0.00018965484196087345,\n",
       " 0.0001891780375444796,\n",
       " 0.00018858203111449257,\n",
       " 0.00018798603196046315,\n",
       " 0.00018739002553047612,\n",
       " 0.00018691323202801868,\n",
       " 0.00018631722559803165,\n",
       " 0.00018572122280602343,\n",
       " 0.00018524441838962957,\n",
       " 0.00018464841195964254,\n",
       " 0.00018405241280561313,\n",
       " 0.0001834564063756261,\n",
       " 0.0001828604072215967,\n",
       " 0.00018238360280520283,\n",
       " 0.00018190680202678777,\n",
       " 0.00018131079923477955,\n",
       " 0.0001808339948183857,\n",
       " 0.0001801187863748055,\n",
       " 0.00017964198013942223,\n",
       " 0.0001790459737094352,\n",
       " 0.0001784499745554058,\n",
       " 0.00017797316650103312,\n",
       " 0.00017725795441947412,\n",
       " 0.0001766619552654447,\n",
       " 0.00017618515084905084,\n",
       " 0.0001755891426000744,\n",
       " 0.0001748739268805366,\n",
       " 0.00017427791681257077,\n",
       " 0.00017356270291202236,\n",
       " 0.00017296669284405652,\n",
       " 0.00017213228420587257,\n",
       " 0.00017141706484835595,\n",
       " 0.0001704634396446636,\n",
       " 0.000169629014635575,\n",
       " 0.00016867538943188265,\n",
       " 0.0001677217715041479,\n",
       " 0.0001668873428570805,\n",
       " 0.00016605292330496013,\n",
       " 0.0001654569114180049,\n",
       " 0.0001647417047934141,\n",
       " 0.0001640264908928657,\n",
       " 0.00016343047900591046,\n",
       " 0.0001629536745895166,\n",
       " 0.00016235767361649778,\n",
       " 0.00016176166718651075,\n",
       " 0.00016152327043528203,\n",
       " 0.0001609272621863056,\n",
       " 0.00016045046140789054,\n",
       " 0.0001598544513399247,\n",
       " 0.00015949685257510282,\n",
       " 0.0001589008443261264,\n",
       " 0.0001585432455613045,\n",
       " 0.00015806644114491064,\n",
       " 0.00015758963672851678,\n",
       " 0.0001571128232171759,\n",
       " 0.00015675522445235401,\n",
       " 0.00015615921438438818,\n",
       " 0.0001558016156195663,\n",
       " 0.0001553248021082254,\n",
       " 0.00015484799769183155,\n",
       " 0.00015425199126184452,\n",
       " 0.00015377518502646126,\n",
       " 0.00015341757534770295,\n",
       " 0.00015294076911231969]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}