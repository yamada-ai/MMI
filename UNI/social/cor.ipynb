{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"../../corpus/twitter/\"\n",
    "data_name = \"hate_labeled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(corpus_path+data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>マスゴミってエリートも糞もないんだってねW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ランサーぶっぱするな死ね。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ジエンまじでクソ死ねばいいのに(殺せない)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ねぇ〜〜バカー！！！！！！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ゴミコントローラー引いてCSだせまんでした御免なさい...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>2</td>\n",
       "      <td>与野党マスゴミ茶番芸能人言論人、全部グル。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>2</td>\n",
       "      <td>非実在児童ポルノ規制とホラー映画規制は等しく馬鹿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1</td>\n",
       "      <td>そんな事ないです…自分全然ゴミカスっす</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>3</td>\n",
       "      <td>マジでキモすぎ死ぬんやったら勝手に死ねよ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>3</td>\n",
       "      <td>はー最悪まじ気分悪ぃ、死ね</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1138 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                            txt\n",
       "0         2          マスゴミってエリートも糞もないんだってねW\n",
       "1         2                  ランサーぶっぱするな死ね。\n",
       "2         2          ジエンまじでクソ死ねばいいのに(殺せない)\n",
       "3         3                  ねぇ〜〜バカー！！！！！！\n",
       "4         1  ゴミコントローラー引いてCSだせまんでした御免なさい...\n",
       "...     ...                            ...\n",
       "1133      2          与野党マスゴミ茶番芸能人言論人、全部グル。\n",
       "1134      2       非実在児童ポルノ規制とホラー映画規制は等しく馬鹿\n",
       "1135      1            そんな事ないです…自分全然ゴミカスっす\n",
       "1136      3           マジでキモすぎ死ぬんやったら勝手に死ねよ\n",
       "1137      3                  はー最悪まじ気分悪ぃ、死ね\n",
       "\n",
       "[1138 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    465\n",
       "2    351\n",
       "0    181\n",
       "1    141\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.BertTokenizer = transformers.BertJapaneseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] : (768,)\n",
      "[1] : (768,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "\n",
    "transformer = models.Transformer('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "pooling = models.Pooling(transformer.get_word_embedding_dimension(),    \n",
    "  pooling_mode_mean_tokens=True,\n",
    "  pooling_mode_cls_token=False, \n",
    "  pooling_mode_max_tokens=False\n",
    ")\n",
    "model = SentenceTransformer(modules=[transformer, pooling])\n",
    "\n",
    "sentences = ['吾輩は猫である',  '本日は晴天なり']\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "  print(\"[%d] : %s\" % (i, embedding.shape, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models ,losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.losses import TripletDistanceMetric, BatchAllTripletLoss\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sentence_transformers.readers import TripletReader\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, mode=\"All\"):\n",
    "    X = []\n",
    "    y = []\n",
    "    path = \"../hand_labeled/\"\n",
    "    datalist = ['DCM', 'DIT', 'IRS']\n",
    "    convs = read_conv(path, datalist)\n",
    "\n",
    "    usr_utt = []\n",
    "    for conv in convs:\n",
    "        for i, ut in enumerate(conv):\n",
    "            if not ut.is_system():\n",
    "                usr_utt.append(clean_text(ut.utt))\n",
    "    import random\n",
    "\n",
    "    if mode==\"All\":\n",
    "        for la, txt in zip(df.label, df.txt):\n",
    "            X.append( InputExample(guid=\"\", texts=[txt], label=float(la) ) )\n",
    "            # y.append(la)\n",
    "        print(\"length of X\", len(X))\n",
    "        # 0 の要素を増やしておきますわよ\n",
    "        sampled = random.sample(usr_utt, len(X)//3)\n",
    "        for sample in sampled:\n",
    "            X.append( InputExample(guid=\"\" , texts=[sample], label=0.0 ) )\n",
    "            # y.append(0)\n",
    "        print(\"added length of X\", len(X))\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X 1138\n",
      "added length of X 1517\n"
     ]
    }
   ],
   "source": [
    "X = make_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, train_size=0.8, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "EVAL_STEPS = 1000\n",
    "WARMUP_STEPS = int(len(X_train) // BATCH_SIZE * 0.1) \n",
    "OUTPUT_PATH = \"../../corpus/sbert_stair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SentenceLabelDataset(X_train, samples_per_label=BATCH_SIZE//2)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "train_loss = losses.BatchAllTripletLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e40c9bfb444b83af0e5f5f1f90d658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ea8e825f4749fdbbbf392ca1626c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2a82bde23d4170aa09e4a66b119aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db169bf2bd44931a195b59b68aa80e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "         epochs=NUM_EPOCHS,\n",
    "         evaluation_steps=EVAL_STEPS,\n",
    "         warmup_steps=WARMUP_STEPS,\n",
    "         output_path=OUTPUT_PATH\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eval',\n",
       " '1_Pooling',\n",
       " 'tokenizer_config.json',\n",
       " 'sentence_bert_config.json',\n",
       " 'README.md',\n",
       " 'config.json',\n",
       " 'modules.json',\n",
       " 'vocab.txt',\n",
       " 'config_sentence_transformers.json',\n",
       " 'pytorch_model.bin',\n",
       " 'special_tokens_map.json']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考\n",
    "\n",
    "1. Sentence transformerで日本語モデルを学習して文章の分散表現を得る方法  \n",
    "https://www.subcul-science.com/post/20210203sbert/#%E5%88%86%E6%95%A3%E8%A1%A8%E7%8F%BE%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B\n",
    "\n",
    "2.  第9回 Sentence BERT による類似文章検索の検証  \n",
    "https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part9.html\n",
    "\n",
    "3. Sentence BERT論文-和訳\n",
    "https://www.vareal.co.jp/column/sentence-bert%E8%AB%96%E6%96%87-%E5%92%8C%E8%A8%B3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
