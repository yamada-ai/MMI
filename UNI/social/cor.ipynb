{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"../../corpus/twitter/\"\n",
    "# data_name = \"impolite.csv\"\n",
    "data_name = \"hate_labeled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(corpus_path+data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_ = 1100\n",
    "# df = df[:max_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    465\n",
       "2    352\n",
       "0    180\n",
       "1    141\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.BertTokenizer = transformers.BertJapaneseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../corpus/cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[24145] 2022-01-05 17:59:23,684 Info sentence_transformers.SentenceTransformer :Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d928891094e34b9cb0f7726468c27179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] : (768,)\n",
      "[1] : (768,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "\n",
    "download_path = \"../../corpus/\"\n",
    "# download_path = \"\"\n",
    "transformer = models.Transformer(download_path+'cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "pooling = models.Pooling(transformer.get_word_embedding_dimension(),    \n",
    "  pooling_mode_mean_tokens=True,\n",
    "  pooling_mode_cls_token=False, \n",
    "  pooling_mode_max_tokens=False\n",
    ")\n",
    "model = SentenceTransformer(modules=[transformer, pooling])\n",
    "\n",
    "sentences = ['吾輩は猫である',  '本日は晴天なり']\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "  print(\"[%d] : %s\" % (i, embedding.shape, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models ,losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.losses import TripletDistanceMetric, BatchAllTripletLoss\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sentence_transformers.readers import TripletReader\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0, 2: 1, 3: 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la2la = {0:0,1:0, 2:1, 3:1}\n",
    "la2la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, mode=\"All\"):\n",
    "    X = []\n",
    "    y = []\n",
    "    path = \"../hand_labeled/\"\n",
    "    datalist = ['DCM', 'DIT', 'IRS']\n",
    "    convs = read_conv(path, datalist)\n",
    "\n",
    "    usr_utt = []\n",
    "    for conv in convs:\n",
    "        for i, ut in enumerate(conv):\n",
    "            if not ut.is_system():\n",
    "                usr_utt.append(clean_text(ut.utt))\n",
    "    import random\n",
    "\n",
    "    if mode==\"All\":\n",
    "        for la, txt in zip(df.label, df.txt):\n",
    "            # X.append( InputExample(guid=\"\", texts=[txt], label=float(la2la[la]) ) )\n",
    "            X.append( InputExample(guid=\"\", texts=[txt], label=float(la) ) )\n",
    "            # y.append(la)\n",
    "        print(\"length of X\", len(X))\n",
    "        # 0 の要素を増やしておきますわよ\n",
    "        sampled = random.sample(usr_utt, len(X)//3)\n",
    "        for sample in sampled:\n",
    "            X.append( InputExample(guid=\"\" , texts=[sample], label=0.0 ) )\n",
    "            # y.append(0)\n",
    "        print(\"added length of X\", len(X))\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X 1138\n",
      "added length of X 1517\n"
     ]
    }
   ],
   "source": [
    "X = make_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, train_size=0.8, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "EVAL_STEPS = 1000\n",
    "WARMUP_STEPS = int(len(X_train) // BATCH_SIZE * 0.1) \n",
    "OUTPUT_PATH = \"../../corpus/sbert_stair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24145] 2022-01-05 17:59:25,718 Info sentence_transformers.datasets.SentenceLabelDataset :SentenceLabelDataset: 1213 examples, from which 1213 examples could be used (those labels appeared at least 16 times). 4 different labels found.\n"
     ]
    }
   ],
   "source": [
    "train_data = SentenceLabelDataset(X_train, samples_per_label=BATCH_SIZE//2)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "train_loss = losses.BatchAllTripletLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a93bc12e094391a3617e2dc2b5585a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ace99d37ec3493eaef38fb16b80fe04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aad7b9633664071bc39c4e4492c598f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b52ed5397984eb1adb18457eb740f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24145] 2022-01-05 18:00:06,745 Info sentence_transformers.SentenceTransformer :Save model to ../../corpus/sbert_stair\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "         epochs=NUM_EPOCHS,\n",
    "         evaluation_steps=EVAL_STEPS,\n",
    "         warmup_steps=WARMUP_STEPS,\n",
    "         output_path=OUTPUT_PATH\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eval',\n",
       " '1_Pooling',\n",
       " 'tokenizer_config.json',\n",
       " 'sentence_bert_config.json',\n",
       " 'README.md',\n",
       " 'config.json',\n",
       " 'modules.json',\n",
       " 'vocab.txt',\n",
       " 'config_sentence_transformers.json',\n",
       " 'pytorch_model.bin',\n",
       " 'special_tokens_map.json']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考\n",
    "\n",
    "1. Sentence transformerで日本語モデルを学習して文章の分散表現を得る方法  \n",
    "https://www.subcul-science.com/post/20210203sbert/#%E5%88%86%E6%95%A3%E8%A1%A8%E7%8F%BE%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B\n",
    "\n",
    "2.  第9回 Sentence BERT による類似文章検索の検証  \n",
    "https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part9.html\n",
    "\n",
    "3. Sentence BERT論文-和訳\n",
    "https://www.vareal.co.jp/column/sentence-bert%E8%AB%96%E6%96%87-%E5%92%8C%E8%A8%B3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
