{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../corpus/sbert_stair2\"\n",
    "# data_name = \"hate_labeled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5100] 2022-01-12 20:48:04,318 Info sentence_transformers.SentenceTransformer :Load pretrained SentenceTransformer: ../../corpus/sbert_stair2\n",
      "[5100] 2022-01-12 20:48:07,127 Info sentence_transformers.SentenceTransformer :Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "sbert = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    sentences = []\n",
    "    if isinstance(text, str):\n",
    "        sentences = [text]\n",
    "    elif isinstance(text, list):\n",
    "        sentences = text\n",
    "    \n",
    "    return sbert.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "la2la = {0:0,1:0, 2:1, 3:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, mode=\"All\"):\n",
    "    X = []\n",
    "    y = []\n",
    "    path = \"../hand_labeled/\"\n",
    "    datalist = ['DCM', 'DIT', 'IRS']\n",
    "    convs = read_conv(path, datalist)\n",
    "\n",
    "    usr_utt = []\n",
    "    for conv in convs:\n",
    "        for i, ut in enumerate(conv):\n",
    "            if not ut.is_system():\n",
    "                usr_utt.append(clean_text(ut.utt))\n",
    "    import random\n",
    "\n",
    "    if mode==\"All\":\n",
    "        for la, txt in zip(df.label, df.txt):\n",
    "            # X.append( InputExample(guid=\"\", texts=[txt], label=float(la) ) )\n",
    "            X.append(txt)\n",
    "            # y.append(la)\n",
    "            y.append( la2la[la] )\n",
    "        print(\"length of X\", len(X))\n",
    "        # 0 の要素を増やしておきますわよ\n",
    "        sampled = random.sample(usr_utt, len(X)//3)\n",
    "        for sample in sampled:\n",
    "            # X.append( InputExample(guid=\"\" , texts=[sample], label=0.0 ) )\n",
    "            X.append(sample)\n",
    "            y.append(0)\n",
    "        print(\"added length of X\", len(X))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"../../corpus/twitter/\"\n",
    "# data_name = \"hate_labeled.csv\"\n",
    "data_name = \"impolite.csv\"\n",
    "df = pd.read_csv(corpus_path+data_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = 1100\n",
    "df = df[:max_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X 1100\n",
      "added length of X 1466\n"
     ]
    }
   ],
   "source": [
    "X, y = make_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1694c20dc0a04a03bde1d0ab11388df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = text2vec(X_train_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(SocialModel, self).__init__()    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hid1= embedding_dim*2\n",
    "        self.hid2 = embedding_dim//2\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, self.hid1)\n",
    "        self.fc2 = nn.Linear(self.hid1, self.hid2)\n",
    "        self.hidden2tag = nn.Linear(self.hid2, tagset_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.hidden2tag( y )\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn import svm\n",
    "# # clf = AdaBoostClassifier()\n",
    "# clf = svm.SVC(kernel='rbf', gamma =0.0001, C=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "epoch_ = 300\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 1536 2\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 768\n",
    "HIDDEN_DIM = EMBEDDING_DIM*2\n",
    "OUTPUT_DIM = 2\n",
    "# seq_len = length\n",
    "print(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SocialModel(EMBEDDING_DIM, OUTPUT_DIM)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 0.4861138644628227\n",
      "epoch 100 \t loss 0.03653045691316947\n",
      "epoch 150 \t loss 0.009939842857420444\n",
      "epoch 200 \t loss 0.003859727649796696\n",
      "epoch 250 \t loss 0.002347189947613515\n",
      "epoch 300 \t loss 0.0009911896995618008\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "for epoch in range(epoch_): \n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0')\n",
    "        X_t_tensor = data[0].cuda()\n",
    "        # y_t_tensor = torch.tensor(data[1], device='cuda:0')\n",
    "        y_t_tensor = data[1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "        score_ = model(X_t_tensor)\n",
    "        loss_ = loss_function(score_, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score_\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['が、こけた早苗ちゃんと同類のようにね!', '起きてるなら付き合え', 'ラインの無料漫画を読んでいます', 'ウザ', 'おいも片付けていけや']\n",
      "[1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_str[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71eebe51753d406da42a67665082ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = text2vec(X_test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 最近俺で遊びすぎWW\n",
      "1 : おみげのディスやべーやん\n",
      "1 : まとめてぶっ飛ばす\n",
      "1 : 寧ろ地獄を見るのは安倍晋三と与党の屑、ぢゃい\n",
      "0 : やってませんよ。でもジャズも好きです。\n",
      "1 : お前のせいだからな\n",
      "1 : 調べるとフラウンダーを鰈とする所もあって、鰈と鮃区別つけてないやろってなってる\n",
      "0 : 空耳では?\n",
      "0 : 沖縄行きたいです!\n",
      "0 : どーしたの．同じくらい好きだよ\n",
      "1 : 絶対半年に0回しか遊んでくれん\n",
      "1 : ずるだったん知らんかった\n",
      "0 : へー。最近のお気に入りは煮干し系ラーメンです。\n",
      "1 : お前が落ち着きなぁ!\n",
      "1 : お前が金出せよ。\n",
      "1 : がんばれ\n",
      "1 : なんて!幸せになれよ!クソ野郎!\n",
      "1 : 私に怒って当たるな!!お前のせいだろうが!!\n",
      "0 : 観光地の食べ物は高いですもんね。\n",
      "1 : ええて\n",
      "0 : 盛大にやってんなW\n",
      "0 : 海いいですね\n",
      "0 : 今日の予定は?\n",
      "0 : 君の方が大事だから死ぬなよ…\n",
      "0 : 巻き込むなよ\n",
      "1 : 食えない俺より、一々そんな反応するの貧乏臭さの方が滑稽だわ\n",
      "0 : 夜もバロすんの?\n",
      "1 : お前のせいなんだぞッ!!\n",
      "1 : プリ撮ったんなら見せろ\n",
      "1 : 落ち着けって\n"
     ]
    }
   ],
   "source": [
    "for y_p, x_s in zip(y_pred[:30], X_test_str[:30]):\n",
    "    print(\"{0} : {1}\".format(y_p, x_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[190  25]\n",
      " [ 16 209]]\n",
      "accuracy =  0.9068181818181819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(test, pred):\n",
    "    if len(collections.Counter(pred)) <= 2:\n",
    "        print('confusion matrix = \\n', confusion_matrix(y_true=test, y_pred=pred))\n",
    "        print('accuracy = ', accuracy_score(y_true=test, y_pred=pred))\n",
    "        print('precision = ', precision_score(y_true=test, y_pred=pred))\n",
    "        print('recall = ', recall_score(y_true=test, y_pred=pred))\n",
    "        print('f1 score = ', f1_score(y_true=test, y_pred=pred))\n",
    "    else:\n",
    "        print('confusion matrix = \\n', confusion_matrix(y_true=test, y_pred=pred))\n",
    "        print('accuracy = ', accuracy_score(y_true=test, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[190  25]\n",
      " [ 16 209]]\n",
      "accuracy =  0.9068181818181819\n",
      "precision =  0.8931623931623932\n",
      "recall =  0.9288888888888889\n",
      "f1 score =  0.9106753812636166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  SVM \n",
    "\n",
    "        confusion matrix = \n",
    "        [[185  30]\n",
    "        [ 34 191]]\n",
    "        accuracy =  0.8545454545454545\n",
    "\n",
    "- FFN\n",
    "\n",
    "         [[190  25]\n",
    "        [ 16 209]]\n",
    "        accuracy =  0.9068181818181819\n",
    "        precision =  0.8931623931623932\n",
    "        recall =  0.9288888888888889\n",
    "        f1 score =  0.9106753812636166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def convert_bool(p):\n",
    "    new_p = copy.deepcopy(p)\n",
    "    for i, v in enumerate(p):\n",
    "        if v<=1:\n",
    "            new_p[i] = 0\n",
    "        else:\n",
    "            new_p[i] = 1\n",
    "    return new_p\n",
    "\n",
    "def convert_bool2(p):\n",
    "    new_p = copy.deepcopy(p)\n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = convert_bool2(y_test)\n",
    "y_pred_ = convert_bool2(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[185  30]\n",
      " [ 34 191]]\n",
      "accuracy =  0.8545454545454545\n",
      "precision =  0.8642533936651584\n",
      "recall =  0.8488888888888889\n",
      "f1 score =  0.8565022421524665\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test_, y_pred=y_pred_))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test_, y_pred=y_pred_))\n",
    "print('precision = ', precision_score(y_true=y_test_, y_pred=y_pred_))\n",
    "print('recall = ', recall_score(y_true=y_test_, y_pred=y_pred_))\n",
    "print('f1 score = ', f1_score(y_true=y_test_, y_pred=y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\"知らんのかい\", \"お前馬鹿だろ\", \"センスないね\", \"雑魚がよ\", \"俺が馬鹿だった\",\"馬鹿みたいに笑ったw\", \"消えろカス\", \"お前頭が悪いな\", \"あの国のやつらはクズ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1e767b6da24b81804c66b051b1c84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = text2vec(tests)\n",
    "# test_res = convert_bool2( clf.predict(vec) )\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(vec,  device='cuda:0').float()\n",
    "            # 推論\n",
    "    test_res = np.array(model(X_tensor).cpu()).argmax(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 知らんのかい\n",
      "1 : お前馬鹿だろ\n",
      "0 : センスないね\n",
      "1 : 雑魚がよ\n",
      "1 : 俺が馬鹿だった\n",
      "1 : 馬鹿みたいに笑ったw\n",
      "1 : 消えろカス\n",
      "1 : お前頭が悪いな\n",
      "1 : あの国のやつらはクズ\n"
     ]
    }
   ],
   "source": [
    "for y_p, x_s in zip(test_res, tests):\n",
    "    print(\"{0} : {1}\".format(y_p, x_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここから学習データでテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error = \"Lack of sociality\"\n",
    "# errors = ['Grammatical error', \"Uninterpretable\"]\n",
    "sys_utt = []\n",
    "y_ = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if ut.is_system() and ut.is_exist_error():\n",
    "            # if not ut.utt[-1] in [\"？\", \"！\", \"。\", \"!\"]:\n",
    "            #     # sys_utt.append(ut.utt+\"。\")\n",
    "            #     sys_utt.append(ut.utt)\n",
    "            # else:   \n",
    "            sys_utt.append(ut.utt)\n",
    "            if ut.is_error_included(error):\n",
    "                y_.append(1)\n",
    "                # print(ut.utt)\n",
    "            else:\n",
    "                y_.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of y:1386, error 'Lack of sociality' counts:7\n"
     ]
    }
   ],
   "source": [
    "print(\"len of y:{0}, error '{1}' counts:{2}\".format(len(y_), error, y_.count(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989068f05be047b4bd207f4b529c14ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = text2vec(sys_utt)\n",
    "\n",
    "# y_pred_ = convert_bool2( clf.predict(vec) ) \n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(vec,  device='cuda:0').float()\n",
    "            # 推論\n",
    "    y_pred_ = np.array(model(X_tensor).cpu()).argmax(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1329   50]\n",
      " [   3    4]]\n",
      "accuracy =  0.9617604617604618\n",
      "precision =  0.07407407407407407\n",
      "recall =  0.5714285714285714\n",
      "f1 score =  0.13114754098360656\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_, y_pred=y_pred_))\n",
    "print('accuracy = ', accuracy_score(y_true=y_, y_pred=y_pred_))\n",
    "print('precision = ', precision_score(y_true=y_, y_pred=y_pred_))\n",
    "print('recall = ', recall_score(y_true=y_, y_pred=y_pred_))\n",
    "print('f1 score = ', f1_score(y_true=y_, y_pred=y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ようよう\n",
      "分かったからそう急かすな\n",
      "おはよん。\n",
      "なんで？\n"
     ]
    }
   ],
   "source": [
    "for utt, a, b in zip(sys_utt, y_,  y_pred_):\n",
    "    if a==1 and b==1:\n",
    "        print(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/social/impolite.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/social/\"\n",
    "model_name = \"impolite.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 検出は不可能でしょこれは\n",
    "\n",
    "        confusion matrix = \n",
    "        [[1314   34]\n",
    "        [   1    0]]\n",
    "        accuracy =  0.9740548554484804\n",
    "        precision =  0.0\n",
    "        recall =  0.0\n",
    "        f1 score =  0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
