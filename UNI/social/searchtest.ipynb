{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatools.analyzer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# 起動1\n",
    "# java -jar C:\\Users\\mmi-lab\\Documents\\selenium\\selenium-server-standalone-3.141.59.jar -role hub\n",
    "\n",
    "# 起動2\n",
    "# java -Dwebdriver.chrome.driver=\"C:\\Users\\mmi-lab\\Documents\\selenium\\chromedriver.exe\" -jar C:\\Users\\mmi-lab\\Documents\\selenium\\selenium-server-standalone-3.141.59.jar -role node -hub http://192.168.1.224:4444\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "# 使えそう\n",
    "# ジャンルからフレーズ・例文を探す\n",
    "# https://gogakuru.com/english/phrase/genre/index.html\n",
    "\n",
    "# \"\"\"\n",
    "print(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import spacy\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, corpus_path, name) -> None:\n",
    "        self.query = \"\"\n",
    "        self.stop_word = \"← ww ・ 「 」 #  ( （ 【 ≪ \".split()\n",
    "        self.nlp =  spacy.load('ja_ginza')\n",
    "\n",
    "        self.corpus_path = corpus_path\n",
    "        self.name = name\n",
    "\n",
    "        self.tweet_all = set()\n",
    "    \n",
    "    def connect(self, server=None):\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        if server:\n",
    "            command = \"http://{0}:4444/wd/hub\".format(server)\n",
    "            self.driver = webdriver.Remote(\n",
    "                command_executor=\"http://{0}:4444/wd/hub\".format(server),\n",
    "                options=chrome_options\n",
    "            )\n",
    "            \n",
    "    def _in(self, arg1, arg2, mode=\"or\"):\n",
    "        \"\"\"\n",
    "            arg1: str or list\n",
    "            arg2: str\n",
    "        \"\"\"\n",
    "        result = False\n",
    "        if isinstance(arg1, list):\n",
    "            if mode == \"and\":\n",
    "                for element in arg1:\n",
    "                    if isinstance(element, str):\n",
    "                        if element not in arg2:\n",
    "                            return False\n",
    "                return True\n",
    "            # or\n",
    "            for element in arg1:\n",
    "                if isinstance(element, str):\n",
    "                    # print(\"str\")\n",
    "                    if element in arg2:\n",
    "                        result = True\n",
    "                        break\n",
    "                else:\n",
    "                    if element == arg2:\n",
    "                        result = True\n",
    "                        break\n",
    "        else:\n",
    "            if isinstance(arg1, str):\n",
    "                if arg1 in arg2:\n",
    "                    result = True\n",
    "            else:\n",
    "                if arg1 == arg2:\n",
    "                    result = True\n",
    "        return result\n",
    "        \n",
    "    def query2formated(self, query, mode=\"impolite\"):\n",
    "        if mode==\"impolite\":\n",
    "            return ' \"{0}\"%20lang%3Aja%20-filter%3Alinks%20filter%3Areplies&src=typed_query'.format(query)\n",
    "        \n",
    "        else:\n",
    "            splited = query.split()\n",
    "\n",
    "            return \"(\" + \" OR \".join(splited) + \")\" + \" -filter:links\"\n",
    "    \n",
    "    def crawl(self, query, num):\n",
    "        url = \"https://twitter.com/search?f=live&q=\"\n",
    "        self.query_list = query.split()\n",
    "        self.driver.get( url+self.query2formated(query) )\n",
    "        WebDriverWait(self.driver, 10).until(EC.visibility_of_element_located((By.TAG_NAME, 'article')))\n",
    "        \n",
    "        \n",
    "        tweet_list = self.get_tweets()\n",
    "        while( len(self.tweet_all) < num ):\n",
    "            self.get_tweets()\n",
    "            # self.tweet_all =  self.tweet_all + copy.deepcopy(tweet_list)\n",
    "            self._scroll()\n",
    "            # print(len(self.tweet_all))\n",
    "            time.sleep(3)\n",
    "\n",
    "            \n",
    "        print(\"conplete crawled tweets, the size is : \", len(self.tweet_all))\n",
    "        # searchbox.send_keys(\"ポケモン\")\n",
    "        # searchbox.send_keys(Keys.ENTER)\n",
    "\n",
    "    def _scroll(self):\n",
    "        articles = self.driver.find_elements_by_tag_name('article')\n",
    "        last_art = articles[-2]\n",
    "        actions = ActionChains(self.driver)\n",
    "        actions.move_to_element(last_art)\n",
    "        actions.perform()\n",
    "    \n",
    "    def extract_info(self, data):\n",
    "        soup = BeautifulSoup(data, features='lxml')\n",
    "\n",
    "    # True だとダメ\n",
    "    def _is_invalid_tweet(self, text):\n",
    "        # return (self._in(self.query_list, text) and  not self._in(self.stop_word, text) )\n",
    "        \n",
    "        if not self._in(self.query_list, text):\n",
    "            return True\n",
    "        \n",
    "        if self._in(self.stop_word, text):\n",
    "            return True\n",
    "        \n",
    "        # 発話が短すぎするのもだめ\n",
    "        doc = self.nlp(text)\n",
    "        if len( doc ) >= 30 or len( doc ) <= 2 :\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def get_tweets(self):\n",
    "        html = self.driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        # articles = self.driver.find_elements_by_tag_name('article')\n",
    "        articles = soup.find_all(\"article\")\n",
    "        # print(\"articles : \", len(articles))\n",
    "        tweet_list = []\n",
    "        for art in articles:\n",
    "            # contents = art.find_elements_by_css_selector('.css-901oao.css-16my406.r-1tl8opc r-bcqeeo.r-qvutc0')\n",
    "            # contents = art.find_elements_by_tag_name('span')\n",
    "            cell = art.find(\"div\",{'lang': 'ja'})\n",
    "            try:\n",
    "                contents = cell.find_all(\"span\")\n",
    "                text = \"\"\n",
    "                for content in contents:\n",
    "                    text += content.get_text().replace('\\n','')\n",
    "\n",
    "\n",
    "                # しっかりと正規化\n",
    "                text = clean_text(text)\n",
    "                if text in self.tweet_all:\n",
    "                    continue\n",
    "                # チェック\n",
    "                if not self._is_invalid_tweet(text):\n",
    "                    # print(text)\n",
    "                    # tweet_list.append(text)\n",
    "                    self.tweet_all.add(text)\n",
    "\n",
    "                    if len(self.tweet_all) % 30 == 0:\n",
    "                        print(\"crawled : \", len(self.tweet_all))\n",
    "            except:\n",
    "                print(\"not japanese\")\n",
    "            # print(contents.get_text())\n",
    "        # print(tweet_list)\n",
    "        # return tweet_list\n",
    "\n",
    "    def save_tweet(self):\n",
    "        with open(self.corpus_path+self.name, \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for t in list(self.tweet_all):\n",
    "                writer.writerow([\"0\", t])\n",
    "\n",
    "    def load_tweet(self, name):\n",
    "        with open(self.corpus_path+name, \"r\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            for r in reader:\n",
    "                self.tweet_all.add(r[1])\n",
    "        \n",
    "        print(\"success loading from {0}, the size is {1}\".format(self.corpus_path+name, len(self.tweet_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../../corpus/twitter/\"\n",
    "name = \"hate3.csv\"\n",
    "crawler = Crawler(out_path, name)\n",
    "crawler.connect(\"192.168.1.224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler.load_tweet(\"hate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawled :  30\n",
      "crawled :  60\n",
      "crawled :  90\n",
      "crawled :  120\n",
      "crawled :  150\n",
      "crawled :  180\n",
      "crawled :  210\n",
      "crawled :  240\n",
      "crawled :  270\n",
      "crawled :  300\n",
      "crawled :  330\n",
      "crawled :  360\n",
      "crawled :  390\n",
      "crawled :  420\n",
      "crawled :  450\n",
      "crawled :  480\n",
      "crawled :  510\n",
      "crawled :  540\n",
      "crawled :  570\n",
      "crawled :  600\n",
      "crawled :  630\n",
      "crawled :  660\n",
      "crawled :  690\n",
      "crawled :  720\n",
      "crawled :  750\n",
      "crawled :  780\n",
      "crawled :  810\n",
      "crawled :  840\n",
      "crawled :  870\n",
      "crawled :  900\n",
      "crawled :  930\n",
      "crawled :  960\n",
      "crawled :  990\n",
      "crawled :  1020\n",
      "crawled :  1050\n",
      "crawled :  1080\n",
      "crawled :  1110\n",
      "crawled :  1140\n",
      "crawled :  1170\n",
      "crawled :  1200\n",
      "crawled :  1230\n",
      "crawled :  1260\n",
      "crawled :  1290\n",
      "crawled :  1320\n",
      "crawled :  1350\n",
      "crawled :  1380\n",
      "crawled :  1410\n",
      "crawled :  1440\n",
      "crawled :  1470\n",
      "crawled :  1500\n",
      "crawled :  1530\n",
      "crawled :  1560\n",
      "crawled :  1590\n",
      "crawled :  1620\n",
      "crawled :  1650\n",
      "crawled :  1680\n",
      "crawled :  1710\n",
      "crawled :  1740\n",
      "crawled :  1770\n",
      "crawled :  1800\n",
      "crawled :  1830\n",
      "crawled :  1860\n",
      "crawled :  1890\n",
      "crawled :  1920\n",
      "crawled :  1950\n",
      "crawled :  1980\n",
      "crawled :  2010\n",
      "crawled :  2040\n",
      "crawled :  2070\n",
      "crawled :  2100\n",
      "crawled :  2130\n",
      "crawled :  2160\n",
      "crawled :  2190\n",
      "crawled :  2220\n",
      "crawled :  2250\n",
      "crawled :  2280\n",
      "crawled :  2310\n",
      "crawled :  2340\n",
      "crawled :  2370\n",
      "crawled :  2400\n",
      "crawled :  2430\n",
      "crawled :  2460\n",
      "crawled :  2490\n",
      "crawled :  2520\n",
      "crawled :  2550\n",
      "crawled :  2580\n",
      "crawled :  2610\n",
      "crawled :  2640\n",
      "crawled :  2670\n",
      "crawled :  2700\n",
      "crawled :  2730\n",
      "crawled :  2760\n",
      "crawled :  2790\n",
      "crawled :  2820\n",
      "crawled :  2850\n",
      "crawled :  2880\n",
      "crawled :  2910\n",
      "crawled :  2940\n",
      "crawled :  2970\n",
      "crawled :  3000\n",
      "conplete crawled tweets, the size is :  3003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crawler.crawl(\"お前ら\", 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.save_tweet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crawler.tweet_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['お前ら']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"お前ら\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
