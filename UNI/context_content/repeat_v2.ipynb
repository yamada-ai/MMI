{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.366666666666666"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "191*120/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章ごとに n-gram を考えてみる\n",
    "def get_ngram_set(doc, N=3):\n",
    "    if isinstance(doc, str):\n",
    "        doc = nlp(doc)\n",
    "    surfaces = [token.text for token in doc]\n",
    "    ngram_set = set()\n",
    "    filled = [\"FOS\", *surfaces, \"EOS\"]\n",
    "    # print(filled)\n",
    "    for i in range(len(filled)-N+1):\n",
    "        f = \"_\".join(filled[i:i+N])\n",
    "        ngram_set.add(f)\n",
    "    return ngram_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repeat_rate(target:set, history:list, border=0.7):\n",
    "    t_list = list(target)\n",
    "    for prev_set in history:\n",
    "        size = len(prev_set)\n",
    "        hit = 0\n",
    "        for t in t_list:\n",
    "            if t in prev_set:\n",
    "                hit+=1\n",
    "        if hit/size >= border:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Repetition\"\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        # \n",
    "        if ut.is_exist_error():\n",
    "            if ut.is_error_included(error):\n",
    "                # print(ut.errors)\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1386"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_path = \"../../corpus/wiki/idf_wiki_v2.json\"\n",
    "with open(idf_path, \"r\") as f:\n",
    "    idf_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_under_idf_border_morpheme(text, idf_border):\n",
    "    for token in mecab_tokenize(text):\n",
    "        key = token\n",
    "        if key in idf_dict:\n",
    "            if idf_dict[key] > idf_border:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repeat_rate_ret_i(target:set, history:list, border=0.7):\n",
    "    t_list = list(target)\n",
    "    for i, prev_set in enumerate(history):\n",
    "        size = len(prev_set)\n",
    "        hit = 0\n",
    "        for t in t_list:\n",
    "            if t in prev_set:\n",
    "                hit+=1\n",
    "        if hit/size >= border:\n",
    "            return i\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:26<00:00,  7.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "y_pred = []\n",
    "border = 0.8\n",
    "\n",
    "border_10 = 14.62592894814336\n",
    "border_1 = 9.88253708481772\n",
    "border_005 = 5.340630805533431\n",
    "\n",
    "pair_list = []\n",
    "\n",
    "for conv in tqdm(convs):\n",
    "    ngram_sets = []\n",
    "    prev_sents = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        if ut.is_exist_error():\n",
    "            y_pred.append(0)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            # idf が小さい場合は，スルーしましょう\n",
    "            if is_under_idf_border_morpheme(sent.orth_, border_005):\n",
    "                # print(sent)\n",
    "                continue\n",
    "            ngram_set = get_ngram_set(sent, N=3)\n",
    "            # これまでのセットで重複が大きいものがあるかチェック！\n",
    "            if ut.is_exist_error():\n",
    "                ngram_checked = check_repeat_rate_ret_i(target=ngram_set, history=ngram_sets, border=0.8)\n",
    "                # if check_repeat_rate_ret_i(target=ngram_set, history=ngram_sets, border=0.8):\n",
    "                    # print(ut, ut.errors)\n",
    "                if ngram_checked >= 0:\n",
    "                    y_pred[-1] = 1\n",
    "                    pair_list.append([sent.orth_, prev_sents[ngram_checked], \"ngram\"])\n",
    "\n",
    "                if y_pred[-1] == 0:\n",
    "                    for i, prev in enumerate( prev_sents ) :\n",
    "                        if Levenshtein.ratio(sent.orth_, prev) >= border:\n",
    "                            pair_list.append([sent.orth_, prev_sents[i], \"leven\"])\n",
    "                            y_pred[-1] = 1\n",
    "                \n",
    "\n",
    "            ngram_sets.append(ngram_set)\n",
    "            prev_sents.append(sent.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1296   34]\n",
      " [  10   46]]\n",
      "accuracy =  0.9682539682539683\n",
      "precision =  0.575\n",
      "recall =  0.8214285714285714\n",
      "f1 score =  0.676470588235294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram  border 0.8\n",
    "\n",
    "        confusion matrix = \n",
    "        [[2131   21]\n",
    "        [  29   19]]\n",
    "        accuracy =  0.9772727272727273\n",
    "        precision =  0.475\n",
    "        recall =  0.3958333333333333\n",
    "        f1 score =  0.4318181818181817\n",
    "\n",
    "    - 再現率が欲しい\n",
    "\n",
    "- Levenshtein bordr 0.8\n",
    "    \n",
    "        confusion matrix = \n",
    "        [[2102   50]\n",
    "        [  19   29]]\n",
    "        accuracy =  0.9686363636363636\n",
    "        precision =  0.3670886075949367\n",
    "        recall =  0.6041666666666666\n",
    "        f1 score =  0.45669291338582674\n",
    "\n",
    "    - 悪くないが，適合率が低い\n",
    "\n",
    "- ngram  border 0.8 and Levenshtein bordr 0.8 and idf 005%\n",
    "    - 仮定 : 「そうですよね」のように，全く情報量がない発話が繰り返された場合は不要な繰り返しの可能性が低い\n",
    "        - idf 辞書の活用\n",
    "    - 予想 : 誤検出が減る\n",
    "    \n",
    "            confusion matrix = \n",
    "            [[2091   61]\n",
    "            [  15   33]]\n",
    "            accuracy =  0.9654545454545455\n",
    "            precision =  0.35106382978723405\n",
    "            recall =  0.6875\n",
    "            f1 score =  0.46478873239436624\n",
    "\n",
    "\n",
    "    - 評価データでの実行\n",
    "            \n",
    "            confusion matrix = \n",
    "            [[2080   64]\n",
    "            [  10   46]]\n",
    "            accuracy =  0.9663636363636363\n",
    "            precision =  0.41818181818181815\n",
    "            recall =  0.8214285714285714\n",
    "            f1 score =  0.5542168674698795\n",
    "\n",
    "\n",
    "    - 誤検出が増えたが，未検出が減った\n",
    "        - len(sent) <3 の中に，情報量があり，似た発話があるが不要ではない\n",
    "        - len(sent) <3 の中に検出するべきエラーが存在した\n",
    "    \n",
    "    - 今後\n",
    "        - どのような発話が雑談において不要ではないのかの検討が出来ていない\n",
    "            - 共起辞書が活用できるか？\n",
    "                - データを見た限り，そう簡単にはいかない\n",
    "        - 非常に近い距離で，動詞や形容詞だけが違うという場合が検出出来ていない\n",
    "            - 検出された発話中で，名詞や動詞などを比較するべきか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "よく検出した！えらいぞ 1470646494\n",
      "['こんばんは！海へ行きたいね。', 'そうですね。最近とても暑いですから。', '海に行かないんですね？', '休日に行きたいと思います。', '海は大丈夫ですね', 'はい。あなたは海に行きますか？', '海は危険です', '何故ですか？', '海はいけないです', 'そうですか。山に行くのはどうでしょうか？', 'わあ！いいですね！', '山はお好きなのですか？', 'わあ！いいですね！']\n",
      "['わあ！', 'わあ！', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1470623274\n",
      "['こんにちは。夏といえばスイカだね。', '美味しいですね。', '美味しいよ', 'でも高いですね', 'いくら', '1000円ぐらいかな？', 'スイカは実がなってたことあります', '意味が分かりません', 'あなたの好きな映画について教えてください', 'サスペンスが好きです', '映画はヒューマン系が好きですね', '映画の題名は？', '映画にはまるのです', '好きな俳優さんはいますか？', '映画が楽しみです', 'そうですか', 'あはい', '他に好きなものは？', '映画が楽しみですね']\n",
      "['いいですね！', 'いいですね！', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624021\n",
      "['映画が楽しみですね', '映画が楽しみです', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624021\n",
      "['ラーメンは美味しいですね', 'ラーメンはスープが美味しいですね', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1502868470\n",
      "['こんにちは！海へ行きたいね。', '海いいですね', '気持ちがいいですね', '海いいですね！大好きです。', '海は好きですね', 'どこの海がお勧めですか？', '海は楽しいですね', 'でも私は泳げないんです。', '海は大好きですね', 'そういえば、山はどうですか？', '海はサイコーですね', '海外の海もいいものですよね', '海はキレーですね', '魚もおいしいですよね', '魚はおいんですか？？海は素晴らしいですね', '船旅もいいらしいですね', '海は素晴らしいですね']\n",
      "['麺が美味しいですね', '麺が美味しいですね', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624007\n",
      "['海は素晴らしいですね', '海は素晴らしいですね', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624301\n",
      "['予防は難しいですね', '予防はいいですね', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1550628547\n",
      "['こんにちは。気軽に声かけてね。', '今日はいい天気ですね。', 'どこか出かけるんですか？', '九州に旅行に行きます。', '九州のうまいものめぐりをしに旅行に行きたいと常日頃から思ってるんだよ。', '何が一番食べたいですか？', '一番人気の海外旅行は韓国のソウルらしいよ。', '海外旅行の経験はありますか？', '海外旅行も楽しいかな。', '興味があるスポーツは何ですか？', '今は、してないですね。', '学生時代に何かスポーツしていましたか？', '今は、してないですね。']\n",
      "['家族で食べに行きますか？', '家族で食事に行きますか？', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470624084\n",
      "['こんにちは。ちょうど退屈してたんだ。', 'わたしもちょうど退屈していました。お話しましょう。', '好きな食べ物はなんですか？', 'チョコレートです。あなたの好きな食べ物はなんですか？', '食べ物は和食が好きですね', 'わたしも和食好きです。ごはんに味噌汁がいいですね。あなたは味噌汁の具はなにが好きですか？', '一緒がいいですね', '私は大根の味噌汁が好きです', '味噌汁がご飯にいいですね', 'これに納豆でもあれば、最高ですね！', '納豆は最高ですか。食べるのがいいですね', 'ねぎとからしを入れた納豆最高ですよ。', '食べるのがいいですね']\n",
      "['今は、してないですね。', '今は、してないですね。', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1502868462\n",
      "['こんにちは！夏といえばスイカだね。', 'スイカは大好きだよ。', 'スイカは大好きですか。スイカは実が大きいですね', '子玉スイカもあるみたい。', 'そういえば、あなたは美味しいものを食べるのが好きでしたよね？スイカは大好きですね', 'ぶどうやメロンも好きです。', 'メロンは好きですか。スイカは大好きですね']\n",
      "['食べるのがいいですね', '食べるのがいいですね', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1502868462\n",
      "['スイカは大好きですね', 'スイカは大好きですね', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1502868462\n",
      "['こんにちは！夏といえばスイカだね。', 'スイカは大好きだよ。', 'スイカは大好きですか。スイカは実が大きいですね', '子玉スイカもあるみたい。', 'そういえば、あなたは美味しいものを食べるのが好きでしたよね？スイカは大好きですね', 'ぶどうやメロンも好きです。', 'メロンは好きですか。スイカは大好きですね', 'メロンとスイカならスイカの方が好きです。', 'メロンは大好きですね', 'なしは好きですか？', 'メロンは好きですね']\n",
      "['メロンは大好きですね', 'メロンは好きですか。', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470623529\n",
      "['こんにちは！海へ行きたいね。', '夏は海だよね', '気持ちがいいですね', '気持ちいいよね', '海は好きですね', '大好きだよ。泳ぐのもいいね。', '泳ぐを繰り返しますよねー', 'そうそう、繰り返しちゃうよね。', '泳ぐを繰り返すのです', '楽しいもんね', '楽しい', '楽しいでしょ？', '泳ぐを覚えるのです', '覚えたの？泳げるようになった？', '泳ぐを覚えるんです']\n",
      "['メロンは好きですね', 'メロンは好きですか。', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470623529\n",
      "['こんにちは！海へ行きたいね。', '夏は海だよね', '気持ちがいいですね', '気持ちいいよね', '海は好きですね', '大好きだよ。泳ぐのもいいね。', '泳ぐを繰り返しますよねー', 'そうそう、繰り返しちゃうよね。', '泳ぐを繰り返すのです', '楽しいもんね', '楽しい', '楽しいでしょ？', '泳ぐを覚えるのです', '覚えたの？泳げるようになった？', '泳ぐを覚えるんです', 'どうやって覚えたの？', '泳ぐを繰り返すようですよ']\n",
      "['メロンは好きですね', 'メロンは大好きですね', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623090\n",
      "['泳ぐを覚えるんです', '泳ぐを覚えるのです', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470623090\n",
      "['こんにちは。熱中症に気をつけて。', 'こんにちは。ホントに気をつけないとね。', '熱中症に気をつけないんですか？', '気をつけていますよ。水分を多めに摂っています。', '熱中症に気をつけてるんですね。熱中症に気をつけか？', '明日は猛暑らしいですから', '猛暑は欲しいですね', 'え〜欲しいの？わたしは要らないな', '猛暑は嬉しいですね', '暑いの好きなんですね', '猛暑は欲しいですよねー']\n",
      "['泳ぐを繰り返すようですよ', '泳ぐを繰り返すのです', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623090\n",
      "['猛暑は嬉しいですね', '猛暑は欲しいですね', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1502868919\n",
      "['猛暑は欲しいですよねー', '猛暑は欲しいですね', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1502868919\n",
      "['こんにちは！気軽に声かけてね。', 'こんにちは！元気ですか？', '元気ですかは元気です', '今日はどこに行くんですか？', 'もちろんですってのが元気ですかにいくないですよ', '元気なら良かったです。', '元気は良いんですか？？元気ですかは元気か', '良いことですよ。', '元気ですかは元気ですね']\n",
      "['猛暑は欲しいですよねー', '猛暑は嬉しいですね', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623437\n",
      "['冷夏で野菜が高いです', '冷夏は野菜が高いです', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624484\n",
      "['元気ですかは元気か', '元気ですかは元気です', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470624484\n",
      "['こんにちは。ちょうど退屈してたんだ。', 'こんにちは。暑いですね。', 'ところで、気になるニュースがあったのですが、7月29日から公開された映画「シン・ゴジラ」の観客数が1日までに71万人に達し、1954年に第1作が公開された「ゴジラ」シリーズの累計観客数が1億人突破し、実写映画では初めてだそうですよ。', 'ゴジラシリーズって60年以上も続いてるんですね。他にもこんなに続いているシリーズはあるんでしょうか？', '実写映画を見るらしいですよ', 'ゴジラにはアニメもあるんですか？', '実写映画を含めるらしいです', 'とにかく世界一はいい事ですね。オリンピックは見てますか？', '実写映画に劣らないんです', '事実は映画より面白いですよ。', '実写映画に対抗だそうです', 'オリンピックは映画に対抗する気はないと思いますよ。', '実写映画を問わないそうですよ', '洋画と邦画はどちらが好きですか？', '実写映画を含めるのです']\n",
      "['元気ですかは元気ですね', '元気ですかは元気です', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624484\n",
      "['国内で死者は出てるんですね。', '国内で死者が出てるんですかね？', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623280\n",
      "['実写映画を含めるらしいです', '実写映画を見るらしいですよ', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623280\n",
      "['実写映画を含めるのです', '実写映画を含めるらしいです', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470645655\n",
      "['こんばんは！海へ行きたいね。', 'こんばんは、夏といえば海ですね。', '朝から海に行きますか？？', '朝の海も気持ちよさそうですね。', '海は気持ちよさそうですか。沖縄で行きます？', '沖縄行きたいです！', '沖縄の人は泳ぎが苦手って聞いたけど、本当ならおもしろいよね。', 'みんな泳ぎ得意そうですけどね。', '沖縄は大好きですね', '私も沖縄大好きですよ。', '沖縄はサイコーですね', '青い海と空、最高ですね！', '沖縄で有名ですね', '沖縄といえばゴーヤチャンプルですね。', 'ゴーヤチャンプルは美味しいですよねー', '私大好物です。', 'ゴーヤチャンプルは好きですね', 'ちょっと苦みがあるのがいいんですよね。', '苦みはあってるんですね。ゴーヤチャンプルは美味しいですね']\n",
      "['実写映画を見るかもしれない', '実写映画を越えるかもしれない', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470645655\n",
      "['こんばんは！海へ行きたいね。', 'こんばんは、夏といえば海ですね。', '朝から海に行きますか？？', '朝の海も気持ちよさそうですね。', '海は気持ちよさそうですか。沖縄で行きます？', '沖縄行きたいです！', '沖縄の人は泳ぎが苦手って聞いたけど、本当ならおもしろいよね。', 'みんな泳ぎ得意そうですけどね。', '沖縄は大好きですね', '私も沖縄大好きですよ。', '沖縄はサイコーですね', '青い海と空、最高ですね！', '沖縄で有名ですね', '沖縄といえばゴーヤチャンプルですね。', 'ゴーヤチャンプルは美味しいですよねー', '私大好物です。', 'ゴーヤチャンプルは好きですね', 'ちょっと苦みがあるのがいいんですよね。', '苦みはあってるんですね。ゴーヤチャンプルは美味しいですね', '他にどんな食べ物が好きですか？', 'ゴーヤチャンプルは美味しいですね']\n",
      "['天気は気持ちがいいですね', '気持ちがいいですね', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623923\n",
      "['晴れで気持ちがいいですね', '気持ちがいいですね', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1502869853\n",
      "['ゴーヤチャンプルは美味しいですね', 'ゴーヤチャンプルは美味しいですよねー', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623800\n",
      "['ゴーヤチャンプルは美味しいですね', 'ゴーヤチャンプルは好きですね', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1502868983\n",
      "['こんにちは。海へ行きたいね。', '海水浴は楽しいですよね', '海水浴は楽しいんですか？海は楽しいですね', '夏の晴れた日の海は最高ですね', '海は穏やかですね', 'でも、たまに荒ぶりますよね', '海は楽しいですね']\n",
      "['ゴーヤチャンプルは美味しいですね', 'ゴーヤチャンプルは美味しいですね', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470646042\n",
      "['お金にルーズなようですよ', 'お金にルーズです', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470646042\n",
      "['こんばんは。海へ行きたいね。', '山でキャンプはどう？', 'キャンプに備えるのです', 'キャンプで食べるお肉はおいしいよね', '宮崎で張ります', '宮崎は自然が豊かだからね', '宮崎は温かいですね', 'マンゴーが有名ですよね', 'マンゴーは甘酸っぱいですよねー', 'マンゴー好き？', 'マンゴーは味が濃いですね', '果物は好きなんですか？', '果物は食べるのがいいですね', '毎朝食べていますよ', '果物は朝がいいですね', 'バナナは栄養の吸収が良いから朝食べると良いらしいですよ', '果物は生がいいですね', 'みかんは冷凍も美味しいですよ', '冷凍は美味しいんですか？果物が良いですね', 'ヨーグルトに混ぜて食べても美味しいですよ', '果物が良いですね']\n",
      "['ごまかしはうまいですね', 'ごまかしはいいですね', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1502868564\n",
      "['こんにちはー。ちょうど退屈してたんだ。', '私もです', 'ですよねー', '何か話題ある', '最近どうですか？', '暑いね', 'ねー', '早く冬こないかなあ', 'ところで、マンボウって一度の産卵で三億個も卵を産むそうですよ。', 'たったの3億個', 'どんな音楽がお好きですか？', 'ロック', 'ロックは好きですか', 'もちろん', 'ありがとう', 'こちらこそ', 'ありがとう']\n",
      "['ビーチバレーは熱いですよねえ', 'ビーチバレーは面白いですね', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470645999\n",
      "['こんばんは。海へ行きたいね。', '海はいいですよね。', '海はいんですか？？海は楽しいですね', '夏の海は最高です。', '海は最高ですか。海は好きですね', 'あなたは海が好きですか?', 'そうですね、海は好きですよ。', '海のどこが好きですか?', '気持ちがいいですね', '私もそう思います。', 'ですよね', 'スイカなんかもいいですよね。', 'スイカは好きですね', '私も大好きなんです。', '嬉しいです！', '気が合いそうですね。', 'スイカは大好きですね']\n",
      "['海は楽しいですね', '海は楽しいですね', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624619\n",
      "['果物は生がいいですね', '果物は朝がいいですね', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1470624619\n",
      "['こんにちは！声かけてくれるのを待ってたんだ。', 'こんにちは', 'こんにちは?', '機嫌が悪いの？', '機嫌がなおりますよねえ', 'ああ。言われるとそんなことないよってあるね', '機嫌はおかしいですよねー', '期限と間違えない？', '機嫌をなおしますよねえ', 'そうそう。賞味期限を直しましょうって、おーい', '機嫌をなおすのです', '印刷？それとも本当の機嫌の方？', '機嫌をよくします', '良くなったんだ。機嫌の方ね。調子はどう？', '機嫌が悪くなってないです', '調子については聞いても良いのね。良かったよ！', '機嫌は悪いです', 'この短時間に何があったの！まあまあ、笑って笑って', '良いのが救いですけどねぇ', 'ありがとう。そうだね。こっちは君と話せるだけで機嫌は良いよ', '機嫌はいいのが救いですけどねぇ']\n",
      "['果物が良いですね', '果物が良いですね', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1470622805\n",
      "['こんにちは。気軽に声かけてね。', 'こんにちは', 'おはよ', '元気ですか？', '元気ですよ', '今日は暑いですね', '元気ですかは元気です', 'そうですか', 'ごめんね', '謝らなくていいですよ', '元気ですかは元気ですね']\n",
      "['ありがとう', 'ありがとう', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1503631428\n",
      "['スイカは大好きですね', 'スイカは好きですね', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471869912\n",
      "['機嫌をなおしますよねえ', '機嫌がなおりますよねえ', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1503631674\n",
      "['機嫌はいいのが救いですけどねぇ', '良いのが救いですけどねぇ', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471923455\n",
      "['元気ですかは元気ですね', '元気ですかは元気です', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1471916977\n",
      "['何かお話しましょうか？', 'どこから来ましたか？', 'インドと新潟なら、どちらが楽しいでしょうか？', '私はインドの方が楽しいと思います。', 'インドへ行くと、人生観が変わったりして大好きになる人と、2度と行きなくないという人と2つに分かれると言いますよね。私は女優の香川真司さんのようにインドに行く勇気がありません。', '香川真司はサッカー選手の事ですね。私はインドに行ってみたいです。', '香川真司選手が初めて海外のサッカーチームに移籍して、早々に大活躍してセグウェイにも選ばれた事にとても驚きました。', 'セグウェイとは？大活躍でしたね。', 'サッカーの香川真司選手のセグウェイジャパンへのカムバックは、本田圭祐監督をはじめ、サポーターの皆さんから歓迎されたと思います。', '日本代表のことですね。私もそう思います。', '日本で放映する場合にも、昔の声優さんは亡くなられている方が多くなってしまったので、代替わりしていますね。', 'はい、有名なところではルパン三世の声ですね。', '日本の慣習からいえば、やはり桜の下での入学、卒業の方が雰囲気が出ますからね。', '記念写真に良く使いますね。', '日本在住の海外の人や海外に住む日本人にフォーカスをあてたバラエティ番組は、最近よく見かけますね。', '確かに多いです。いろいろな人生を見れますね。', '雪合戦は日本だけでなく、「Yukigassen」の名で海外にも広まっているのですね。', 'そうなのですか。そのうちオリンピックの種目に成るかもしれませんね。', '日本で放映する場合にも、昔の声優さんは亡くなられている方が多くなってしまったので、代替わりしていますね。']\n",
      "['スイーツのヒルナンデス!は、マドリッドでも大人気のクロワッサンドーナツを最初に生み出した表参道の日本穀物検定協会というお店が、原宿のスペインに2015年の春に販売を開始するそうです。', 'は、マドリッドでも大人気のクロワッサンドーナツを最初に生み出した表参道のマクドナルドというお店が、スペインの日本に2015年の春に販売を開始するそうです。', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471917180\n",
      "['漫才コンビであるGREEN CAFE', '漫才コンビであるGREEN CAFE', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471917413\n",
      "['→Pia-no-jaC←のスポーツ番組であるニコニコ動画で初めて元陸上競技選手のあばれるくんさんが登場した時の放送をたまたま見ましたが、とてもわくわくしました。', '→Pia-no-jaC←', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471927106\n",
      "['サッカーが好きですので、ミラノでこれだけサッカーがメジャーになったのはクリスティアーノ・ロナウド選手がカズダンスなど一般でも話題になるようなネタを提供してくれたからだと思っています。', 'サッカーが好きですので、ミラノでこれだけサッカーがメジャーになったのは佐村河内守選手がカズダンスなど一般でも話題になるようなネタを提供してくれたからだと思っています。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471927106\n",
      "['日本で放映する場合にも、昔の声優さんは亡くなられている方が多くなってしまったので、代替わりしていますね。', '日本で放映する場合にも、昔の声優さんは亡くなられている方が多くなってしまったので、代替わりしていますね。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471928252\n",
      "['さんの生姜チャイは、しまなみ海道産しらすを使用した台風19号、四国みかん果汁を使用したおとなのふりかけ・愛媛みかん、愛媛産かつおを使用したおとなのふりかけ、ローソン・フタバ図書GIGA広島駅前店名物のラーメンをふりかけにしたおとなのふりかけ・ちりめんじゃこの詰め合わせ4種類が入っているみたいですね。', 'さんの生姜チャイ、中国産かつおを使用したおとなのふりかけ、しまなみ海道名物のラーメンをふりかけにしたおとなのふりかけ・ちりめんじゃこの詰め合わせ4種類が入っているみたいですね。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471916240\n",
      "['広島県をそれほど熱心に見ていたわけではないので、カップケーキのことは良く知らないのですが、瀬戸内よりもカラフルなカップケーキがあることは知っています。', 'いかめしをそれほど熱心に見ていたわけではないので、カップケーキのことは良く知らないのですが、瀬戸内よりもカラフルなカップケーキがあることは知っています。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471916240\n",
      "['俳優の小籔千豊さんといえば、TBSのドラマで放送されたWOWOWの宮川大輔役のイメージが強く、その役に成りきられるイメージが強いです。', '俳優の小籔千豊さんといえば、TBSのドラマで放送された珍獣ハンターイモトの宮川大輔役のイメージが強く、その役に成りきられるイメージが強いです。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1550793521\n",
      "['TBSで放送されていた噂の現場直行', 'TBSで放送されていた', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471927839\n",
      "['TBSで放送されていた', 'TBSで放送されていた', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471917265\n",
      "['さんの生姜チャイは、愛媛産しらすを使用した台風19号、四国みかん果汁を使用したおとなのふりかけ・愛媛みかん、香川県産かつおを使用したおとなのふりかけ・ちりめんじゃこ、徳島名物のラーメンをふりかけにしたおとなのふりかけ・徳島ラーメン風の詰め合わせ4種類が入っているみたいですね。', 'さんの生姜チャイ、香川県産かつおを使用したおとなのふりかけ・ちりめんじゃこ、徳島名物のラーメンをふりかけにしたおとなのふりかけ・徳島ラーメン風の詰め合わせ4種類が入っているみたいですね。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1503291271\n",
      "['私が日本穀物検定協会というメーカーの清涼飲料水を買うのは、以前JR東日本へ行ったことがあるので、JR東日本産のものを買いたくなってしまう、ということくらいで選んでいるんです。', '私がチャーリーズというメーカーの清涼飲料水を買うのは、以前JR東日本へ行ったことがあるので、JR東日本産のものを買いたくなってしまう、ということくらいで選んでいるんです。', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1550565796\n",
      "['こんにちは。好きな食べ物は何ですか？', 'グラタンですね', '可愛い・・・。グラッド、良い名前ですね。', 'グラッドじゃないよ', '生後三ヶ月ですね。片手に乗りますよ。', 'グラッドってなに？', '耳が熱いか冷たいか', 'それって質問してるの？', '近くに遊びに行く時はコンパクトカメラで撮りますし、「写真を撮りに行くぞっ！」って時は、デジ一を持って行ったりもします。', '写真好きなの？', 'でもきっと写真屋さんで育たれたのなら”良い写真”というのがわかっているから、普通の人が撮ろうとする写真とは構図とか違うんじゃないですか？', '構図についてそこまで詳しく勉強したことないなぁ', 'そうですね。私もミスチルは大好きで', 'ミスチルは別に好きじゃないかな', 'あはは本当ですね。（←大阪の友達に怒られそう…）東京のもんじゃも似てますけどね。', 'もんじゃよりお好み焼きが好きです', 'あはは本当ですね。（←大阪の友達に怒られそう…）東京のもんじゃも似てますけどね。']\n",
      "['日本テレビに来春アナウンサーとして入社予定だった南原清隆さんが内定取消をされたニュースをどう思いますか？ 。', '日本テレビに来春アナウンサーとして入社予定だった南原清隆さんが内定取消をされたニュースは最近まで話題でしたよね。', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1550567546\n",
      "['こんにちは。音楽はどんなのを聴かれていますか？', 'こんにちは。ヒーリングが好きです。', 'こんにちは。']\n",
      "['ジブリの作品は子供から大人まで楽しめますね。', 'ジブリの作品は子供から大人まで楽しめますね。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471415606\n",
      "['デラックスさんが出演するテレビ番組で一番好きなのは、望月製紙の番組であるホンマでっか！？', 'ホンマでっか！？', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1471400749\n",
      "['こんにちは。最近のマイブームは何ですか？', 'こんにちわ。マイブーム、特にないんだよね。', 'こんにちは。']\n",
      "['豆腐ですか。', '豆腐です。', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1471415240\n",
      "['こんにちは。好きな食べ物は何ですか？', 'こんにちは。鶏肉だよ。君は？', '確か途中から来てましたよ。でかかったです。', '何の話？', '好きな食べ物は？']\n",
      "['あはは本当ですね。', 'あはは本当ですね。', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1503293520\n",
      "['こんにちは。スポーツは何かしてますか？', '特に何もしていないです', '地下鉄で。名大前ができてすごく便利になりましたね、ここの大学。', '名古屋についてはほとんど知らないです', 'そうかなー名古屋のどこ遊びに行ったりするの？', '名古屋には遊びに行ったことはないです', '人の多いところは苦手なので・・・万博は結構行きましたよ。', '万博はどんな感じでしたか？', '何だか取り留めのない朝食の話をしました。', '朝食は何を食べましたか？', 'あなたは朝方', '朝型のことですか？', 'そうですねえ', 'どちらかと言えば朝型だと思います', 'そうですねえ']\n",
      "['（←大阪の友達に怒られそう…）', '（←大阪の友達に怒られそう…）', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1503293520\n",
      "['東京のもんじゃも似てますけどね。', '東京のもんじゃも似てますけどね。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1503293520\n",
      "['こんにちは。', 'こんにちは。', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1471414705\n",
      "['こんにちは。好きな動物は何ですか？', 'ネコが好きです', 'かわいいですよね', 'あなたの好きな動物は？', 'ペットは飼ってませんが動物は好きです。', 'どんな動物が好きなんですか？', 'ペットは飼ってませんが動物は好きです。']\n",
      "['そうですね。', 'そうですね。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471414705\n",
      "['こんにちは。', 'こんにちは。', 'ngram']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471400579\n",
      "['好きな食べ物は？', '好きな食べ物は何ですか？', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1471414296\n",
      "['こんにちは。あなたはいつも家では何をしてますか？', 'インターネットをしてることが多いです。', 'なるほど、インターネットは便利ですよね。', 'あなたはどんなことをしているんですか？', '最近だと旅行にはまりだしたので、旅代で消えてしまいますね。', '旅行良いですね。私は最近沖縄に行きましたよ。', 'いいですね', '暑かったですが楽しかったです。あなたはどんなところへ行ったことがありますか？', '最近だと旅行にはまりだしたので、旅代で消えてしまいますね。']\n",
      "['そうですねえ', 'そうですねえ', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1471400446\n",
      "['こんにちは。スポーツは何かしてますか？', 'こんにちは。たまにヨガをしています。ヨガはやったことがありますが？', 'ヨガちょっとやってみたいです！やっぱりやり始めてからは体の調子いいですか？お風呂上がりにストレッチやろうと思うのですが、なかなか続かなくて。', 'こんにちは。たまにヨガをしています。ヨガはやったことがありますが？', 'ヨガちょっとやってみたいです！やっぱりやり始めてからは体の調子いいですか？お風呂上がりにストレッチやろうと思うのですが、なかなか続かなくて。']\n",
      "['そうですね。', 'そうですねえ', 'leven']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471420297\n",
      "['そうですね。', 'そうですねえ', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1471401010\n",
      "['こんにちは。最近気になることとか、はまっていることとかありますか？', '車を軽自動車に買い替えようかな。', '大きい買い物だから悩みますよね。', 'そうなんだ、気にいった車を探してんだよ', 'いま住んでるところから駅がちょっと遠いので駅までは車ですよ。', 'じゃあ、毎日車に乗っているんだね', 'いま住んでるところから駅がちょっと遠いので駅までは車ですよ。']\n",
      "['ＵＳＪでですか？', 'ＵＦＪですか？', 'leven']\n",
      "\n",
      "よく検出した！えらいぞ 1471420471\n",
      "['こんにちは。よくチャットはやられるんですか？', 'こんにちは', 'こんにちは、よろしくお願いします', 'チャットは好きですか', '慣れてる方ですね', 'どんなときにチャットをしますか', '何だか取り留めのない朝食の話をしました。', '取り留めのない話とは何ですか', 'なるほどー。てか、初めてなんですね。', 'チャットをどれくらいしますか', '慣れてる方ですね']\n",
      "['ペットは飼ってませんが動物は好きです。', 'ペットは飼ってませんが動物は好きです。', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1503290726\n",
      "['こんにちは。最近のマイブームは何ですか？', '懸賞だよ', '当たらないだろうと思って出していません。', '懸賞だよ', '当たらないだろうと思って出していません。']\n",
      "['海水浴です。', '海水浴です。', 'ngram']\n",
      "\n",
      "よく検出した！えらいぞ 1503290726\n",
      "['こんにちは。最近のマイブームは何ですか？', '懸賞だよ', '当たらないだろうと思って出していません。', '懸賞だよ', '当たらないだろうと思って出していません。', 'ごめんね。2回押しちゃった。でも、賢くなったね', '学習能力ゼロです。ははは', 'そんなことないよ', 'あんな風に、別売りしているんですね', '何を', '地下鉄で。名大前ができてすごく便利になりましたね、ここの大学。', '何を別売りしているの？', '地下鉄で。名大前ができてすごく便利になりましたね、ここの大学。']\n",
      "['魚も見えましたよ', '魚も見えましたよ', 'ngram']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for conv in convs:\n",
    "    conv_list = []\n",
    "    for ut in conv:\n",
    "        conv_list.append(ut.utt)\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        # 本来エラーではないけどエラー扱い\n",
    "        if ut.is_exist_error():\n",
    "            if y[i]==0 and y_pred[i]==1:\n",
    "                print(\"本来エラーではないけどエラー扱い\", ut.did)\n",
    "                print(pair_list[j])\n",
    "                print()\n",
    "            # if y[i]==1 and y_pred[i]==0:\n",
    "            #     print(\"本来エラーなのに非エラー扱い\", ut.did)\n",
    "            #     print(conv_list)\n",
    "            #     print()\n",
    "            if y[i]==1 and y_pred[i]==1:\n",
    "                print(\"よく検出した！えらいぞ\", ut.did)\n",
    "                print(conv_list)\n",
    "                print(pair_list[j])\n",
    "                print()\n",
    "            \n",
    "            if y_pred[i]==1:\n",
    "                j += 1\n",
    "            \n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起辞書の活用\n",
    "ppmi_dataname = \"../../corpus/collocation/ppmi_ntt1\"\n",
    "ppmi_matrix = np.load(ppmi_dataname+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictname = \"../../corpus/collocation/word_dict/ppmi_word_dict_ntt1.json\"\n",
    "with open(dictname, \"r\") as f:\n",
    "    word_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(ppmi_matrix, word_dict, x, y):\n",
    "    if x not in word_dict.keys():\n",
    "        return 0\n",
    "    elif y not in word_dict.keys():\n",
    "        return 0\n",
    "\n",
    "    x_id = word_dict[x]\n",
    "    y_id = word_dict[y]\n",
    "    return ppmi_matrix[x_id, y_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi(ppmi_matrix, word_dict, \"祭り\", \"掛け声\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colocate_rate(utt1, utt2):\n",
    "    left_1 = filtering( nlp(utt1) , toyoshima_set)\n",
    "    left_2 = filtering( nlp(utt2) , toyoshima_set)\n",
    "    \n",
    "    # 名詞がないよ\n",
    "    if left_1[0] == \"[NONE]\" or  left_2[0] == \"[NONE]\":\n",
    "        return 0\n",
    "\n",
    "    # print(left_1, left_2)\n",
    "    rate = 0\n",
    "    for lx in left_1:\n",
    "        for ly in left_2:\n",
    "            rate += ppmi(ppmi_matrix, word_dict, lx, ly)\n",
    "    rate = rate/(len(left_1)+len(left_2))\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1489] 2022-01-16 11:57:28,977 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/model.vec\n",
      "[1489] 2022-01-16 11:58:33,262 Info gensim.utils :KeyedVectors lifecycle event {'msg': 'loaded (351122, 300) matrix of type float32 from ../../corpus/w2v/model.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-01-16T11:58:33.262489', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "# fasttext\n",
    "# https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "# w2v_name =  \"dep-ja-300dim\"\n",
    "w2v_name =  \"model.vec\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyoshima_set = set(\"NOUN PROPN VERB ADJ\".split())\n",
    "\n",
    "def filtering(doc, filter_set):\n",
    "    left = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in filter_set:\n",
    "            left.append(token.lemma_)\n",
    "    return left if len(left)>0 else [\"[NONE]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(word, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    if word in SYMBOL_w2v:\n",
    "        vector = SYMBOL_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOL_w2v[\"[UNK]\"]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\", \"[NONE]\"]\n",
    "add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "SYMBOL_w2v = dict(zip(add_keys, add_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec2(doc, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    left = filtering(doc, independent_set)\n",
    "    return np.mean([ w2v(w, w2v_model, SYMBOL_w2v) for w in left], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2formated(sen, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    docs = sentence2docs(sen, sents_span=False)\n",
    "    vector = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        if i==0:\n",
    "            prev_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "        else:\n",
    "            current_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "            diff_vec = np.abs(prev_vector-current_vector)       \n",
    "            vector.append( diff_vec)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec_diff(sen, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    docs = sentence2docs(sen, sents_span=False)\n",
    "    vector = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        if i==0:\n",
    "            # prev_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "            prev_left= set( filtering(doc, independent_set) ) \n",
    "        else:\n",
    "            # current_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "            currtnt_left = set ( filtering(doc, independent_set) )   \n",
    "            prev_diff = currtnt_left - prev_left\n",
    "            current_diff = prev_left - currtnt_left\n",
    "            if len(prev_diff)==0:\n",
    "                prev_v = 0\n",
    "            else:\n",
    "                prev_v = np.mean([ w2v(w, w2v_model, SYMBOL_w2v) for w in list(prev_diff)], axis=0)\n",
    "            if len(current_diff)==0:\n",
    "                current_v = 0\n",
    "            else:\n",
    "                current_v = np.mean([ w2v(w, w2v_model, SYMBOL_w2v) for w in list(current_diff)], axis=0)\n",
    "            vector.append( np.abs(prev_v-current_v)   )\n",
    "    return vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_plain(test, pred):\n",
    "    return f1_score(y_true=test, y_pred=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eval border : 3.1\n",
    "- train border : 4.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:01<00:52,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:0.6923076923076924, border:2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:06<00:45,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:0.7102803738317757, border:2.549999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:11<00:40,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:0.7222222222222223, border:2.799999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [00:17<00:34,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:0.7272727272727273, border:3.099999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:51<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "border_w2v = 2.3\n",
    "max_f1 = 0\n",
    "epoch = 50\n",
    "gamma = 0.05\n",
    "for k in tqdm(range(epoch)):\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    y_pred2 = []\n",
    "    for conv in convs:\n",
    "        conv_list = []\n",
    "        for ut in conv:\n",
    "            conv_list.append(ut.utt)\n",
    "            if not ut.is_system():\n",
    "                continue\n",
    "            # 本来エラーではないけどエラー扱い\n",
    "            if ut.is_exist_error():\n",
    "                if y_pred[i]==1:\n",
    "                    # ペアをベクトル化し，差分を計算\n",
    "                    # print(pair_list[j][:2])\n",
    "                    vector = sentence2vec_diff(pair_list[j][:2], w2v_model, SYMBOL_w2v)[0]\n",
    "                    # print(np.linalg.norm(vector))\n",
    "                    if np.linalg.norm(vector) > border_w2v:\n",
    "                        y_pred2.append(0)\n",
    "                        # if y[i] == 1:\n",
    "                        #     print( pair_list[j][:2] ) \n",
    "                    else:\n",
    "                        y_pred2.append(1)\n",
    "                    j += 1\n",
    "                else:\n",
    "                    y_pred2.append(0)\n",
    "                \n",
    "                i+=1\n",
    "    f1 = f1_score_plain(y, y_pred2)\n",
    "    # print(\"f1 score:{0}, border:{1}\".format(max_f1, border_w2v))\n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        print(\"f1 score:{0}, border:{1}\".format(max_f1, border_w2v))\n",
    "    border_w2v += gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['天ぷらは食べるのが好きですね', '天ぷらは食べるのが美味しいですね']\n",
      "['かけ声をだしたことありますね', 'かけ声を覚えたことあります']\n",
      "['かけ声をあげるかもしれない', 'かけ声を覚えるかもしれない']\n",
      "['映画は音楽がいいですね', '映画は一人がいいですね']\n",
      "['仲間由紀恵は美しいですね', '仲間由紀恵は怖いですね']\n",
      "['コミュニケーションは得意です', 'コミュニケーションは難しいですね']\n",
      "['料理は下手です', '料理は不得手です']\n",
      "['イタリア料理は得意ですね', 'イタリア料理は大好きですね']\n",
      "['塩分に気をつけないんですか？', '熱中症に気をつけないんですか？']\n",
      "['だから荒野というドラマのこれまでの回がNHKで再放送されたら、ぜひ見てみたいです。', '紙の月というドラマのこれまでの回がNHKで再放送されたら、ぜひ見てみたいです。']\n",
      "['以前にインターネットの通販サイトの広島てっぱんグランプリで送料無料キャンペーンの時に広島の牡蠣も購入したことがあります。', '以前にインターネットの通販サイトのハイチュウで送料無料キャンペーンの時に広島の牡蠣も購入したことがあります。']\n",
      "['ぼくはくまに参加するアーティストは、宇多田ヒカルさん、ゴッホさん、フェルメールさん、フェルーメルさん、井上陽水さん、椎名林檎さんほか、とても豪華な13組です。', 'Music Key betaに参加するアーティストは、宇多田ヒカルさん、フェルメールさん、ゴッホさん、井上陽水さん、フェルーメルさん、椎名林檎さんほか、とても豪華な13組です。']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "y_pred2 = []\n",
    "for conv in convs:\n",
    "    conv_list = []\n",
    "    for ut in conv:\n",
    "        conv_list.append(ut.utt)\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "            # 本来エラーではないけどエラー扱い\n",
    "        if ut.is_exist_error():\n",
    "            if y_pred[i]==1:\n",
    "                    # ペアをベクトル化し，差分を計算\n",
    "                    # print(pair_list[j][:2])\n",
    "                vector = sentence2vec_diff(pair_list[j][:2], w2v_model, SYMBOL_w2v)[0]\n",
    "                    # print(np.linalg.norm(vector))\n",
    "                if np.linalg.norm(vector) > 4.15:\n",
    "                    y_pred2.append(0)\n",
    "                    if y[i] == 0:\n",
    "                        print( pair_list[j][:2] ) \n",
    "                else:\n",
    "                    y_pred2.append(1)\n",
    "                j += 1\n",
    "            else:\n",
    "                y_pred2.append(0)\n",
    "                \n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1308   22]\n",
      " [  16   40]]\n",
      "accuracy =  0.9725829725829725\n",
      "precision =  0.6451612903225806\n",
      "recall =  0.7142857142857143\n",
      "f1 score =  0.6779661016949152\n"
     ]
    }
   ],
   "source": [
    "score(y, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../X_y_data/context_content/\"\n",
    "data_name = \"repeat.pickle\"\n",
    "dataM = DataManager(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/context_content/repeat.pickle\n"
     ]
    }
   ],
   "source": [
    "dataM.save_data(data_name, [y, y_pred2])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
