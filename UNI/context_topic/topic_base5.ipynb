{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy_4test(convs, N=4):\n",
    "    errors = [\"Topic transition error\", 'Lack of information', 'Unclear intention']\n",
    "    # errors = errors[:1]\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in convs:\n",
    "        dialogue = [\"\"]*N\n",
    "        for i, ut in enumerate( conv ) :\n",
    "            # ユーザ発話駆動\n",
    "            dialogue.append(clean_text( ut.utt) )\n",
    "            if ut.is_exist_error():\n",
    "                X.append( dialogue[-N:] )\n",
    "                    # X.append(dialogue[-N:])\n",
    "                if ut.is_error_included(errors) :\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = [\"Topic transition error\", 'Lack of information', 'Unclear intention']\n",
    "# errors[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 4\n",
    "X_str, y = make_Xy_4test(convs, N=N)\n",
    "y.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1349"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, vocab_dict):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim , tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        tag_space = self.hidden2tag(torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 ))\n",
    "        y =self.softmax(tag_space)\n",
    "        return y\n",
    "    \n",
    "    def last_context(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # print(emb1.shape)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        context = torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 )\n",
    "        return context\n",
    "    \n",
    "    def text2context(self, text):\n",
    "        if isinstance(text, str):\n",
    "            utt_id = self._sentence2ids(text, self.vocab_dict)\n",
    "            utt_id_tensor = torch.tensor( [utt_id] , device='cuda:0', dtype=torch.int)\n",
    "            # utt_id_tensor = torch.tensor( [utt_id] , device='cpu', dtype=torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        if isinstance(text, list):\n",
    "            X = self._make_X(text, self.vocab_dict)\n",
    "            utt_id_tensor = X.to(torch.int).cuda()\n",
    "            # utt_id_tensor = X.to(torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def _sentence2ids(self, sentence:str, vocab_dict:dict):\n",
    "        doc = self._sentence2formated(sentence)\n",
    "        ids = np.zeros(len(doc))\n",
    "        for i, key in enumerate(doc):\n",
    "            # key = token.orth_\n",
    "            if key in vocab_dict:\n",
    "                ids[i] = vocab_dict[key]\n",
    "            else:\n",
    "                ids[i] = vocab_dict[\"[UNK]\"]\n",
    "        return ids\n",
    "    \n",
    "    def _sentence2formated(self, sen):\n",
    "        return sum( fill_SYMBOL_ONE( sentence2normalize_noun(sen) ), [] )\n",
    "    \n",
    "    def _padding_vector(self, Xseq):\n",
    "        Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "        Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "        Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "        return Xseq\n",
    "\n",
    "\n",
    "    def _make_X(self, utt_list:list, vocab_dict:dict):\n",
    "        utt_id_list = []\n",
    "        for utt in tqdm( utt_list) :\n",
    "            utt_id = self._sentence2ids(utt, vocab_dict)\n",
    "            utt_id_list.append(utt_id)\n",
    "\n",
    "        utt_id_pad = self._padding_vector(utt_id_list)\n",
    "        upl = len(utt_id_pad[0])\n",
    "        # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "        # print(usr_pad_len, sys_pad_len)\n",
    "        X = torch.zeros( (len(utt_list), upl) )\n",
    "        for i, u in enumerate(utt_id_pad):\n",
    "            X[i, :upl] = u\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response2/forward_v3.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response2/\"\n",
    "model_name = \"forward_v3.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "fmodel = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_xy_name = \"../X_y_data/context_topic/X_forward_topic_ERROR_N={0}\".format(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5396/5396 [01:57<00:00, 45.77it/s]\n"
     ]
    }
   ],
   "source": [
    "X_forward_all_str = sum(X_str, [])\n",
    "\n",
    "if os.path.exists(forward_xy_name+\".npy\"):\n",
    "    # X_forward_ids  = np.load(forward_xy_name+\".npy\")\n",
    "    X_forward  = np.load(forward_xy_name+\".npy\")\n",
    "    print(\"success load {0}.npy\".format(forward_xy_name))\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        fmodel.cpu()\n",
    "        # X_forward_l =  fmodel.text2context(X_str)\n",
    "        # 手で書くしかない\n",
    "        x_length = len(X_forward_all_str)//N\n",
    "        X_forward_ids = fmodel._make_X(X_forward_all_str, fmodel.vocab_dict).to(torch.int)\n",
    "        X_forward_ids = X_forward_ids.reshape(x_length, N, -1)\n",
    "        X_forward = np.array( [fmodel.last_context(Xfi).numpy() for Xfi in X_forward_ids] ) \n",
    "        fmodel.cuda()\n",
    "        np.save(forward_xy_name, X_forward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(forward_xy_name, X_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[182] 2022-01-23 02:31:37,273 Info sentence_transformers.SentenceTransformer :Load pretrained SentenceTransformer: ../../corpus/pretrained/sbert_context_form2\n",
      "[182] 2022-01-23 02:31:39,970 Info sentence_transformers.SentenceTransformer :Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from sentence_transformers import models\n",
    "\n",
    "# bert_path = \"../../corpus/pretrained/sbert_unclear1\"\n",
    "bert_path = \"../../corpus/pretrained/sbert_context_form2\"\n",
    "sbert = SentenceTransformer(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929b2c33b5c143f6b288b01cf5eb1444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_length = len(X_forward_all_str)//N\n",
    "# X_topic_vec = smodel.encode(X_forward_all_str).reshape(x_length, N, -1)\n",
    "X_topic_vec = sbert.encode(X_forward_all_str).reshape(x_length, N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2feature(vector):\n",
    "    diff = np.abs( vector[0] - vector[1] )\n",
    "    return np.concatenate([vector.flatten(), diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 768\n",
    "def sentence2formated(vectors):\n",
    "    features = []\n",
    "    prev_vector = np.zeros(emb_dim)\n",
    "    for i, vector in enumerate(vectors):\n",
    "        feature = vec2feature( np.array([prev_vector, vector]) ) \n",
    "        features.append(feature)\n",
    "        prev_vector = vector\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topic = np.array([ sentence2formated(vec) for vec in X_topic_vec ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_topic, X_forward], axis=2)\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, topic_dim, forward_dim, topic_hid, for_hid, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.tlen = topic_dim\n",
    "        self.flen = forward_dim\n",
    "        # self.hidden = hidden_dim\n",
    "        # 768->256\n",
    "        self.tlstm = nn.LSTM(topic_dim, topic_hid, batch_first=True)\n",
    "        # self.lay2_lstm = nn.LSTM(hidden_dim+forward_dim//2, hidden_dim2, batch_first=True)\n",
    "        self.flstm = nn.LSTM(forward_dim, for_hid, batch_first=True)\n",
    "        # self.for2hid = nn.Linear(forward_dim , forward_dim//2)\n",
    "        self.hid2out = nn.Linear(topic_hid+for_hid , tagset_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_topic = x[:, :, :self.tlen].to(torch.float)\n",
    "        x_forward = x[:, :, self.tlen:].to(torch.float)\n",
    "        # x_for_hid = self.for2hid(x_forward)\n",
    "        # print(x_topic.shape)\n",
    "\n",
    "        # forward_c = torch.stack( [ self.fmodel.last_context(xfid) for xfid in x_forward_id])\n",
    "        # topic_out, _ = self.tlstm(x_topic)\n",
    "        _, tout = self.tlstm(x_topic)\n",
    "        _, fout = self.flstm(x_forward)\n",
    "\n",
    "        # print(\"topic_out: \", topic_out.shape)\n",
    "        # topic_out = self.relu(topic_out)\n",
    "        # x_lay2 = torch.cat([topic_out, x_for_hid)], dim=2)\n",
    "\n",
    "        # _, hc = self.lay2_lstm(x_lay2)\n",
    "        # out = self.hid2out(hc[0][0])\n",
    "        out = self.hid2out(torch.cat([tout[0][0], fout[0][0]], dim=1) )\n",
    "        y = self.softmax(out)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "epoch_ = 150\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_DIM = emb_dim*3\n",
    "FORWARD_DIM = 256\n",
    "TOPIC_HID_DIM = emb_dim\n",
    "FOR_HID_DIM = FORWARD_DIM//2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicClassifier(TOPIC_DIM, FORWARD_DIM, TOPIC_HID_DIM, FOR_HID_DIM, OUTPUT_DIM)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 13%|█▎        | 20/150 [00:11<01:12,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 \t loss 0.006927083479240537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 40/150 [00:23<01:06,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 \t loss 0.0014610214766435092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 60/150 [00:35<00:50,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 \t loss 0.0005248263405519538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 80/150 [00:46<00:39,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 \t loss 0.00021885237401875202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 100/150 [00:57<00:27,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 \t loss 0.00011425065849834937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 120/150 [01:08<00:16,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 120 \t loss 7.127596404643555e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 140/150 [01:19<00:05,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 140 \t loss 4.5624425524692924e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:24<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in tqdm( range(epoch_)  ):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        X_t_tensor = data[0].cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\n",
    "\n",
    "        score_ = model(X_t_tensor)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score_,  y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score_\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/context_topic/sbert_context_form_LSTM_N=4.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM = DataManager(\"../models/context_topic/\")\n",
    "model_name = \"sbert_context_form_LSTM_N={}.pickle\".format(N)\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[150  40]\n",
      " [ 43 172]]\n",
      "accuracy =  0.7950617283950617\n",
      "precision =  0.8113207547169812\n",
      "recall =  0.8\n",
      "f1 score =  0.8056206088992974\n"
     ]
    }
   ],
   "source": [
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- epoch 150 \n",
    "\n",
    "        confusion matrix = \n",
    "        [[146  44]\n",
    "        [ 57 158]]\n",
    "        accuracy =  0.7506172839506173\n",
    "        precision =  0.7821782178217822\n",
    "        recall =  0.7348837209302326\n",
    "        f1 score =  0.7577937649880097\n",
    "\n",
    "- epoch 250\n",
    "\n",
    "        confusion matrix = \n",
    "        [[140  50]\n",
    "        [ 53 162]]\n",
    "        accuracy =  0.745679012345679\n",
    "        precision =  0.7641509433962265\n",
    "        recall =  0.7534883720930232\n",
    "        f1 score =  0.7587822014051522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs_ = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "X_str, y = make_Xy_4test(convs_, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/context_topic/sbert_context_form_LSTM_N=4.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM = DataManager(\"../models/context_topic/\")\n",
    "# model_name = \"sbert_context_form_LSTM_2.pickle\"\n",
    "model_name = \"sbert_context_form_LSTM_N={}.pickle\".format(N)\n",
    "model = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_xy_eval_name = \"../X_y_data/context_topic/X_forward_topic_ERROR_eval_N={0}\".format(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5544/5544 [02:00<00:00, 46.20it/s]\n"
     ]
    }
   ],
   "source": [
    "X_forward_all_str = sum(X_str, [])\n",
    "\n",
    "if os.path.exists(forward_xy_eval_name+\".npy\"):\n",
    "    # X_forward_ids  = np.load(forward_xy_name+\".npy\")\n",
    "    X_forward  = np.load(forward_xy_eval_name+\".npy\")\n",
    "    print(\"success load {0}.npy\".format(forward_xy_eval_name))\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        fmodel.cpu()\n",
    "        # X_forward_l =  fmodel.text2context(X_str)\n",
    "        # 手で書くしかない\n",
    "        x_length = len(X_forward_all_str)//N\n",
    "        X_forward_ids = fmodel._make_X(X_forward_all_str, fmodel.vocab_dict).to(torch.int)\n",
    "        X_forward_ids = X_forward_ids.reshape(x_length, N, -1)\n",
    "        X_forward = np.array( [fmodel.last_context(Xfi).numpy() for Xfi in X_forward_ids] ) \n",
    "        # X_forward = X_forward.reshape(-1, 4, 256)\n",
    "        fmodel.cuda()\n",
    "        # X_forward_l = np.array( fmodel.text2context(X_forward_all_str).cpu() ) \n",
    "        # np.save(forward_xy_name, X_forward_ids)\n",
    "        np.save(forward_xy_eval_name, X_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1386"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db789e44b604a828165a04e5db7f710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_length = len(X_forward_all_str)//N\n",
    "# X_topic_vec = smodel.encode(X_forward_all_str).reshape(x_length, N, -1)\n",
    "X_topic_vec = sbert.encode(X_forward_all_str).reshape(x_length, N, -1)\n",
    "X_topic = np.array([ sentence2formated(vec) for vec in X_topic_vec ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_topic, X_forward], axis=2)\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X, device='cuda:0').float()\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[472 210]\n",
      " [175 529]]\n",
      "accuracy =  0.7222222222222222\n",
      "precision =  0.7158322056833559\n",
      "recall =  0.7514204545454546\n",
      "f1 score =  0.7331947331947332\n"
     ]
    }
   ],
   "source": [
    "score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/y_pred/context_form.pickle\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../X_y_data/y_pred/\"\n",
    "data_name = \"context_form.pickle\"\n",
    "dataM = DataManager(data_path)\n",
    "dataM.save_data(data_name, [y, y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2つのLSTM の結果を統合(話題遷移エラー，発話意図不明確，情報不足すべて)\n",
    "\n",
    "        confusion matrix = \n",
    "        [[471 211]\n",
    "        [193 511]]\n",
    "        accuracy =  0.7085137085137085\n",
    "        precision =  0.7077562326869806\n",
    "        recall =  0.7258522727272727\n",
    "        f1 score =  0.7166900420757363\n",
    "\n",
    "        TOPIC_DIM = emb_dim*3\n",
    "        FORWARD_DIM = 256\n",
    "        TOPIC_HID_DIM = emb_dim\n",
    "        FOR_HID_DIM = FORWARD_DIM//2\n",
    "        OUTPUT_DIM = 2\n",
    "            \n",
    "    - baseline よりも精度は高い\n",
    "\n",
    "    - epoch 250\n",
    "    \n",
    "            confusion matrix = \n",
    "            [[433 249]\n",
    "            [171 533]]\n",
    "            accuracy =  0.696969696969697\n",
    "            precision =  0.6815856777493606\n",
    "            recall =  0.7571022727272727\n",
    "            f1 score =  0.7173620457604307\n",
    "\n",
    "- ハイパーパラメータを少なくしてみた\n",
    "\n",
    "\n",
    "- sbert の学習対象を変更してみた\n",
    "\n",
    "        confusion matrix = \n",
    "        [[632  50]\n",
    "        [ 53 651]]\n",
    "        accuracy =  0.9256854256854257\n",
    "        precision =  0.9286733238231099\n",
    "        recall =  0.9247159090909091\n",
    "        f1 score =  0.9266903914590747\n",
    "\n",
    "    - おそらくこれは幻想！\n",
    "    - 夢を見てはいけない！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['握手してもらったりしますか。', '人間関係を深める為の挨拶としてよく用いられるよね。', 'サインはしてもらいますか?', '新しい会社での人間関係が難しくて嫌になっちゃうよ。']\n",
      "['サインはしてもらいますか?', '新しい会社での人間関係が難しくて嫌になっちゃうよ。', '写真は撮りますか?', 'あそうでしたか。所で、九州に行かれたことありますか?']\n",
      "['ですね。好きです', 'えっ', 'あなたも好きですか?', 'スポーツは好きですか']\n",
      "['こんにちは。ありがとう、水分とるように気を付けてます。', '気をとってるんですね。熱中症に気をつけないんですか?', '大丈夫です。お仕事がお休みの日は何してるんですか?', '熱中症に気をつけたいのです']\n",
      "['車で帰りますよ。', '病院は有名ですね', '公園は好きですか?', '行くのがいいですね']\n",
      "['何をですか?', '退屈させてしまいましたか。うまくコミュニケーション取れるように頑張るので、お付き合いください。話題を変えましょう、最近観た映画は何ですか?', '最近は観ていません。', '言葉でコミュニケーションがとれるかもしれない']\n",
      "['許してくれてありがとうございます', '渾名がついてるのです', '渾名で呼んでいいですか?', '渾名をつけたいのです']\n",
      "['一人で見る方が気楽でいいですよね', '映画は音楽がいいですね', '音楽好きなんですか?', '映画は映画館がいいですね']\n",
      "['そうなんですね。日本の世界遺産の数は幾つあるんでしょう?', 'インターネットで調べた所、1位は、0箇所です。', '1位はどこの国ですか?', '1位ヶ岳の所在地は山口県長門市・下関市です。']\n",
      "['海は気持ちが良いですからね。', '海は楽しいですね', '海に行って何をしたら楽しいですか?', '海は大好きですね']\n",
      "['潮風に吹かれていると気持ちが良いですね。', '潮風が心地良いですね', '海に行く時は、帽子を忘れないでください。', '潮風は肌に感じます']\n",
      "['仲間だ', 'ナカーマ', '仲間由紀恵', '仲間由紀恵は怖いですね']\n",
      "['今のインスタント食品はいろいろ味がよくできていますね。', 'カップラーメンでもインスタントカレーでも、立派な料理だよね。', 'やっぱり自分で作った方が美味しい物ができますね。', '料理は美味しいですね']\n",
      "['そうはおっしゃらず笑い', '微熱が良いですね', '微熱?', '微熱は多いです']\n",
      "['今からですか?', 'うん', '今日は予定があるので行けません。', '海に行ってないなあ']\n",
      "['私も今年はまだ行ってないですね。', '海まで散歩に行くのです', '海が大好きなんですね。', '海は大好きですか。気持ちがいいですね']\n",
      "['海が大好きなんですね。', '海は大好きですか。気持ちがいいですね', '私も好きですよ。', 'ありがとう']\n",
      "['初めて聞いたよ!物知りなんだね。', '人類が初めて宇宙でプレーしたスポーツは、月面で行われたゴルフなんだよ。', '本当?月でゴルフってなんか不思議。', 'パーより0打少ないアルバトロスはホールインワンよりも出る確率が少ないんだって。']\n",
      "['私は資格はあまり持ってません', '時期から資格を取りますねぇ', '何か資格を取るんですか?', '資格でもとります']\n",
      "['おはおは', 'ようよう', '好きな漫画は?', '最近じゃあ漫画もスマホで読めるから、すごいよね。']\n",
      "['電池がすぐなくなっちゃうよね。', '電池は必要なんですよねー', '好きな映画は何?', '電池はエネループがいいですね']\n",
      "['それな', 'ねー', 'いつまでこの話するの?', '干支は辰年です。']\n",
      "['あちこちに自動販売機もありますしね。', '自動販売機はいいですね', '自動販売機は便利です。', '自動販売機はほしいですね']\n",
      "['ギョーザです。あなたの得意料理はなんですか?', '料理は不得手です', '所で好きな映画はなんですか?', '料理は下手です']\n",
      "['何が食べたいですか?', '御飯を食べに行きますよねえ', 'じゃあどんな食べ物が好きですか?', '炊き立てがおいしいですね']\n",
      "['どうんなゲームが好きですか?', 'どうだと思いますか?', 'パズルゲームが好きそうですが違いますか?', '寝られなくなるから夜はやめた方がいいよ。']\n",
      "['家で食べましたよ', '体調の方はいかがでしょう', 'あまり良くないですね', '好きな人はいますか?']\n",
      "['縁側いいですね。私は雲丹が好きです。', '縁側は美味しいですね', 'どこの寿司屋さんがお気に入りですか?', '私も大好きです!']\n",
      "['野球が好きだから、野球ゲームをやっているよ。', '野球は好きですか。料理は素晴らしいですね', '君の好きなスポーツは何?', '料理は素晴らしいですね']\n",
      "['君の好きなスポーツは何?', '料理は素晴らしいですね', 'そうだね。今日はオリンピック観戦するよ。', '料理は大切ですね']\n",
      "['よう!', 'おはよー', 'おはようございます', 'メリークリスマス']\n",
      "['泳ぐのも好きですか?', '泳ぐを覚えるのです', '暑い時、どうしてますか?', '泳ぐを繰り返すのです']\n",
      "['そうなんですね、山は好きですか?', '泳ぐを覚えたりとかします', '泳ぎ覚えるの大変ですか?', '泳ぐを覚えないんです']\n",
      "['力が付きますよね!暑いのは好きですか?', '尻尾が美味しいですね', 'お魚のことですか?お寿司は好きですか?', 'ねたが美味しいですね']\n",
      "['そうだな', 'はい', '楽しみって何が?', '楽しみが続くっていいですよね']\n",
      "['熱中症になったら大変だよ', '予防が大切ですね', '他にも予防してることはありますか?', '手洗い嗽で予防が大切です']\n",
      "['ちゃんと手洗い嗽するようにします', '予防は大切ですね', '他にもありますか?', '受けるのが効果的ですね']\n",
      "['今日も暑いですからね', '予防が大切ですね', '何か熱中症の対策をしていますか?', '日頃から予防を心掛けるかもしれない']\n",
      "['何を売るのですか?', 'IDを使います', 'そういえば、ドコモのサービスでIDという物がありますね。', 'ドコモでiPhone0が使えます']\n",
      "['やっはろー', 'おはようございます', '性別は何?', '逆子で性別がわかってないかもです']\n",
      "['どうやったら恋人できる?', '恋人は重いですね', '勉強は好き?', '「恋人たちの予感」って、あれ、本当におもしろい映画だよね。']\n",
      "['恋しい天気とはどんな天気なんでしょう。', '天気は怪しそうですよ', '怪しいんですか。雨が降りそうなのですか?', '出かけない日に限って、晴天になるよね。']\n",
      "['ご婚約ですか?', '私のこと好きなんですか', 'なぜそう思うのですか?', 'そういえば、あなたは美味しい物を食べるのが好きでしたよね?メロンにトマトを見せます']\n",
      "['何か良いアプリありますか?', 'スマホ用の手袋って、大ヒットしたんだよね。', 'そうなんですね。もし良い物があったら教えてください。', '「物」をウィキペディアで検索した結果は、「物とは、広義には対象を特定化せず一般的・包括的にいう語であり、人間が知覚し思考し得る対象の一切である。」だよ']\n",
      "['そうですね。wikipeiaはよく使いますか?', 'アクセスを増やせます', 'アクセスを増やすには良い方法がありますか?', 'アクセスのステーを聴くと感動します']\n",
      "['知りませんでした。どんな風に良い曲なんですか?', 'アクセスが都心に良いらしいです', '東京駅までどれくらいかかりますか?', '東京駅には、JR東日本とJR東海の0人の駅長がいるんだって。']\n",
      "['確かに特殊な環境ですよね。私はあまり学校が好きではありませんでした。', 'レベルが高いですね', 'あなたは学校が好きでしたか?', '学校は安全ですね']\n",
      "['有名な御店なんですか?', '先程も言った通り、名護にあるファミリー マートという名護そばの御店は沖縄という場所にあり、私の住んでいる地域からは遠いのです。', 'そうなんですね。私も沖縄に行く機会があれば沖縄そばを食べたいです。', '沖縄で有名なゴーヤー等健康的な食材を使って調理していても油で炒めてしまうので、食材が油でコーティングされて、脂肪やカロリーが増加してしまいますね。']\n",
      "['ダイエットなどでカロリーを気にされた生活をしているんですか?', '沖縄の名物の味噌煮込み飴は、健康に良くてパッケージも可愛らしいのですが、味に関しては好みがとても分かれると思います。', '味噌煮込み飴という物は初めて聞いたのですが、味があまり想像できませんね。', '沖縄へ旅行に行った時に食べた海葡萄は、沖縄の居酒屋で食べました。']\n",
      "['そうですね。テレビか何かで見た記憶があります。', '私の住んでいる沖縄では、ブラックモンブランがテレビ放送されていません。', 'それは残念ですね。', '沖縄の食材である海葡萄という名前から、フルーツのような甘い味を想像していたのですね。']\n",
      "['', 'こんにちは!大阪とマドリードなら、どちらに関心がありますか?', '大阪です', '私の住んでいる大阪では全日本空輸の話題は朝の挨拶のような物です。']\n",
      "['海遊館は行ったことはないですね', '大阪の世界最大級の水族館である海遊館の鮫はどのような餌を食べていたのですか?。', '鮫についての知識は全然ないです', '0年の0月に計画している大阪旅行では、大阪天保山に行く予定があるので、大阪天保山の海遊館にも行くとなると、日程的に難しいかもしれませんね。']\n",
      "['向井理さんです。彼の出ているどんなドラマが好きですか?', '向井理さんのような格好が良くて人気のある俳優さんが、モテない役だとか悪役などをすると、どのような演技をされるのかと興味をそそられるので、そういう意味でもゲゲゲの女房という映画は気になっていました。', 'ゲゲゲの女房は朝ドラですよ。ですが、悪役がハマる俳優さんといえば誰でしょう', '俳優の向井理さんが出演しているNHKの連続テレビ小説ゲゲゲの女房を見ていません。']\n",
      "['心配ですね。好きなハンバーガーショップはどこですか?', '抹茶味のshall ウィー ダンス?が海外旅行者に人気があるのは日本限定で販売されているからだそうです。', '限定商品には魅力があります。何時間も並んで購入したりしますか?', 'ブラッドオレンジは日本の蜜柑に比べて味が濃いし、甘いのでミスタークロワッサンドーナツの情報を見ただけでも美味しそうだと思いますね。']\n",
      "['そうなんですか?詳しいんですね。テレビをよく御覧になるんですか?', '私の街にNHKの町歩きテレビ番組忍たま乱太郎のロケが行われるとしたら、私の街に昔から存在する歓楽街があるので、そこでタモリさんにロケをしていただきたいと思います。', '忍たま乱太郎は町歩き番組なのですか?アニメではないんですね。', '以前、テレビでタモリさんが、「お風呂に入る時には石鹸などは全く使わない」というお話をされていて、驚きました。']\n",
      "['ゲゲゲの女房お好きなんですか', 'NHK連続テレビ小説ゲゲゲの女房が、糠床女子のブームの火付け役となったそうですよ。', '御馳走さんではなくてですか', 'お節料理ははハードルが高いと思っていたので、料理研究家の栗原はるみさんがご提案されていたお節料理を見て、これでいいのだと何かとても安心いたしました。']\n",
      "['御免なさいちょっとわかりません', 'NHKの町歩きテレビ番組魂レボリューションは、古地図に添って、現在の町を歩いてみるなどの企画が面白いと思いました。', 'そんな番組があるのですね', 'タレントの永作博美さんもNHKのテレビ番組などで主婦層にとても人気があるようですね。']\n",
      "['', 'こんにちは!東京ディズニーシーと四季劇場なら、どちらに行ってみたいですか?', '東京ディズニーシーですね', '東京ディズニーランドや東京ディズニーシーには、たまに遊びに行く程度ですが、天下一品は好きです。']\n",
      "['アナウンサーニ転職してたのか', '日本テレビの林家木久蔵さんが持つレギュラー番組といえば、関ジャニ∞のザ!鉄腕!ダッシュ!!や、トキオの月曜から夜更かしなど、トーク番組が多い気がします。', '落語家を捨てたのか', '関西国際空港以北にはよく旅行に行くので、今度関東を訪れた際には、牛タンの刺し身を食べてみたいと思います。']\n",
      "['どんなドラマ?', 'NHK連続テレビ小説あまちゃんが、糠床女子のブームの火付け役となったそうですよ。', 'へぇそうなんだ', 'だから荒野というドラマのこれまでの回がNHKで再放送されたら、ぜひ見てみたいです。']\n",
      "['ハリー・ポッターエリアです', '奈良とイギリスなら、どちらに興味がありますか?', '奈良ですよ', '羽生結弦は奈良の歴史的建造物の建築に、どのような貢献をしましたか?。']\n",
      "['日ハムファンなので盛り上がって欲しい。', '0月0日ではサッポロビール主催の全国丼グランプリが開催され、シドニー全国の丼から、0の「金賞丼」が発表されました。', '金賞丼は、何だったの?', 'シドニーだと、どこまでも真っ直ぐに続く道路で車を走らせると、スピードが知らぬ間にものすごく出てしまうと聞いたことがあります。']\n",
      "['', 'こんにちは!有名人でゴッホとハビエル・アギーレなら、どちらに興味がありますか?', 'ゴッホですね。', 'オランダではコロッケが自動販売機で売られていて、わたしは自動販売機でコロッケを買いました。']\n",
      "['そうなんですね、クリームシチューが0位とは意外ですね。', 'スマップの人気グループakb0の木村拓哉さんは、出演しているジャニーズ系スポーツテレビ番組僕シリーズを、0月0日放送分で卒業することになったそうです。', 'そうなんですか、残念ですね。', '昨年の忘年会では、ジャニーズのコスプレをして、恋するフォーチュンクッキーを踊りました。']\n",
      "['あなたのお名前教えていただますか?', '有名人で鈴木明子とローリーなら、どちらに興味がありますか?', '鈴木明子です。あなたは?', 'セブンイレブンのマルチコピー機の性能が良いので、自宅のプリンターを使用する機会がなくなりました。']\n",
      "['鈴木明子です。あなたは?', 'セブンイレブンのマルチコピー機の性能が良いので、自宅のプリンターを使用する機会がなくなりました。', '便利になりましたよね。コンビニにはよく行くのですか?', '先日セブンイレブンで、契りパンというスイーツを購入し、食べました。']\n",
      "['便利になりましたよね。コンビニにはよく行くのですか?', '先日セブンイレブンで、契りパンというスイーツを購入し、食べました。', '美味しかったですか?', 'セブンイレブンで売られている安納薯の御薩スナックは美味しいですね。私もチョコチップクリームの安納薯の御薩スナックをよく買います。']\n",
      "['安納薯の御薩スナックは私も大好きです。他に好きな御菓子はありますか?', 'セブンイレブンで売られているキリン・フレビア・レモン&ホップと、かっぱ蝦煎という商品が好きです。', 'かっぱ蝦煎って懐かしいなぁ。子供の頃よく食べました。セブンイレブン以外のコンビニには行かないのですか?', 'あなたの好きなセブンイレブンのピザポテトのちょい足しレシピはないようですが、nanacoやポテトチップス、グランドカルビーのちょい足しレシピは話題のようです。']\n",
      "['nanacoでポイント貯めてますか?', '今後、大量に印刷物を作成する機会ができたら、セブンイレブンのマルチコピー機を利用したいと思います。', 'どんな物をプリントアウトするのですか?', 'コンビニエンスストアで売っている商品の中では、私はセブンイレブンのチョコケーキマウンテンが好きで、よく買います。']\n",
      "['海豹ってかわいいですよね', '旭山動物園のオーレでは、他の動物園では見られないようなかばの姿が見られそうですね。', '旭山動物園はどこにあるんですか?', '海豹が好きなので、旭山動物園のチョコレートウエハースがとても気になります。']\n",
      "['鳥だとなにが好きですか?', '旭山動物園のグランドカルビーの海豹のダイナミックな泳ぎはとても迫力があります。', '旭山動物園へ行ったことがあるんですか?', '漁師の力めしネギトロロ飯ができてからは旭山動物園に行っていないので、今度ぜひ旭山動物園に行ってみたいです。']\n",
      "['今度一緒に行きませんか?', '私が旭川市の旭山動物園の中で気に入っている場所は、焼き鳥弁当です。', '美味しそうですね', '動物園はどこの動物園も大体同じように動物の生態を見せているのだと思っていましたが、旭山動物園には特別なこだわりがあったのですね。']\n",
      "['はじめて聞いたスーパーマーケットです。地域によって様々なスーパーマーケットがあるんですね。', 'レッドソックスは鶴岡市立加茂水族館では一番有名なスーパーマーケットだと思います。鶴岡市立加茂水族館を中心に宮城県、山形県、茨城県、栃木県の0県に広く店舗展開しているようです。', '私が住んでいる広島県ではショウジが一番有名なスーパーマーケットです。安くて便利なスーパーマーケットです。', '広島県では、お好み焼きを食べる際に、こてという料理器具を使って食べるのですが、いかフライは小さくちぎってある方が食べやすいのです。']\n",
      "['私が住んでいる広島県ではショウジが一番有名なスーパーマーケットです。安くて便利なスーパーマーケットです。', '広島県では、お好み焼きを食べる際に、こてという料理器具を使って食べるのですが、いかフライは小さくちぎってある方が食べやすいのです。', '私はこてを使うのが下手なので、いつも箸を使ってお好み焼きを食べています。', '広島県内でツキノワグマの捕獲件数や目撃情報が増えているそうで、ツキノワグマ対策協議会の総会が開かれたそうです。']\n",
      "['広島の牡蠣は美味しいですね。牡蠣を焼いてポン酢に浸けて食べるのが御勧めです。', '以前にインターネットの通販サイトの広島鉄ぱんグランプリで送料無料キャンペーンの時に広島の牡蠣も購入したことがあります。', '私もインターネットで牡蠣を購入してみようと思います。', 'やはり新井貴浩選手は、自身の出身地広島でプロ野球人生を終えたいのでしょうかね。']\n",
      "['早く東京オリンピック見たいよね', '世界大会と言えば、0年に東京で鶴の子大粒納豆が開催されることが決定しましたね。', '0年には大阪万博が開催されるし楽しみだね', '日本中央競馬会の話題は大阪では朝の挨拶のように、日本中央競馬会は地元に浸透しているのですね。']\n",
      "['こんにちは', 'こんにちは', '趣味は何ですか?', 'JR東日本と東京ディズニーシーなら、どちらに行ってみたいですか?']\n",
      "['そうですね。乗ってみたいです。', 'トイ・ストーリー・マニア!は東京ディズニーシーにあるアトラクションです。', '東京ディズニーシーには行ったことがなくて、どんなアトラクションがあるか分からないです。', 'トイ・ストーリー・マニア!を利用したので、春休みでとても混雑していましたが、東京ディズニーシーや東京を満喫することができました。']\n",
      "['おお!それもいけそう。カボチャグラタンだから秋冬限定ってこと?', '阪急うめだ本店にある一般的な山芳製菓株式会社のグランドカルビーの約0倍の厚さである、山芳製菓株式会社のニュー焦がしミルク味もそうです。', 'なるほどね。期間限定とか季節限定って言われると、ついつい買っちゃうよね!', '次回、阪急うめだ本店へ行く機会があったら、阪急うめだ本店に立ち寄ってハローキティーで御土産を選ぼうと思います。']\n",
      "['', '何かお話しましょうか?', '今日は天気いいのかな?', '静岡とマドリードなら、どちらに関心がありますか?']\n",
      "['ペンギン可愛いですね!私も大好きですよ', '旭山動物園で行われるというペンギンの散歩というのは想像しただけでも可愛らしいですね。', 'ペタペタ歩く姿が微笑ましいですね。', '花林糖チョコレートができてからは旭山動物園に行っていないので、今度ぜひ旭山動物園に行ってみたいです。']\n",
      "['ペタペタ歩く姿が微笑ましいですね。', '花林糖チョコレートができてからは旭山動物園に行っていないので、今度ぜひ旭山動物園に行ってみたいです。', '動物が大好きなんですね。', '旭山動物園では、去年できたかば館も人気のようです。']\n",
      "['かばの欠伸とか生で見てみたい物です。', '海豹を間近に見ることができるというのは、旭山動物園の妖怪ウォッチは魅力的ですね。', 'やはりテレビで見るのとは一味違うのでしょうね。', '旭川市の旭山動物園には、0回しか行ったことがありません。']\n",
      "['知らないです。どんなCMですか?', 'メロンパンの皮焼いちゃいましたというパンはスーパーマーケットではPOPなどで宣伝されていましたか?。', '私はスーパーマーケットでは見かけたことはないんです。', 'メロンパンの皮焼いちゃいましたというパンを食べてみた感想は、普段目にしているメロンパンの皮より重量感があって、皮というよりも、しっとりとしたクッキーのような感じでした。']\n",
      "['テレビ見ないからわからないけど、それってすごいことなんだよね?', 'その他、グラッチェガーデンでは、数十種類のパスタやグラタンなども0種類だけメニューから選び、食べることができます。ドリンクバーやサラダもついて、0円です。いつか岩手にも建てば、ご家族で楽しめるかと思います。', '建てるならもっと集客ができる所がよいのではないかと?', '岩手在住の主婦です。現在は子育てをしながらみけらんを時々やっています。']\n",
      "['そうですね', '他に、私が住んでいる町では養蚕で栄えていた時代に御嶽山では初めてのフジテレビの出張所が設立されて、素晴らしい建物だったのですが、それも保存されずに取り壊されてしましました。', '悲しいですね', '今年は我が家でちょっと残念な出来事があったので、面白おかしくねたにしてフジテレビの番組であるとんねるずの皆さんの御陰でしたに応募してみようかなと思います。']\n",
      "['面白そうですね', 'フジテレビの番組であるとんねるずの皆さんの御陰でしたの博士と助手細かすぎて伝わらない物真似選手権のコーナーで、私が印象に残っているのは芸人高島彩さんの箱根駅伝予選会にて、本選出場ラインスレスレを発表する関東学連のスタッフという物ですが、確かに面白いのですがなかなか他の番組では披露する機会はなさそうなねたでした。', 'それは見てみたいですね', '最近のテレビ番組ではなかなか漫才を見る機会がないので、今年フジテレビ系列で放送される漫才番組のザ manzaiを楽しみにしています。']\n",
      "['薬師丸ひろ子さんは女優さんですよね', '薬師丸ひろ子さんと小泉今日子さんの心の葛藤が見物でしたね。私もブラタモリ見ましたよ。', 'ブラタモリは歴史好きな人に人気がありますね自分も好きです', 'あなたの住む街でNHKの町歩きテレビ番組ブラタモリの地方ロケが行われるとしたら、松たか子さんに歩いて欲しい場所はありますか?。']\n",
      "['松坂大輔さんの話をしましょう。', '日本野球に復帰したら、松坂大輔投手のピッチングを見に、多くのファンが球場まで駆け付けてくれるでしょうね。', '活躍してくれるといいですね。', '野球選手である田中将大選手のプレーが日本でまた見られるのは、嬉しいですね。']\n",
      "['日本人が活躍することはうれしいことです。', '田中将大投手が昨シーズンのディズニー・アニメーション・スタジオ日本一に一番貢献した選手だということは誰もが認めていると思います。', 'イチロウも活躍しましたね。', '田中将大投手のピッチングの安心感はたしかに他の若い野球選手とは違うと思います。']\n",
      "['イチロウも活躍しましたね。', '田中将大投手のピッチングの安心感はたしかに他の若い野球選手とは違うと思います。', 'どんどん才能ある選手が出てきてくれるといいですね。', '田中将大投手の肘の怪我というのは投手にとって致命傷ですね。']\n",
      "['日本の野球界を引っ張っていってほしいです。', '最近の横綱は海外力士ばかりなので、日本人で立派な横綱が出てくるといいですね。', 'モンゴル人が多いですね。', '日本にいて見られるかどうかがポイントなのですね。']\n",
      "['I ウイル オールウェーズ ラブ ユーをカバーしてほしいですね', 'テレビ番組「YouTube」のカラオケ対決コーナーに時々出演している、台湾の宇多田ヒカルさんも燐・ユーチュンの楽曲を歌っていますね。', 'そうなのですか?', 'サカナクションの他に、アジアン kung-FU generationや亀梨和也、スマップも好きです。']\n",
      "['栗原さんは知らないから、長谷部さんの方にしましょうよ', 'サッカー日本代表のホンジュラス戦では、ボランチは栗原はるみ選手と長谷部誠選手のベテランが先発しましたが、あなたはAFCアジアカップではどのようなボランチの組み合わせが望ましいと思いますか?。', '栗原さんはわからないけれど、長谷部さんには活躍してもらい', '栗原はるみ選手や、長谷部誠選手、宮本恒靖選手、山口螢選手などのブログは時々チェックします。']\n",
      "['スイートカボチャと牛蒡の肉巻きは美味しそう。', '0年の0月に料理研究家の宮本恒靖さんが、ガンバ大阪のあさイチで、指導されていたお節料理は、気軽に楽しめるカジュアル御節ということで、「冷めてもおいしい」「日持ちがする」弁当の御数の延長線上でのお節料理ということで、和洋折衷料理の物でした。', 'おいしかったですか?', 'ガンバ大阪のサポーターなので、宇佐美貴史選手が好きです。']\n",
      "['女子サッカーの選手だから無理なんじゃないですか', 'サッカーチームであるガンバ大阪のゴールキーパー永島昭浩選手の御陰で、勝ち点0がとれた試合も幾つかあり、彼のスーパーセーブでガンバ大阪は随分と勝ち点を伸ばせたと思います。', 'よくわからないけれど、そうなんですか', 'サッカーチームガンバ大阪のアンデルソン・パトリック・アグウイアル・オリベイラ選手も、「サッカーは年齢でやる物じゃないとこれからも証明したい」と発言していたようです。']\n",
      "['よくわからないけれど、そうなんですか', 'サッカーチームガンバ大阪のアンデルソン・パトリック・アグウイアル・オリベイラ選手も、「サッカーは年齢でやる物じゃないとこれからも証明したい」と発言していたようです。', 'カズみたく年配になってもやれたらいいですね。でも体力的には難しいですよね。', '実力的にはガンバ大阪が有利だとは思いますが、モンテディオ山形も勢いはあるのでサッカー天皇杯決勝は今からとても楽しみですね。']\n",
      "['', 'こんにちは!有名人で松島みどりと菅野美穂なら、どちらに興味がありますか?', '松島みどりは知りません', 'やはり、松島みどり氏の行為は、買収とみられて仕方のないことだと私も思います。']\n",
      "['松島みどりは政治家ですか?', '政治家は信用が大事なのはわかりますが、今回の松島みどり氏の決断は少し早いかな。', '政治資金の使い方が間違ってますよね', '0年に農林水産省と経済産業省が総額000000000円の補助金を出したことで、植物工場建設が促されましたが、採算ラインは未だ厳しい状態のようです。']\n",
      "['', '何かお話しましょうか?', 'あなたのお名前を教えていただけますか?', '鶴岡市立加茂水族館と国営備北丘陵公園なら、どちらに関心がありますか?']\n",
      "['それは残念ですね。所で、どちらにお住まいなのですか?', 'やくらいフーズのグループ会社であるソントンホールディングスが、山形県の鶴岡市立加茂水族館で水揚げされたかきを使った「FNS歌謡祭」を発売したそうです。', '水族館で牡蠣が水揚げされるの?それ、美味しいのですか?', '鶴岡市立加茂水族館へ行ったことがありますか?。']\n",
      "['世界に一つだけの花はスマップが歌ったと思いますよ。この歌好きですか?', '児玉清さん主演のドラマではアイドルグループスマップの曲が主題歌となることが多いですね。', 'そうなのですか!どんなドラマが好きですか?', 'アイドルグループのスマップの中居正広さんは、どんな時でもアイドルを貫いている所もすごいですよね。']\n",
      "['PVはどれもいいよね', '宇多田ヒカルさんが歌う曲のプロモーションビデオの中で一番印象深く好きなのが彼氏になって優しくなってです。', '見たこと無い、どうゆう奴なの?', '宇多田ヒカルさんの宇多田ヒカルの歌-0組の音楽家による0の解釈について-のプロモーションビデオは、自分の部屋でセルフ撮影しているような設定なのが特徴ですよね。']\n",
      "['香川真司は絵画ですか?', '香川真司選手が初めて海外のサッカーチームに移籍して、早々に大活躍してカミングアウトバラエティー秘密の県民SHOWにも選ばれた事にとても驚きました。', '秘密の県民SHOWという賞でしょうか?', 'オランダも世界選手権を連覇する実力があるので、次回は誰の文句も出ないような差をつけて沖縄に優勝してもらいたいですね。']\n",
      "['沖縄に優勝するってどういうこと?', '沖縄のオランダは、赤の漆塗りが施され、セブンネットショッピングの塗装がつやつやとしているのが特徴で、0年に都市景観0選を受賞をしたそうです。', 'すいません意味がわからないです。', '0月のオランダに遊びに行ったことがありますが、上着がいらないほどの暖かさでした。']\n",
      "['すいません意味がわからないです。', '0月のオランダに遊びに行ったことがありますが、上着がいらないほどの暖かさでした。', 'オランダの0月って暖かいんですね。', 'エドワール・マネなどの有名な画家の絵画が、オランダで展示されると、人が押し寄せてとても混雑しますが、沖縄での美術館は混雑具合などはどうですか?。']\n",
      "['コーヒーと紅茶どっちがすき?', 'すき家のような会社が増えれば、日本の景気も明るくなるような気がしますが、あなたは最近の日本の景気についてどう思いますか?。', 'あまり上向いていない気がします。どうしてすき家がいいと思うの?', 'ココロココを見ると、今年の日本の状況が分かりますよね。']\n",
      "['ちょっとのぞいてみた。日本のいろいろな地域の情報がありそうだね。', '日本は、おせち料理のように地方により文化や風習が異なるので、とても面白い国ですよね。', '内はお雑煮がお澄ましではなく、お味噌ですよ。おせち料理で何が好き?', '日本ではまだエボラ出血熱の感染者は出ていませんが、時間の問題だとも言われていますね。']\n",
      "['内はお雑煮がお澄ましではなく、お味噌ですよ。おせち料理で何が好き?', '日本ではまだエボラ出血熱の感染者は出ていませんが、時間の問題だとも言われていますね。', 'エボラ、怖いです。本当に日本の対策で大丈夫か不安になります。', '日本の病院では当たり前のように目にする保育器も、0台約00000円ほどもするので、発展途上国の病院や助産院が買うには難しい物だったのですね。']\n",
      "['エボラ、怖いです。本当に日本の対策で大丈夫か不安になります。', '日本の病院では当たり前のように目にする保育器も、0台約00000円ほどもするので、発展途上国の病院や助産院が買うには難しい物だったのですね。', '日本は医療が発達していて恵まれていますよね。赤ちゃんを産むなら総合病院、産婦人科、助産院どれを選ぶ?', '同じ日本でも、だいぶ気温差がありますよね。']\n",
      "['日本は医療が発達していて恵まれていますよね。赤ちゃんを産むなら総合病院、産婦人科、助産院どれを選ぶ?', '同じ日本でも、だいぶ気温差がありますよね。', '北海道の方が暑い日もありますよね。どうなっているのでしょうね。', '高校の修学旅行の時、北海道に0週間滞在しました。']\n",
      "['私は奈良に', '中島啓江は奈良の歴史的建造物の建築に、どのような貢献をしましたか?。', '初めて聞くお名前ですね。どんなお寺が好き?', '奈良出身なので、どちらかといえば、letaoを応援していました。']\n",
      "['物知りなんですね', '年末年始にヨーロッパ旅行に行く値段で、閑散期のヨーロッパ旅行に0回行けてしまいますね。', 'お得な方を選びたいと思います', '事故で下半身不随になったヨーロッパ人の男性が、ルパン3世の技術からヒントを得て試作品を開発し、ヨーロッパの一部では既に販売されているそうです。']\n",
      "['そんな物があるのですか。美味しそうですね。', '御嶽山を始め、東北地方はまだ完全に復興していませんし、グラッチェガーデンの事故は大きな事故だったのに、原子力発電所再稼働なんて、信じがたいですよね。', '原発の再稼動は信じがたいですね。放射能が怖いです。', '鈴木さんは近年かなり精力的にライブやコンサート活動をしているのですが、御嶽山地方でのライブは行われていないので、ぜひ見てみたいと思っています。']\n",
      "['鈴木さんてどなたですか。', 'グラッチェガーデンの鈴木先生の原作は、漫画家の鈴木杏樹さんが書かれた、テレビ東京から出版されているだから荒野が原作だそうです。', 'そうなんですか。漫画といえばナルトしか知りません。', 'グラッチェガーデンの鈴木先生で、他の学園ドラマと違った点などがあれば教えてください。']\n",
      "['テレビドラマは好きですか', '宮崎駿監督が鈴木章プロデューサーにテレビ東京の今後について直撃した際に、リストラの話になったみたいです。', '好きな宮崎アニメは何ですか', '私が好きなバラエティー番組は、テレビ東京のもやもやさまぁずです。']\n",
      "['神戸です。', '神戸にあるIKEAの鸚哥味のアイスクリームは、以前、鳥好きの間でかなりのブームになったそうですよ。', 'アイスクリームはよく食べますか?', '私の住んでいる所にIKEAはございません。一番近いのは神戸店なのです。']\n",
      "['アイスクリームはよく食べますか?', '私の住んでいる所にIKEAはございません。一番近いのは神戸店なのです。', 'どこに住んでいるんですか?', 'アンパンマンミュージアムは神戸にもあるんですね。']\n",
      "['チャイって香辛料が効いてておいしいですよね!私も大好きですよ', '紅茶飲料の「冷え知らず」さんの生姜チャイはどちらのメーカーの商品か御存知ですか?。', 'いいえ、知りません', 'あなたが、JR東日本と愛媛地方の四国で行っているサイクリングガイドとは、どのような物ですか?。']\n",
      "['いいえ、知りません', 'あなたが、JR東日本と愛媛地方の四国で行っているサイクリングガイドとは、どのような物ですか?。', 'サイクリングやったことないんですよ、自転車が好きなんですか?', '私が提案している町作りは、JR東日本と四国地方の広い地域にまたがっています。']\n",
      "['いろいろな地方の人が集まると面白い話がたくさん聞けそうですね', 'あのようにしまなみ海道のMOSバーガーの工場の衝撃的な映像をニュースで見せられてしまうと、MOSバーガーに行こう。', 'え?MOSバーガーの工場で何があったんですか?', 'MOSバーガーのハンバーガーでモスライスバーガーが好きです。']\n",
      "['林檎美味しいですよね。好きな色は何ですか?', '美味しいですね', 'どんな人が好きですか?', 'そうです、顔にもすぐ出る方なので厄介です。']\n",
      "['どんな人が好きですか?', 'そうです、顔にもすぐ出る方なので厄介です。', 'あなたはどんな性格ですか?', '最近だと旅行にはまりだしたので、旅代で消えてしまいますね。']\n",
      "['スノーボードできるんですか!すごいです。', '天気がよく、暖かい日は、0km位走ることもありますね。', '一人で走るんですか?', 'そうですね!一人でドライブする時は自分の好きな曲を流しながら走っています。']\n",
      "['なんのサークルに入ってるのですか?', '今は入っていないです。今は高校時代の友人や、学祭の実行委員をやっていた時の友人とよく遊ぶことが多いです。', 'ユジンとはどこで遊ぶのですか?', '地球の裏側とかにもですか?']\n",
      "['何を読むの', '最近まったく読んでないですが、浦沢直樹とか手塚治虫関係、闇のパープルアイ描いてる人、花より男子を読んでました。', '漫画は読む物じゃなくて見る物だと思う', '面白い漫画ですよ。吹奏楽部にいた頃を思い出す漫画です。']\n",
      "['何のパートを担当していたの', 'バイオリンです。', '得意な曲はある?', 'そうそう。その曲しかないですね。']\n",
      "['子供が御店でもらってきたのです。', 'セレブですね。ぼく王将育ちなので。。。', 'ギョーザの王将ですか?', '内は、男の子で、0才0箇月。']\n",
      "['写真が飾ってあり、かわいいですね。', 'かわいいというか…どじですね。そういってくれるtefs0さんは、優しいです。', 'そんなことないですよ。これからどこへ行くのですか?', 'いいですね!私はちょっと目が疲れてきました。']\n",
      "['そうなんですね。またコンサートやってくれるのが楽しみですね。', 'そうです。早朝らしいですね。好きな選手とかいますか?', '好きな選手ですか?', '柳沢選手が親戚と同じ名前で親近感があります。']\n",
      "['動画の編集ができるのはすごいですね', 'どんな会話してるんでしょうね。', 'お仕事は何ですか?', 'ええ。気楽に。人と顔を見て話すことってでも大事ですよね。']\n",
      "['クロマニヨンズっていうバンドです。', '昔してました。かなり昔です。', '楽器は何をしていたんですか?', '木管でしたか!知らない世界なので、わからなかったです。']\n",
      "['パチンコ屋さんの話でしたか。よく打つんですか?', 'す…凄いですね。iPodに入れてビジネス英語とありますけれど、中々それだけではこの点数は出せないですよ。', '私は英語は全然喋れないですよ。', 'キリル文字っていうんですかね。Nをyと呼んだりして。']\n",
      "['初耳です。そんな文字があるんですか。', 'さんまとは2度と競演したくないと', 'さんまさんは面白いので好きですよ。', '白夜行も面白いですよ!東野圭吾さんの他の本とは多少傾向が違うようですが。']\n",
      "['いいえ。家の前で家族で花火を少ししました。', '同じですよ。私も先日まで会社員をしていました。', 'では、今は何を?', '今はグッドだけですよ']\n",
      "['最近はあまり聴いてないですね', 'う、私勉強不足でした・・・チェックしておきます。', 'チェックしなくてもいいですよ', 'どの番組が正しいのか分からなくなりますよね。']\n",
      "['今は御腹一杯です。', 'なりましたよ。学食の割においしかったです。', '学食で何を食べたんですか?', 'なにを食べましょうね。']\n",
      "['私は絵をかくことにはまっています。', 'どんな物を見たのですか?オススメの物があれば教えてください。', '何の御勧めですか?', '毎回目的地は設定してませんが']\n",
      "['私もアルバイトをしたいと思っています。', '私は、飲食店で働いています。駅構内にあるファミリーレストランです。', '私はコンビ2で働きたいです。', 'うっそ!お疲れ様でしたー!!!長いこと休み少なめで大変でしたね。']\n",
      "['便利ですよね', 'てことは、PCから入れるんですよね?', '何に入るんですか?', '軽音楽部のジャズセクションです。']\n",
      "['素敵ですね。', 'お気に入りの1枚を撮ると、言葉とか自然に浮かんできますよね。', '何がお気に入りですか?', '定番のクラインキーも好きだけど。']\n",
      "['あなたの趣味はなんですか?', 'うーん趣味とまでいえるかわかりませんが、スポーツと音楽鑑賞ですね。', 'スポーツはなにが好きなんですか?', '保育園の先生なんですね。音楽とかも必須なんですよね?スポーツというか、スノーボードとかですけどね。']\n",
      "['今迄どこに行きましたか?', 'シンガポールです。', 'どんな所でしたか?', '中学高校の頃、雨になると傘さし運転してると補導員みたいな人に注意されて学校に通報されちゃうって言う事があったので、学校指定のかっぱを装備して学校までの道程約0kmを走ってたんですよね…これがもう蒸し暑くて仕様が無かったですね。']\n",
      "['', 'こんにちは。好きな食べ物は何ですか?', 'ギョーザとかが好きです。あなたは?', '一人の方が好きですね。あなたはどうですか?']\n",
      "['私もそうですよ', 'その位は使っていいと思いますけどね、コミュニケーションの1つとして大事だと思います。', '何を使うんですか?', '私は、目覚ましは携帯を利用していますね。']\n",
      "['どうしたんですか', 'やはり、現地で食べる物とはまったく別物ですね。', '新鮮でしたか?', '1年目って言っても、学部時代に0年過ごしてるから、あんまり新鮮味はないかも。']\n",
      "['他にはどんな音楽を聴きますか?', '演歌童謡一部民謡以外はジャンル問わず、自分で良いなぁと思った曲を聴いてます。', 'では、オススメの曲は教えてください。', 'といっても、冬のソナタとか天国の階段とかはまだ見たことがないんです。']\n",
      "['あなたの趣味は何ですか?', '趣味は…寝ること、漫画を読むこと、ネットサーフィン、などなどです。', '小説は読みますか?', 'すごいですね。私はあまり読書できないので。']\n",
      "['小説は読みますか?', 'すごいですね。私はあまり読書できないので。', 'テレビは見ますか?', '衝撃の新事実なんすけど。']\n",
      "['どこに?コンサート?', '地球の裏側とかにもですか?', 'さすが!リオオリンピックだからね', 'そちらこそ!']\n",
      "['こんにちわ。チャットはよくやってます。チャット好きですか?', '最近は全くしません。MSNメッセンジャーでは話したりします。', 'そうなんですか。メッセンジャーは楽しいですか?', '何人で組んでいたのですか?']\n",
      "['機会は作ればいいんですよ。今度一緒にプールに行きましょう。', 'あ、プールは行かないと駄目ですね。', '強制はしませんよ。プール以外に好きな所ありますか?', 'うん、知らないなあ。でもありそうですよね。']\n",
      "['どんな感じなのですか?', '何だか取り留めのない朝食の話をしました。', '朝食は何でしたか?', 'うーん、無難に麺類で済ませようかと思ってます。']\n",
      "['そばは好きですか?', '伝わる物はあると思う。', '何が伝わってきますか?', '今回のもかなりよかったんですよね、でも前回、えっとどこでしたっけ、時代絵巻みたいな開会式のアトラクションがあったのは。']\n",
      "['南になにがあるのですか?', 'がんばりましょう', 'がんばると言えば、オリンピック見いてますか?', 'ありがとうございます、オリンピックは予選くらいしか見られませんね。']\n",
      "['ゲームやスマホは最近すごく高くなりましたね。', '質は痛みにくさという点です。本当野菜が高くて、手が出せません。', '今は夏なのでそうでもないのですが、冬は葉物が高いですよね。', 'ウインタースポーツって、寒さ対策とか、色々と大変そうなイメージがあります。']\n",
      "['マージャンは点数の計算とかもあって、頭を使うイメージがあります。', '頭はどうか分かりませんが、自己組織化というのは、物質でも生き物でも社会でも、自律的にパターンを形成する現象のことです。', 'きっと全ての事象がランダムではなく、何かしらの規則性を持っているんでしょうね。', '最初に日常会話教えてもらったんで、ハングル文字覚えようと頑張ったんですけど暗記が苦手なんで、今じゃ全然覚えてないですでも、しっかり覚えてしまえば規則性もあって覚えれると思いますよ。']\n",
      "['空気清浄機に蚊取り機能が付いたんだって!', '何系の腎臓病なの?', 'アルコール関係かな', '即、ブラックリスト登録ですよっ!!笑い']\n",
      "['電話も便利になったよね', 'てことは、PCから入れるんですよね?', 'どうだろ。覚えてないな', 'こけた。笑い金メダルとった選手が「ライバルは日本の國母だ」って言ってたのになぁ。']\n",
      "['どのぐらいが限界ですか?', '一ゃ楽しそう!その場でしか食べられない物って絶対ありますもんね。', '嫌いな食べ物は何ですか?', '人に対して好き嫌いがはっきりしているので短所ですね。']\n",
      "['素晴らしいですね。私は、すき焼きなど鍋料理が得意です。', '肉!おいしいですよね!でも私、脂身が多すぎると気持ち悪くなってしまうので、あまり高級な肉は駄目なんですよ。', 'へー。ジンギスカンはどうですか?', 'やっぱり鍋でゆでたパスタには敵いませんがなかなかいけますよ。']\n",
      "['へー。ジンギスカンはどうですか?', 'やっぱり鍋でゆでたパスタには敵いませんがなかなかいけますよ。', 'ですよね。好きなパスタ料理は何ですか?', 'フルーティーなのが好きなので、ワインやカクテルですね。']\n",
      "['ポケモンゴーがすごい人気ですよね', '本当ですよね。ディズニー行ったことありますか。', 'はい。ディズニーシーも行きました', 'USJも行く予定ですよ!そうなんですか!年始は名古屋にいるし、年始に行こうかな。']\n",
      "['そうですね、ピンクレディーやキャンディーズなど好きだったわ。', '私も淡いピンクは好きです。', '私はグリーンが大好きです。', 'いやいやいや。私好きなんて言ってません。']\n",
      "['夢を追いかけて頑張っている人は素敵ですよね。何か夢はありますか?', 'あせらなくても、大丈夫ですよ。私も、最後まで悩んでいましたから…。', 'ありがとうございます。私もよく悩みますが、ポジティブに考えることは大切ですね。', 'ぎゅうぎゅうですよ。空腹で乗車すると酔うのでガッツリ食べるようにしています。']\n",
      "['外国語がわからないので殆ど利用しません。', 'すみません。嘘でした。ゼミに外国人いました。', '外国語は何箇国語話せますか?', '教えるのは大変でしょう']\n",
      "['外国語は何箇国語話せますか?', '教えるのは大変でしょう', 'はい、とても大変です。あなたは人に教えた経験はありますか?', '元気ですよ。']\n",
      "['はじめて聞いた映画です。面白かったですか?', 'そうですね', 'だれが主演なのですか?', '誰かと一緒にみます。観てくれるかな・・']\n",
      "['だれが主演なのですか?', '誰かと一緒にみます。観てくれるかな・・', '恋人を一緒に見たいですね。恋人いますか?', 'ドラマはあまり見ないんですが、さんまの奴見ました。']\n",
      "['写真屋さんなら違いがわかるでしょうね。', '構図などはあまり気に竹刀のですが、照明にはうるさいかも。', 'どのようなこだわりがあるのですか?', '私は少し甘みのあるのが好きです。']\n",
      "['', 'こんにちは。好きな動物は何ですか?', 'こんにちは。私はパンダが大好きです。', 'いやいやいや。私好きなんて言ってません。']\n",
      "['え?私の好きな動物を聞かれたんだと思って、答えちゃいました。', '同じような環境ですね!動物といえば、', 'もしかして、家で飼ってるペットの事ですか?', 'なごみますよね。']\n",
      "['', 'こんにちは。好きな食べ物は何ですか?', 'こんにちは、私はカレーが好きですね。あなたはどうですか?', '私も、占いは結構信じます。血液型から星座、6星の奴とか。']\n",
      "['結構当たりますよね。今迄で一番良かった占い結果はどんな物でしたか?', '血液型占いとか、6星占術とかです。', 'なるほど。占い師に見てもらったことはあるのですか?', '特に、痛みがなければ行かなくても良いと思いますよ。']\n",
      "['不思議だ。それって時間を飛び越えたってことですか?', '「時は金なり」って昔の人は良く言った物ですね。', '確かにそうですよね。時間の使い方に気をつけてますか?', 'ポーランドはどうでした?']\n"
     ]
    }
   ],
   "source": [
    "for yt, yp, x in zip(y, y_pred, X_str):\n",
    "    if yt==1 and yp==0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
