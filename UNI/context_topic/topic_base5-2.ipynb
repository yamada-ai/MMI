{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy_4test(convs, N=4):\n",
    "    errors = [\"Topic transition error\", 'Lack of information', 'Unclear intention']\n",
    "    # errors = errors[:1]\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in convs:\n",
    "        dialogue = [\"\"]*N\n",
    "        for i, ut in enumerate( conv ) :\n",
    "            # ユーザ発話駆動\n",
    "            dialogue.append(clean_text( ut.utt) )\n",
    "            if ut.is_exist_error():\n",
    "                X.append( dialogue[-N:] )\n",
    "                    # X.append(dialogue[-N:])\n",
    "                if ut.is_error_included(errors) :\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "X_str, y = make_Xy_4test(convs, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from sentence_transformers import models\n",
    "\n",
    "bert_path = \"../../corpus/pretrained/sbert_unclear1\"\n",
    "sbert = SentenceTransformer(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forward_all_str = sum(X_str, [])\n",
    "x_length = len(X_forward_all_str)//N\n",
    "# X_topic_vec = smodel.encode(X_forward_all_str).reshape(x_length, N, -1)\n",
    "X_topic_vec = sbert.encode(X_forward_all_str).reshape(x_length, N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2feature(vector):\n",
    "    diff = np.abs( vector[0] - vector[1] )\n",
    "    return np.concatenate([vector.flatten(), diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 768\n",
    "def sentence2formated(vectors):\n",
    "    features = []\n",
    "    prev_vector = np.zeros(emb_dim)\n",
    "    for i, vector in enumerate(vectors):\n",
    "        feature = vec2feature( np.array([prev_vector, vector]) ) \n",
    "        features.append(feature)\n",
    "        prev_vector = vector\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topic = np.array([ sentence2formated(vec) for vec in X_topic_vec ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_topic)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, topic_dim, forward_dim, topic_hid, for_hid, tagset_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.tlen = topic_dim\n",
    "        self.flen = forward_dim\n",
    "        # self.hidden = hidden_dim\n",
    "        # 768->256\n",
    "        self.tlstm = nn.LSTM(topic_dim, topic_hid, batch_first=True)\n",
    "        # self.lay2_lstm = nn.LSTM(hidden_dim+forward_dim//2, hidden_dim2, batch_first=True)\n",
    "        self.flstm = nn.LSTM(forward_dim, for_hid, batch_first=True)\n",
    "        # self.for2hid = nn.Linear(forward_dim , forward_dim//2)\n",
    "        self.hid2out = nn.Linear(topic_hid , tagset_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_topic = x[:, :, :self.tlen].to(torch.float)\n",
    "        # x_for_hid = self.for2hid(x_forward)\n",
    "        # print(x_topic.shape)\n",
    "\n",
    "        # forward_c = torch.stack( [ self.fmodel.last_context(xfid) for xfid in x_forward_id])\n",
    "        # topic_out, _ = self.tlstm(x_topic)\n",
    "        _, tout = self.tlstm(x_topic)\n",
    "        out = self.hid2out(tout[0][0] )\n",
    "        y = self.softmax(out)\n",
    "        \n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
