{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../corpus/NTT/persona.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    convs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def make_Xy(convs, n=4, rate=3):\n",
    "\n",
    "    X_str = []\n",
    "    y = []\n",
    "\n",
    "    all_utt = []\n",
    "    for did in tqdm( convs[\"convs\"] ) :\n",
    "        dids = list( did.keys() )[0]\n",
    "        all_utt += did[dids]\n",
    "    random.shuffle(all_utt)\n",
    "\n",
    "    j = 0\n",
    "\n",
    "    for did in tqdm( convs[\"convs\"] ):\n",
    "        dids = list( did.keys() )[0]\n",
    "        conv = did[dids]\n",
    "        # print(conv)\n",
    "        for i in range(n-1, len(conv)):\n",
    "            p = (i-n+1)\n",
    "            # print(i, \"[{0}:{1}]\".format(p, p+n), conv[p:p+n-1])\n",
    "            # 正例\n",
    "            if i%rate != 0:\n",
    "                X_str.append( conv[p:p+n] )\n",
    "                y.append(0)\n",
    "                # print(i, conv[p:p+n])\n",
    "            # 負例\n",
    "            else:\n",
    "                X_str.append( conv[p:p+n-1]+[all_utt[j]] )\n",
    "                j += 1\n",
    "                y.append(1)\n",
    "    \n",
    "    return X_str, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5016/5016 [00:00<00:00, 1487775.18it/s]\n",
      "100%|██████████| 5016/5016 [00:00<00:00, 58752.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# n: 発話長， rate: エラー発話の確率\n",
    "X_str_topic, y = make_Xy(convs, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4674"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_str_topic[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str_topic_ = X_str_topic[::30]\n",
    "y_ = y[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-1403b403e768>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "y_.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[612] 2022-01-02 15:46:53,028 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/model.vec\n",
      "[612] 2022-01-02 15:47:55,875 Info gensim.utils :KeyedVectors lifecycle event {'msg': 'loaded (351122, 300) matrix of type float32 from ../../corpus/w2v/model.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-01-02T15:47:55.875335', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/context_topic/symbol.pickle\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_dataname = \"../../corpus/collocation/ppmi_ntt1\"\n",
    "ppmi_matrix2 = np.load(ppmi_dataname+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyoshima_set = set(\"NOUN PROPN VERB ADJ\".split())\n",
    "\n",
    "def w2v(word, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    if word in SYMBOL_w2v:\n",
    "        vector = SYMBOL_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOL_w2v[\"[UNK]\"]\n",
    "    return vector\n",
    "\n",
    "def filtering(doc, filter_set):\n",
    "    left = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in filter_set:\n",
    "            left.append(token.lemma_)\n",
    "    return left if len(left)>0 else [\"[NONE]\"]\n",
    "\n",
    "def doc2vec(doc, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    left = filtering(doc, toyoshima_set)\n",
    "    return np.mean([ w2v(w, w2v_model, SYMBOL_w2v) for w in left], axis=0)\n",
    "\n",
    "# 副詞など，ほぼすべて\n",
    "def doc2vec2(doc, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    left = filtering(doc, independent_set)\n",
    "    return np.mean([ w2v(w, w2v_model, SYMBOL_w2v) for w in left], axis=0)\n",
    "\n",
    "def sentence2formated(sen, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    docs = sentence2docs(sen, sents_span=False)\n",
    "    vector = [np.zeros(300)]\n",
    "    for i, doc in enumerate(docs):\n",
    "        if i==0:\n",
    "            prev_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "        else:\n",
    "            current_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "            diff_vec = np.abs(prev_vector-current_vector)\n",
    "            norm = np.linalg.norm(diff_vec)\n",
    "            if norm==0:\n",
    "                norm = 1            \n",
    "            # vector.append( diff_vec/norm )\n",
    "            vector.append( diff_vec)\n",
    "            prev_vector = current_vector\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_x_name = \"../X_y_data/context_topic/X_topic_mini\"\n",
    "topic_y_name = \"../X_y_data/context_topic/y_topic_mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load ../X_y_data/context_topic/X_topic_mini.npy\n",
      "success load : ../models/context_topic/symbol.pickle\n",
      "success load symbol.pickle\n"
     ]
    }
   ],
   "source": [
    "symbol_path = \"../models/context_topic/\"\n",
    "symbol_name = \"symbol.pickle\"\n",
    "symbolM = DataManager(symbol_path)\n",
    "\n",
    "if os.path.exists(topic_x_name+\".npy\"):\n",
    "    X_topic = np.load(topic_x_name+\".npy\")\n",
    "    y = np.load(topic_y_name+\".npy\")\n",
    "    print(\"success load {0}.npy\".format(topic_x_name))\n",
    "    SYMBOL_w2v = symbolM.load_data(symbol_name)\n",
    "    print(\"success load {0}\".format(symbol_name))\n",
    "else:\n",
    "    w2v_path = \"../../corpus/w2v/\"\n",
    "    # fasttext\n",
    "    # https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "    w2v_name =  \"dep-ja-300dim\"\n",
    "    w2v_name =  \"model.vec\"\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)\n",
    "\n",
    "    wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "    add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\", \"[NONE]\"]\n",
    "    add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "    add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "    SYMBOL_w2v = dict(zip(add_keys, add_weights))\n",
    "\n",
    "    \n",
    "    symbolM.save_data(symbol_name, SYMBOL_w2v)\n",
    "\n",
    "    X_topic = []\n",
    "    for conv in tqdm(X_str_topic_):\n",
    "        vector = sentence2formated(conv, w2v_model, SYMBOL_w2v)\n",
    "        X_topic.append(vector)\n",
    "    X_topic = np.array(X_topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1558,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(topic_x_name, X_topic)\n",
    "np.save(topic_y_name, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, vocab_dict):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim , tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        tag_space = self.hidden2tag(torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 ))\n",
    "        y =self.softmax(tag_space)\n",
    "        return y\n",
    "    \n",
    "    def last_context(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # print(emb1.shape)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        context = torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 )\n",
    "        return context\n",
    "    \n",
    "    def text2context(self, text):\n",
    "        if isinstance(text, str):\n",
    "            utt_id = self._sentence2ids(text, self.vocab_dict)\n",
    "            utt_id_tensor = torch.tensor( [utt_id] , device='cuda:0', dtype=torch.int)\n",
    "            # utt_id_tensor = torch.tensor( [utt_id] , device='cpu', dtype=torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        if isinstance(text, list):\n",
    "            X = self._make_X(text, self.vocab_dict)\n",
    "            utt_id_tensor = X.to(torch.int).cuda()\n",
    "            # utt_id_tensor = X.to(torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def _sentence2ids(self, sentence:str, vocab_dict:dict):\n",
    "        doc = self._sentence2formated(sentence)\n",
    "        ids = np.zeros(len(doc))\n",
    "        for i, key in enumerate(doc):\n",
    "            # key = token.orth_\n",
    "            if key in vocab_dict:\n",
    "                ids[i] = vocab_dict[key]\n",
    "            else:\n",
    "                ids[i] = vocab_dict[\"[UNK]\"]\n",
    "        return ids\n",
    "    \n",
    "    def _sentence2formated(self, sen):\n",
    "        return sum( fill_SYMBOL_ONE( sentence2normalize_noun(sen) ), [] )\n",
    "    \n",
    "    def _padding_vector(self, Xseq):\n",
    "        Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "        Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "        Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "        return Xseq\n",
    "\n",
    "\n",
    "    def _make_X(self, utt_list:list, vocab_dict:dict):\n",
    "        utt_id_list = []\n",
    "        for utt in tqdm( utt_list) :\n",
    "            utt_id = self._sentence2ids(utt, vocab_dict)\n",
    "            utt_id_list.append(utt_id)\n",
    "\n",
    "        utt_id_pad = self._padding_vector(utt_id_list)\n",
    "        upl = len(utt_id_pad[0])\n",
    "        # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "        # print(usr_pad_len, sys_pad_len)\n",
    "        X = torch.zeros( (len(utt_list), upl) )\n",
    "        for i, u in enumerate(utt_id_pad):\n",
    "            X[i, :upl] = u\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response2/forward_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response2/\"\n",
    "model_name = \"forward_v2.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "fmodel = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機能ベクトルの特徴量化\n",
    "X_forward_all_str = sum(X_str_topic_, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_xy_name = \"../X_y_data/context_topic/X_forward_mini_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load ../X_y_data/context_topic/X_forward_mini_id.npy\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(forward_xy_name+\".npy\"):\n",
    "    X_forward_ids  = np.load(forward_xy_name+\".npy\")\n",
    "    print(\"success load {0}.npy\".format(forward_xy_name))\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "    # X_forward_l =  fmodel.text2context(X_forward_all_str[:50])\n",
    "        # 手で書くしかない\n",
    "        fmodel.cpu()\n",
    "        x_length = len(X_str_topic_)\n",
    "        X_forward_ids = fmodel._make_X(X_forward_all_str, fmodel.vocab_dict).to(torch.int)\n",
    "        X_forward_ids = X_forward_ids.reshape(x_length, 4, -1).numpy()\n",
    "        # X_forward = fmodel.last_context(X_forward_ids).numpy()\n",
    "        # X_forward = X_forward.reshape(-1, 4, 256)\n",
    "        fmodel.cuda()\n",
    "        # X_forward_l = np.array( fmodel.text2context(X_forward_all_str).cpu() ) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_length = len(X_str_topic_)\n",
    "X_forward_ids = X_forward_ids.reshape(x_length, 4, -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(forward_xy_name, X_forward_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1558, 4, 75)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_forward_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = np.concatenate([X_topic, X_forward_ids], axis=2)\n",
    "X = torch.from_numpy(X_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_, test_size=0.30, random_state=5, stratify=y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, topic_dim, id_len, forward_dim,  hidden_dim, tagset_size, fmodel_path=\"../models/response2/\"):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.tlen = topic_dim\n",
    "        self.idlen = id_len\n",
    "        self.flen = forward_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.tlstm = nn.LSTM(topic_dim, hidden_dim, batch_first=True)\n",
    "        self.lay2_lstm = nn.LSTM(hidden_dim+forward_dim, hidden_dim, batch_first=True)\n",
    "        self.hid2out = nn.Linear(hidden_dim , tagset_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        fmodel_name = \"forward_v2.pickle\"\n",
    "        modelM = DataManager(fmodel_path)\n",
    "        self.fmodel = modelM.load_data(fmodel_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_topic = x[:, :, :self.tlen].to(torch.float)\n",
    "        x_forward_id = x[:, :, self.tlen:].to(torch.int)\n",
    "        # print(x_topic.shape)\n",
    "\n",
    "        forward_c = torch.stack( [ self.fmodel.last_context(xfid) for xfid in x_forward_id])\n",
    "        topic_out, _ = self.tlstm(x_topic)\n",
    "\n",
    "        # print(\"forward_c: \", forward_c.shape, \"topic_out: \", topic_out.shape)\n",
    "        x_lay2 = torch.cat([forward_c, topic_out], dim=2)\n",
    "        # print(\"x_lay2: \", x_lay2.shape)\n",
    "\n",
    "        _, hc = self.lay2_lstm(x_lay2)\n",
    "        out = self.hid2out(hc[0][0])\n",
    "        y = self.softmax(out)\n",
    "\n",
    "        # print(\"hc: \",len(hc),  hc[0][0].shape)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "epoch_ = 300\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_DIM = 300\n",
    "ID_LEN = 75\n",
    "FORWARD_DIM = 256\n",
    "HIDDEN_DIM = TOPIC_DIM//2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response2/forward_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "model = TopicClassifier(TOPIC_DIM, ID_LEN, FORWARD_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7503, -0.6391],\n",
       "        [-0.7394, -0.6489],\n",
       "        [-0.7448, -0.6440]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X[:3].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  7%|▋         | 20/300 [28:57<6:43:58, 86.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 \t loss 0.01004382390237879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 40/300 [58:01<6:15:41, 86.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 \t loss 0.00231717195129022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 60/300 [1:26:57<5:48:20, 87.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 \t loss 0.0008760984319309273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 72/300 [1:45:46<5:34:57, 88.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-e95f79abcb28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-31cd345ff178>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# print(x_topic.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mforward_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxfid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_forward_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtopic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-31cd345ff178>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# print(x_topic.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mforward_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxfid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_forward_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtopic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-07e9640992d8>\u001b[0m in \u001b[0;36mlast_context\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0memb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# print(emb1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mlstm1_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm1_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 662\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in tqdm( range(epoch_)  ):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        X_t_tensor = data[0].cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score,  y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0')\n",
    "    y_tensor = torch.tensor(y_test, device='cuda:0', dtype=torch.long)\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(test, pred):\n",
    "    if len(collections.Counter(pred)) <= 2:\n",
    "        print('confusion matrix = \\n', confusion_matrix(y_true=test, y_pred=pred))\n",
    "        print('accuracy = ', accuracy_score(y_true=test, y_pred=pred))\n",
    "        print('precision = ', precision_score(y_true=test, y_pred=pred))\n",
    "        print('recall = ', recall_score(y_true=test, y_pred=pred))\n",
    "        print('f1 score = ', f1_score(y_true=test, y_pred=pred))\n",
    "    else:\n",
    "        print('confusion matrix = \\n', confusion_matrix(y_true=test, y_pred=pred))\n",
    "        print('accuracy = ', accuracy_score(y_true=test, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[222  77]\n",
      " [ 88  81]]\n",
      "accuracy =  0.6474358974358975\n",
      "precision =  0.5126582278481012\n",
      "recall =  0.47928994082840237\n",
      "f1 score =  0.4954128440366972\n"
     ]
    }
   ],
   "source": [
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
