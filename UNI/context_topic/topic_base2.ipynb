{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../corpus/NTT/persona.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    convs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def make_Xy(convs, n=4, rate=3):\n",
    "\n",
    "    X_str = []\n",
    "    y = []\n",
    "\n",
    "    all_utt = []\n",
    "    for did in tqdm( convs[\"convs\"] ) :\n",
    "        dids = list( did.keys() )[0]\n",
    "        all_utt += did[dids]\n",
    "    random.shuffle(all_utt)\n",
    "\n",
    "    j = 0\n",
    "\n",
    "    for did in tqdm( convs[\"convs\"] ):\n",
    "        dids = list( did.keys() )[0]\n",
    "        conv = did[dids]\n",
    "        # print(conv)\n",
    "        for i in range(n-1, len(conv)):\n",
    "            p = (i-n+1)\n",
    "            # print(i, \"[{0}:{1}]\".format(p, p+n), conv[p:p+n-1])\n",
    "            # 正例\n",
    "            if i%rate != 0:\n",
    "                X_str.append( conv[p:p+n] )\n",
    "                y.append(0)\n",
    "                # print(i, conv[p:p+n])\n",
    "            # 負例\n",
    "            else:\n",
    "                X_str.append( conv[p:p+n-1]+[all_utt[j]] )\n",
    "                j += 1\n",
    "                y.append(1)\n",
    "    \n",
    "    return X_str, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5016/5016 [00:00<00:00, 1359260.17it/s]\n",
      "100%|██████████| 5016/5016 [00:00<00:00, 59041.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# n: 発話長， rate: エラー発話の確率\n",
    "X_str, y = make_Xy(convs, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3131] 2021-12-31 16:49:04,665 Info gensim.models.keyedvectors :loading projection weights from ../../corpus/w2v/model.vec\n",
      "[3131] 2021-12-31 16:50:06,571 Info gensim.utils :KeyedVectors lifecycle event {'msg': 'loaded (351122, 300) matrix of type float32 from ../../corpus/w2v/model.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-12-31T16:50:06.571591', 'gensim': '4.0.1', 'python': '3.6.9 (default, Jan 26 2021, 15:33:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_path = \"../../corpus/w2v/\"\n",
    "# fasttext\n",
    "# https://qiita.com/Hironsan/items/513b9f93752ecee9e670\n",
    "w2v_name =  \"dep-ja-300dim\"\n",
    "w2v_name =  \"model.vec\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(w2v_path+w2v_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsv_dim = w2v_model[\"あ\"].shape[0]\n",
    "add_keys = [\"FOS\", \"EOS\", \"[SEP]\", \"[UNK]\", \"[NONE]\"]\n",
    "add_weights = [np.random.randn(wsv_dim) for _ in range(len(add_keys))]\n",
    "add_weights = [ v/np.linalg.norm(v) for v in add_weights ]\n",
    "SYMBOL_w2v = dict(zip(add_keys, add_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/context_topic/symbol.pickle\n"
     ]
    }
   ],
   "source": [
    "symbol_path = \"../models/context_topic/\"\n",
    "symbol_name = \"symbol.pickle\"\n",
    "symbolM = DataManager(symbol_path)\n",
    "symbolM.save_data(symbol_name, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_dataname = \"../../corpus/collocation/ppmi_ntt1\"\n",
    "ppmi_matrix2 = np.load(ppmi_dataname+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyoshima_set = set(\"NOUN PROPN VERB ADJ\".split())\n",
    "\n",
    "def w2v(word, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    if word in SYMBOL_w2v:\n",
    "        vector = SYMBOL_w2v[word]\n",
    "    elif word in w2v_model:\n",
    "        vector = w2v_model[word]\n",
    "    else:\n",
    "        vector = SYMBOL_w2v[\"[UNK]\"]\n",
    "    return vector\n",
    "\n",
    "def filtering(doc, filter_set):\n",
    "    left = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in filter_set:\n",
    "            left.append(token.lemma_)\n",
    "    return left if len(left)>0 else [\"[NONE]\"]\n",
    "\n",
    "def doc2vec(doc, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    left = filtering(doc, toyoshima_set)\n",
    "    return np.mean([ w2v(w, w2v_model, SYMBOL_w2v) for w in left], axis=0)\n",
    "\n",
    "# 副詞など，ほぼすべて\n",
    "def doc2vec2(doc, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    left = filtering(doc, independent_set)\n",
    "    return np.mean([ w2v(w, w2v_model, SYMBOL_w2v) for w in left], axis=0)\n",
    "\n",
    "def sentence2formated(sen, w2v_model:KeyedVectors, SYMBOL_w2v:dict):\n",
    "    docs = sentence2docs(sen, sents_span=False)\n",
    "    vector = [np.zeros(300)]\n",
    "    for i, doc in enumerate(docs):\n",
    "        if i==0:\n",
    "            prev_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "        else:\n",
    "            current_vector = doc2vec2(doc, w2v_model, SYMBOL_w2v)\n",
    "            diff_vec = np.abs(prev_vector-current_vector)\n",
    "            norm = np.linalg.norm(diff_vec)\n",
    "            if norm==0:\n",
    "                norm = 1            \n",
    "            # vector.append( diff_vec/norm )\n",
    "            vector.append( diff_vec)\n",
    "            prev_vector = current_vector\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = X_str[7]\n",
    "vector = sentence2formated(conv, w2v_model, SYMBOL_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 2.1304090245956444, 1.270484948329428, 0.9575943953814545]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.linalg.norm(v) for v in vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, vocab_dict):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim , tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        tag_space = self.hidden2tag(torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 ))\n",
    "        y =self.softmax(tag_space)\n",
    "        return y\n",
    "    \n",
    "    def last_context(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # print(emb1.shape)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        context = torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 )\n",
    "        return context\n",
    "    \n",
    "    def text2context(self, text):\n",
    "        if isinstance(text, str):\n",
    "            utt_id = self._sentence2ids(text, self.vocab_dict)\n",
    "            utt_id_tensor = torch.tensor( [utt_id] , device='cuda:0', dtype=torch.int)\n",
    "            # utt_id_tensor = torch.tensor( [utt_id] , device='cpu', dtype=torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        if isinstance(text, list):\n",
    "            X = self._make_X(text, self.vocab_dict)\n",
    "            utt_id_tensor = X.to(torch.int).cuda()\n",
    "            # utt_id_tensor = X.to(torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def _sentence2ids(self, sentence:str, vocab_dict:dict):\n",
    "        doc = self._sentence2formated(sentence)\n",
    "        ids = np.zeros(len(doc))\n",
    "        for i, key in enumerate(doc):\n",
    "            # key = token.orth_\n",
    "            if key in vocab_dict:\n",
    "                ids[i] = vocab_dict[key]\n",
    "            else:\n",
    "                ids[i] = vocab_dict[\"[UNK]\"]\n",
    "        return ids\n",
    "    \n",
    "    def _sentence2formated(self, sen):\n",
    "        return sum( fill_SYMBOL_ONE( sentence2normalize_noun(sen) ), [] )\n",
    "    \n",
    "    def _padding_vector(self, Xseq):\n",
    "        Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "        Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "        Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "        return Xseq\n",
    "\n",
    "\n",
    "    def _make_X(self, utt_list:list, vocab_dict:dict):\n",
    "        utt_id_list = []\n",
    "        for utt in tqdm( utt_list) :\n",
    "            utt_id = self._sentence2ids(utt, vocab_dict)\n",
    "            utt_id_list.append(utt_id)\n",
    "\n",
    "        utt_id_pad = self._padding_vector(utt_id_list)\n",
    "        upl = len(utt_id_pad[0])\n",
    "        # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "        # print(usr_pad_len, sys_pad_len)\n",
    "        X = torch.zeros( (len(utt_list), upl) )\n",
    "        for i, u in enumerate(utt_id_pad):\n",
    "            X[i, :upl] = u\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response2/forward_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response2/\"\n",
    "model_name = \"forward_v2.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "fmodel = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, w2v_model, SYMBOL_w2v):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # モデルを2つ定義\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    \n",
    "        self.w2v_model = w2v_model\n",
    "        self.SYMBOL_w2v = SYMBOL_w2v\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out, hc = self.lstm(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def set_forward_model(self, fmodel:LSTMClassifier):\n",
    "        self.fmodel = fmodel\n",
    "        self.lstm_f = self.fmodel.lstm1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
