{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xy_4test(convs, N=4):\n",
    "    # errors = [\"Topic transition error\", 'Lack of information', 'Unclear intention']\n",
    "    errors = [\"Unclear intention\"]\n",
    "    X = []\n",
    "    y = []\n",
    "    for conv in convs:\n",
    "        dialogue = [\"\"]*N\n",
    "        for i, ut in enumerate( conv ) :\n",
    "            # ユーザ発話駆動\n",
    "            dialogue.append(clean_text( ut.utt) )\n",
    "            if ut.is_exist_error():\n",
    "                X.append( dialogue[-N:] )\n",
    "                    # X.append(dialogue[-N:])\n",
    "                if ut.is_error_included(errors) :\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "X_str, y = make_Xy_4test(convs, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, vocab_dict):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim,  padding_idx=0)\n",
    "        # モデルを2つ定義\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        # self.lstm2 = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True,  bidirectional=True )\n",
    "        self.hidden2tag = nn.Linear(hidden_dim , tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "        self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        tag_space = self.hidden2tag(torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 ))\n",
    "        y =self.softmax(tag_space)\n",
    "        return y\n",
    "    \n",
    "    def last_context(self, x):\n",
    "        emb1 = self.word_embeddings(x)\n",
    "        # print(emb1.shape)\n",
    "        _, lstm1_out = self.lstm1(emb1)\n",
    "        context = torch.cat([ lstm1_out[0][0], lstm1_out[0][1]], dim=1 )\n",
    "        return context\n",
    "    \n",
    "    def text2context(self, text):\n",
    "        if isinstance(text, str):\n",
    "            utt_id = self._sentence2ids(text, self.vocab_dict)\n",
    "            utt_id_tensor = torch.tensor( [utt_id] , device='cuda:0', dtype=torch.int)\n",
    "            # utt_id_tensor = torch.tensor( [utt_id] , device='cpu', dtype=torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        if isinstance(text, list):\n",
    "            X = self._make_X(text, self.vocab_dict)\n",
    "            utt_id_tensor = X.to(torch.int).cuda()\n",
    "            # utt_id_tensor = X.to(torch.int)\n",
    "            return self.last_context(utt_id_tensor)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    def _sentence2ids(self, sentence:str, vocab_dict:dict):\n",
    "        doc = self._sentence2formated(sentence)\n",
    "        ids = np.zeros(len(doc))\n",
    "        for i, key in enumerate(doc):\n",
    "            # key = token.orth_\n",
    "            if key in vocab_dict:\n",
    "                ids[i] = vocab_dict[key]\n",
    "            else:\n",
    "                ids[i] = vocab_dict[\"[UNK]\"]\n",
    "        return ids\n",
    "    \n",
    "    def _sentence2formated(self, sen):\n",
    "        return sum( fill_SYMBOL_ONE( sentence2normalize_noun(sen) ), [] )\n",
    "    \n",
    "    def _padding_vector(self, Xseq):\n",
    "        Xseq = [ torch.tensor( xseq[:, None] ) for xseq in Xseq]\n",
    "        Xseq = rnn.pad_sequence(Xseq, batch_first=True)\n",
    "        Xseq = [ torch.flatten(xseq) for xseq in Xseq ] \n",
    "        return Xseq\n",
    "\n",
    "\n",
    "    def _make_X(self, utt_list:list, vocab_dict:dict):\n",
    "        utt_id_list = []\n",
    "        for utt in tqdm( utt_list) :\n",
    "            utt_id = self._sentence2ids(utt, vocab_dict)\n",
    "            utt_id_list.append(utt_id)\n",
    "\n",
    "        utt_id_pad = self._padding_vector(utt_id_list)\n",
    "        upl = len(utt_id_pad[0])\n",
    "        # X =   [ torch.Tensor([u, s]) for u, s in zip(usr_id_pad, sys_id_pad) ] \n",
    "        # print(usr_pad_len, sys_pad_len)\n",
    "        X = torch.zeros( (len(utt_list), upl) )\n",
    "        for i, u in enumerate(utt_id_pad):\n",
    "            X[i, :upl] = u\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../models/response2/forward_v2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/response2/\"\n",
    "model_name = \"forward_v2.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "fmodel = modelM.load_data(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_xy_name = \"../X_y_data/context_topic/X_forward_topic_ERROR_N={0}\".format(N)\n",
    "forward_xy_name = \"../X_y_data/context_topic/X_forward_topic_ERROR_N={0}\".format(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4047/4047 [01:45<00:00, 38.25it/s]\n"
     ]
    }
   ],
   "source": [
    "X_forward_all_str = sum(X_str, [])\n",
    "\n",
    "if os.path.exists(forward_xy_name+\".npy\"):\n",
    "    # X_forward_ids  = np.load(forward_xy_name+\".npy\")\n",
    "    X_forward  = np.load(forward_xy_name+\".npy\")\n",
    "    print(\"success load {0}.npy\".format(forward_xy_name))\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        fmodel.cpu()\n",
    "        # X_forward_l =  fmodel.text2context(X_str)\n",
    "        # 手で書くしかない\n",
    "        x_length = len(X_forward_all_str)//N\n",
    "        X_forward_ids = fmodel._make_X(X_forward_all_str, fmodel.vocab_dict).to(torch.int)\n",
    "        X_forward_ids = X_forward_ids.reshape(x_length, N, -1)\n",
    "        X_forward = np.array( [fmodel.last_context(Xfi).numpy() for Xfi in X_forward_ids] ) \n",
    "        # X_forward = X_forward.reshape(-1, 4, 256)\n",
    "        fmodel.cuda()\n",
    "        # X_forward_l = np.array( fmodel.text2context(X_forward_all_str).cpu() ) \n",
    "        # np.save(forward_xy_name, X_forward_ids)\n",
    "        np.save(forward_xy_name, X_forward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../corpus/cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[1340] 2022-01-12 16:17:10,031 Info sentence_transformers.SentenceTransformer :Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76c7499004f442681520066f793ffe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "\n",
    "download_path = \"../../corpus/\"\n",
    "# download_path = \"\"\n",
    "transformer = models.Transformer(download_path+'cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "pooling = models.Pooling(transformer.get_word_embedding_dimension(),    \n",
    "  pooling_mode_mean_tokens=True,\n",
    "  pooling_mode_cls_token=False, \n",
    "  pooling_mode_max_tokens=False\n",
    ")\n",
    "smodel = SentenceTransformer(modules=[transformer, pooling])\n",
    "\n",
    "sentences = ['吾輩は猫である']\n",
    "embeddings = smodel.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1340] 2022-01-12 16:17:10,751 Info sentence_transformers.SentenceTransformer :Load pretrained SentenceTransformer: ../../corpus/pretrained/sbert_unclear1\n",
      "[1340] 2022-01-12 16:17:13,120 Info sentence_transformers.SentenceTransformer :Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from sentence_transformers import models\n",
    "\n",
    "bert_path = \"../../corpus/pretrained/sbert_unclear1\"\n",
    "sbert = SentenceTransformer(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316d06f0ca3a4c04b40dfd10ccf0f473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_length = len(X_forward_all_str)//N\n",
    "# X_topic_vec = smodel.encode(X_forward_all_str).reshape(x_length, N, -1)\n",
    "X_topic_vec = sbert.encode(X_forward_all_str).reshape(x_length, N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2feature(vector):\n",
    "    diff = np.abs( vector[0] - vector[1] )\n",
    "    return np.concatenate([vector.flatten(), diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2formated(vectors):\n",
    "    features = []\n",
    "    prev_vector = np.zeros(emb_dim)\n",
    "    for i, vector in enumerate(vectors):\n",
    "        feature = vec2feature( np.array([prev_vector, vector]) ) \n",
    "        features.append(feature)\n",
    "        prev_vector = vector\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topic = np.array([ sentence2formated(vec) for vec in X_topic_vec ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1349, 3, 2304)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.concatenate([X_topic, X_forward_ids], axis=2)\n",
    "X = np.concatenate([X_topic, X_forward], axis=2)\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_len = emb_dim*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, topic_dim, forward_dim, hidden_dim, hidden_dim2, tagset_size, fmodel_path=\"../models/response2/\"):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.tlen = topic_dim\n",
    "        self.flen = forward_dim\n",
    "        self.hidden = hidden_dim\n",
    "        self.tlstm = nn.LSTM(topic_dim, hidden_dim, batch_first=True)\n",
    "        self.lay2_lstm = nn.LSTM(hidden_dim+forward_dim//2, hidden_dim2, batch_first=True)\n",
    "        self.for2hid = nn.Linear(forward_dim , forward_dim//2)\n",
    "        self.hid2out = nn.Linear(hidden_dim2 , tagset_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        # fmodel_name = \"forward_v2.pickle\"\n",
    "        # modelM = DataManager(fmodel_path)\n",
    "        # self.fmodel = modelM.load_data(fmodel_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_topic = x[:, :, :self.tlen].to(torch.float)\n",
    "        # x_forward_id = x[:, :, self.tlen:].to(torch.int)\n",
    "        x_forward = x[:, :, self.tlen:].to(torch.float)\n",
    "        x_for_hid = self.for2hid(x_forward)\n",
    "        # print(x_topic.shape)\n",
    "\n",
    "        # forward_c = torch.stack( [ self.fmodel.last_context(xfid) for xfid in x_forward_id])\n",
    "        topic_out, _ = self.tlstm(x_topic)\n",
    "        # print(\"topic_out: \", topic_out.shape)\n",
    "        topic_out = self.relu(topic_out)\n",
    "        # x_lay2 = torch.cat([topic_out, forward_c], dim=2)\n",
    "        # x_lay2 = torch.cat([topic_out, x_forward], dim=2)\n",
    "        x_lay2 = torch.cat([topic_out, self.relu(x_for_hid)], dim=2)\n",
    "        # print(\"x_lay2: \", x_lay2.shape)\n",
    "\n",
    "        _, hc = self.lay2_lstm(x_lay2)\n",
    "        out = self.hid2out(hc[0][0])\n",
    "        y = self.softmax(out)\n",
    "\n",
    "        # print(\"hc: \",len(hc),  hc[0][0].shape)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "epoch_ = 150\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_DIM = emb_dim*3\n",
    "FORWARD_DIM = 256\n",
    "HIDDEN_DIM = emb_dim\n",
    "HIDDEN_DIM2 = FORWARD_DIM//2\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicClassifier(TOPIC_DIM, FORWARD_DIM, HIDDEN_DIM, HIDDEN_DIM2, OUTPUT_DIM)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 13%|█▎        | 20/150 [00:12<01:17,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 \t loss 0.017147476959507912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 40/150 [00:24<01:05,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 \t loss 0.002150210741092451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 60/150 [00:35<00:53,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 \t loss 0.0005470665219036164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 80/150 [00:47<00:40,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80 \t loss 0.0002770730152406031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 100/150 [00:59<00:29,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 \t loss 0.0001752406114974292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 120/150 [01:11<00:17,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 120 \t loss 0.00012492437917899224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 140/150 [01:22<00:05,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 140 \t loss 9.55187356339593e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:28<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in tqdm( range(epoch_)  ):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        # X_t_tensor = torch.tensor(data[0], device='cuda:0', dtype=torch.int16)\n",
    "        X_t_tensor = data[0].cuda()\n",
    "        y_t_tensor = data[1].to(torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape , y_t_tensor.view(-1,1).shape)\n",
    "\n",
    "        score_ = model(X_t_tensor)\n",
    "        # print(X_t_tensor.shape, score.view(-1,5).shape, y_t_tensor.view(-1,1).shape)\n",
    "        loss_ = loss_function(score_,  y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score_\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[192  71]\n",
      " [ 93  49]]\n",
      "accuracy =  0.5950617283950618\n",
      "precision =  0.4083333333333333\n",
      "recall =  0.34507042253521125\n",
      "f1 score =  0.3740458015267175\n"
     ]
    }
   ],
   "source": [
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sbert + LSTM(前向き機能ベクトル固定)\n",
    "    - 話題遷移エラーと判定する割合が低い\n",
    "\n",
    "            confusion matrix = \n",
    "            [[598   4]\n",
    "            [ 57   1]]\n",
    "            accuracy =  0.9075757575757576\n",
    "            precision =  0.2\n",
    "            recall =  0.017241379310344827\n",
    "            f1 score =  0.031746031746031744\n",
    "\n",
    "- sbert + LSTM(前向き機能ベクトル学習)\n",
    "    - 比較的マシになったかも\n",
    "\n",
    "            confusion matrix = \n",
    "            [[587  15]\n",
    "            [ 46  12]]\n",
    "            accuracy =  0.9075757575757576\n",
    "            precision =  0.4444444444444444\n",
    "            recall =  0.20689655172413793\n",
    "            f1 score =  0.2823529411764706\n",
    "\n",
    "- 発話意図不明確も混ぜた\n",
    "    - ちょっと無視出来ない割合で未検出(N-4)\n",
    "        \n",
    "            confusion matrix = \n",
    "            [[362  83]\n",
    "            [125  90]]\n",
    "            accuracy =  0.6848484848484848\n",
    "            precision =  0.5202312138728323\n",
    "            recall =  0.4186046511627907\n",
    "            f1 score =  0.4639175257731959\n",
    "    \n",
    "    - 機能ベクトルに一度FFN を挟んだ(N=3)\n",
    "\n",
    "            confusion matrix = \n",
    "            [[363  82]\n",
    "            [109 106]]\n",
    "            accuracy =  0.7106060606060606\n",
    "            precision =  0.5638297872340425\n",
    "            recall =  0.4930232558139535\n",
    "            f1 score =  0.5260545905707196\n",
    "\n",
    "- 学習データを，エラーシステム発話を含むものだけにした\n",
    "    - 話題遷移エラーのみ\n",
    "\n",
    "                confusion matrix = \n",
    "                [[322  25]\n",
    "                [ 44  14]]\n",
    "                accuracy =  0.8296296296296296\n",
    "                precision =  0.358974358974359\n",
    "                recall =  0.2413793103448276\n",
    "                f1 score =  0.288659793814433\n",
    "\n",
    "   - 発話意図不明確のみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/context_topic/sbert_context_unclear.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM = DataManager(\"../models/context_topic/\")\n",
    "model_name = \"sbert_context_unclear.pickle\"\n",
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "convs_ = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_Xy_4test(convs, N=4):\n",
    "#     # errors = [\"Topic transition error\", 'Lack of information', 'Unclear intention']\n",
    "#     # errors = [\"Topic transition error\"]\n",
    "#     errors = [\"Topic transition error\"]\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for conv in convs:\n",
    "#         dialogue = [\"\"]*N\n",
    "#         for i, ut in enumerate( conv ) :\n",
    "#             # ユーザ発話駆動\n",
    "#             dialogue.append(clean_text( ut.utt) )\n",
    "#             if ut.is_exist_error():\n",
    "#                 X.append( dialogue[-N:] )\n",
    "#                     # X.append(dialogue[-N:])\n",
    "#                 if ut.is_error_included(errors) :\n",
    "#                     y.append(1)\n",
    "#                 else:\n",
    "#                     y.append(0)\n",
    "        \n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "X_str, y = make_Xy_4test(convs, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1349"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_xy_eval_name = \"../X_y_data/context_topic/X_forward_topic_ERROR_eval_N={0}\".format(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4047/4047 [01:44<00:00, 38.56it/s]\n"
     ]
    }
   ],
   "source": [
    "X_forward_all_str = sum(X_str, [])\n",
    "\n",
    "if os.path.exists(forward_xy_eval_name+\".npy\"):\n",
    "    # X_forward_ids  = np.load(forward_xy_name+\".npy\")\n",
    "    X_forward  = np.load(forward_xy_eval_name+\".npy\")\n",
    "    print(\"success load {0}.npy\".format(forward_xy_eval_name))\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        fmodel.cpu()\n",
    "        # X_forward_l =  fmodel.text2context(X_str)\n",
    "        # 手で書くしかない\n",
    "        x_length = len(X_forward_all_str)//N\n",
    "        X_forward_ids = fmodel._make_X(X_forward_all_str, fmodel.vocab_dict).to(torch.int)\n",
    "        X_forward_ids = X_forward_ids.reshape(x_length, N, -1)\n",
    "        X_forward = np.array( [fmodel.last_context(Xfi).numpy() for Xfi in X_forward_ids] ) \n",
    "        # X_forward = X_forward.reshape(-1, 4, 256)\n",
    "        fmodel.cuda()\n",
    "        # X_forward_l = np.array( fmodel.text2context(X_forward_all_str).cpu() ) \n",
    "        # np.save(forward_xy_name, X_forward_ids)\n",
    "        np.save(forward_xy_eval_name, X_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(forward_xy_eval_name, X_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b70c55b60d450290bd5cd07123efc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_length = len(X_forward_all_str)//N\n",
    "# X_topic_vec = smodel.encode(X_forward_all_str).reshape(x_length, N, -1)\n",
    "X_topic_vec = sbert.encode(X_forward_all_str).reshape(x_length, N, -1)\n",
    "X_topic = np.array([ sentence2formated(vec) for vec in X_topic_vec ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X_topic, X_forward], axis=2)\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X, device='cuda:0').float()\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[804  71]\n",
      " [ 93 381]]\n",
      "accuracy =  0.8784284655300222\n",
      "precision =  0.8429203539823009\n",
      "recall =  0.8037974683544303\n",
      "f1 score =  0.8228941684665226\n"
     ]
    }
   ],
   "source": [
    "score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 発話意図不明確，情報不足，話題遷移エラー全て\n",
    "\n",
    "        confusion matrix = \n",
    "        [[605  29]\n",
    "        [109 606]]\n",
    "        accuracy =  0.8977020014825797\n",
    "        precision =  0.9543307086614173\n",
    "        recall =  0.8475524475524475\n",
    "        f1 score =  0.8977777777777777\n",
    "\n",
    "- 話題遷移エラーのみ\n",
    "\n",
    "        confusion matrix = \n",
    "        [[1132   25]\n",
    "        [  44  148]]\n",
    "        accuracy =  0.9488510007412898\n",
    "        precision =  0.8554913294797688\n",
    "        recall =  0.7708333333333334\n",
    "        f1 score =  0.8109589041095892\n",
    "\n",
    "- 発話意図不明確のみ\n",
    "\n",
    "        confusion matrix = \n",
    "        [[804  71]\n",
    "        [ 93 381]]\n",
    "        accuracy =  0.8784284655300222\n",
    "        precision =  0.8429203539823009\n",
    "        recall =  0.8037974683544303\n",
    "        f1 score =  0.8228941684665226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y).count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
