{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "emb_dim = nlp(\"形態素\").vector.shape[0]\n",
    "emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_convs(path, filename):\n",
    "    with open(path+filename, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "        # length = json_data[\"length\"]\n",
    "        convs = json_data[\"convs\"]\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_dir = \"continue/\"\n",
    "cont_path = \"../../corpus/nucc/\" + cont_dir\n",
    "length = 2\n",
    "filename = \"cont{0}.json\".format(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = load_convs(cont_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str = [conv for data in convs for conv in data]\n",
    "X_utt = [utt for data in convs for conv in data for utt in conv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "def fake_error(convs):\n",
    "    utt_list = [conv[-1] for conv in convs]\n",
    "    # print(utt_list)\n",
    "    new_conv = copy.deepcopy(convs)\n",
    "    for conv in new_conv:\n",
    "        conv[-1] = random.choice(utt_list)\n",
    "    return new_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UT: 1208\n",
      "UT_NO: 10875\n"
     ]
    }
   ],
   "source": [
    "UT = fake_error( X_str[:int( len(X_str)*0.1) ] ) \n",
    "UT_NO = X_str[int( len(X_str)*0.1):]\n",
    "print(\"UT: {0}\".format(len(UT)))\n",
    "print(\"UT_NO: {0}\".format(len(UT_NO)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['こっからー、４０分で着けると思うけど。', 'やだやだ。いーわねー、ラブラブのところは。'],\n",
       " ['ほんと。そうなんだ。', '何か行くのはかまわないじゃんね、自分も勉強になるし。留学生の人としゃべるの楽しいから。'],\n",
       " ['社会科の人。わかんない。', '９０何点とかばっかり。']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_str[:3]\n",
    "UT[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM?\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True, bidirectional=True )\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        bilstm_out = torch.cat([hidden_layer[0][0], hidden_layer[0][1]], dim=1)\n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        y = self.hidden2tag(bilstm_out)\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(convs, max_len):\n",
    "    # emb_dim = nlp(\"形態素\").vector.shape\n",
    "    X_data = []\n",
    "    \n",
    "    for conv in convs :\n",
    "        # vec_list = np.zeros( (max_len, emb_dim[0]) )\n",
    "        sentence_vectors = Nmodel.encode(conv)\n",
    "        # for i, ut in enumerate(conv):\n",
    "        #     doc = nlp(ut)\n",
    "        #     vec_list[i] = doc.vector\n",
    "        X_data.append(sentence_vectors)\n",
    "    return np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic4-2.pickle\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../X_y_data/context/\"\n",
    "data_name = \"topic4-{0}.pickle\".format(length)\n",
    "dataM = DataManager(data_path)\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/context/topic4-2.pickle\n"
     ]
    }
   ],
   "source": [
    "if dataM.is_exist(data_name):\n",
    "    \n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X = DATA_Xy[0]\n",
    "    y = DATA_Xy[1]\n",
    "else:\n",
    "    # X_data, y_data = pre.extract_X_y(df, error_types, seq_len)\n",
    "    X = make_X(UT + UT_NO, length)\n",
    "    y = np.concatenate( [ np.ones(len(UT)), np.zeros(len(UT_NO)) ] )\n",
    "    dataM.save_data(data_name, [X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9666\n",
      "1, 2, 3, 6, 9, 18, 27, 54, 179, 358, 537, 1074, 1611, 3222, 4833, 9666, "
     ]
    }
   ],
   "source": [
    "# X_train = X_train[:-9]\n",
    "# y_train = y_train[:-9]\n",
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 179\n",
    "epoch_ = 300\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = emb_dim\n",
    "HIDDEN_DIM = emb_dim*4\n",
    "OUTPUT_DIM = 2\n",
    "seq_len = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 0.00040726442512095673\n",
      "epoch 100 \t loss 5.721480903275733e-05\n",
      "epoch 150 \t loss 1.1676504499291696e-05\n",
      "epoch 200 \t loss 2.7318235531481605e-06\n",
      "epoch 250 \t loss 6.613118164189302e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a7239650b6e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mall_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        # y_t_tensor = torch.tensor(data[1].reshape(batch_size, 1), device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAY8klEQVR4nO3dfZBc1X3m8e8z093zIiSQ0BiDhCS/UGxw1ijULMZZ1oVfQkDFmjjxrlG5bJJlS7HLbNm19m7wOmuz3vwRJ+XEhXFMybHW2OtgJxsTUxvZRiZOwAnYHigBAowla0UsIdCAhATobST99o97W2pG3dKou2d6dM7zqerq2+fe6XuOWvXMmd+9fa8iAjMzS1dfrztgZmbTy0FvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9JY1SVskvaPX/TCbTg56M7PEOejNJpE0IOlzkp4uH5+TNFCuWyjp/0p6QdJOSfdJ6ivX/Z6kbZJelPSkpLf3diRmhUqvO2A2C30CuAxYDgTwbeD3gf8OfBTYCoyU214GhKQLgRuBfxURT0taBvTPbLfNmvOM3ux47wU+HRE7ImIc+B/A+8p1E8C5wNKImIiI+6K4YNRhYAC4SFI1IrZExM970nuzSRz0Zsc7D3iq4fVTZRvAHwObgLslbZZ0E0BEbAI+AtwM7JD0DUnnYTYLOOjNjvc0sLTh9ZKyjYh4MSI+GhGvBd4J/Od6LT4i/iIiLi9/NoDPzGy3zZpz0JtBVdJg/QHcAfy+pBFJC4FPAv8bQNI1kl4vScBuipLNEUkXSnpbedB2P7APONKb4Zi9koPeDNZSBHP9MQiMAY8AjwIPAX9QbnsB8H3gJeB+4M8i4gcU9fk/BJ4DngFeBXx85oZg1pp84xEzs7R5Rm9mljgHvZlZ4hz0ZmaJc9CbmSVuVl4CYeHChbFs2bJed8PM7LTx4IMPPhcRI83WzcqgX7ZsGWNjY73uhpnZaUPSU63WuXRjZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiUsm6COCW+7ZyD/8bLzXXTEzm1WSCXpJrL53M3//5I5ed8XMbFY56TdjJa0BrgF2RMQvl23fBC4sNzkLeCEiljf52S3AixR34TkUEaNd6XUL8wYr7Nl3aDp3YWZ22pnKJRC+AtwKfLXeEBHvqS9L+izFLdVaeWtEPNduB0/FvKEqe/ZPzMSuzMxOGycN+oi4V9KyZuvK+2b+e+BtXe5XW+YNVdmzz0FvZtao0xr9vwGejYiNLdYHcLekByWt6nBfJzVvsMqe/S7dmJk16vTqlSuBO06w/vKI2CbpVcA6ST+NiHubbVj+IlgFsGTJkrY6M2+owhPbPaM3M2vU9oxeUgX4TeCbrbaJiG3l8w7gTuDSE2y7OiJGI2J0ZKTpJZVP6kyXbszMjtNJ6eYdwE8jYmuzlZLmSJpbXwauBDZ0sL+TmjdY5cUDhzh8JKZzN2Zmp5WTBr2kO4D7gQslbZV0Q7nqOiaVbSSdJ2lt+fIc4IeSHgZ+DPxtRHy3e10/3ryhKgAvuU5vZnbUVM66Wdmi/bebtD0NrCiXNwMXd9i/UzJvsBjOnv0TnDlcncldm5nNWsl8MxaKGj3AbtfpzcyOSiro66UbH5A1MzsmraAfLIPe3441MzsqraAfKmv0vt6NmdlRSQV9vUbvGb2Z2TFJBf2cWoU++WCsmVmjpIK+r0/MHfS3Y83MGiUV9FDU6X1hMzOzY5IL+jOHqi7dmJk1SC7oXzV3kGd27+91N8zMZo3kgn7RWUNse2Ffr7thZjZrJBf0i+cPsXvfBC/6FEszMyDBoF80fwjAs3ozs1J6QX9WEfRbdzrozcwgwaBfPH8Y8IzezKwuuaBfeEaNgUofW3ft7XVXzMxmheSCXhKL5vvMGzOzuuSCHoo6/dZdDnozM0g06M87c4jt/tKUmRkwtZuDr5G0Q9KGhrabJW2TtL58rGjxs1dJelLSJkk3dbPjJ3LGYIW9B3y9GzMzmNqM/ivAVU3a/zQilpePtZNXSuoHvgBcDVwErJR0USednao5tX72ThwmImZid2Zms9pJgz4i7gV2tvHelwKbImJzRBwEvgFc28b7nLKhWoUI2D9xZCZ2Z2Y2q3VSo79R0iNlaWd+k/WLgF80vN5atjUlaZWkMUlj4+PjHXQLhmv9AOw96PKNmVm7Qf9F4HXAcmA78NlOOxIRqyNiNCJGR0ZGOnqvoaNBf7jTbpmZnfbaCvqIeDYiDkfEEeBLFGWaybYB5ze8Xly2Tbs5teIm4Q56M7M2g17SuQ0v3wVsaLLZT4ALJL1GUg24Drirnf2dKpduzMyOqZxsA0l3AFcACyVtBT4FXCFpORDAFuB3y23PA/48IlZExCFJNwLfA/qBNRHx2HQMYrJ66WafZ/RmZicP+ohY2aT5yy22fRpY0fB6LXDcqZfTrV66edlBb2aW5jdjh1y6MTM7KsmgH3bpxszsqKSD3qUbM7Nkg76o0e9z6cbMLM2gr1X6qPTJ59GbmZFo0ENxQNZBb2aWcNDPqVV81o2ZGQkH/bBn9GZmQMJBP1Tr9+mVZmYkHPTDtX5edunGzCzloK94Rm9mRtJB7xq9mRkkHPQ+vdLMrJBs0Pv0SjOzQrJB79KNmVkh2aAfqvVz4NARDh+JXnfFzKynkg36Y/eNdfnGzPKWbNAP1q9JP+HyjZnlLdmgH6gUQzswcaTHPTEz662TBr2kNZJ2SNrQ0PbHkn4q6RFJd0o6q8XPbpH0qKT1ksa62O+TOhr0hxz0Zpa3qczovwJcNaltHfDLEfFG4GfAx0/w82+NiOURMdpeF9szWC1KNwcOuXRjZnk7adBHxL3Azkltd0dE/SjnA8DiaehbR+oz+v0u3ZhZ5rpRo/8PwHdarAvgbkkPSlp1ojeRtErSmKSx8fHxjjs1UPGM3swMOgx6SZ8ADgFfb7HJ5RFxCXA18CFJb2n1XhGxOiJGI2J0ZGSkk24BMFB1jd7MDDoIekm/DVwDvDcimn4rKSK2lc87gDuBS9vd36karM/oXboxs8y1FfSSrgL+K/DOiNjbYps5kubWl4ErgQ3Ntp0Ox2b0Lt2YWd6mcnrlHcD9wIWStkq6AbgVmAusK0+dvK3c9jxJa8sfPQf4oaSHgR8DfxsR352WUTTh8+jNzAqVk20QESubNH+5xbZPAyvK5c3AxR31rgM+GGtmVkj3m7E+GGtmBiQc9EcPxjrozSxzyQZ9tV9IcMAXNTOzzCUb9JIYqPSx3zN6M8tcskEPxQFZz+jNLHeJB32fa/Rmlr2kg36w2u+gN7PsJR30xYzepRszy1vaQV/t82WKzSx7aQd9pd8zejPLXuJB3+dr3ZhZ9pIOeh+MNTNLPOh9MNbMLIOg98FYM8td4kHvg7FmZmkHfdXfjDUzSzroB6v9PuvGzLKXdNDXD8a2uHe5mVkWphT0ktZI2iFpQ0PbAknrJG0sn+e3+Nnry202Srq+Wx2fioFKH0cCJg476M0sX1Od0X8FuGpS203APRFxAXBP+foVJC0APgW8CbgU+FSrXwjTwfeNNTObYtBHxL3AzknN1wK3l8u3A7/R5Ed/HVgXETsjYhewjuN/YUwb3zfWzKyzGv05EbG9XH4GOKfJNouAXzS83lq2HUfSKkljksbGx8c76NYxvm+smVmXDsZGcbSzo0J4RKyOiNGIGB0ZGelGt47N6H2XKTPLWCdB/6ykcwHK5x1NttkGnN/wenHZNiMGKsXw/O1YM8tZJ0F/F1A/i+Z64NtNtvkecKWk+eVB2CvLthnhg7FmZlM/vfIO4H7gQklbJd0A/CHwa5I2Au8oXyNpVNKfA0TETuB/Aj8pH58u22ZEvXTjGb2Z5awylY0iYmWLVW9vsu0Y8B8bXq8B1rTVuw7VSzcHDzvozSxfSX8zttpfDG/CZ92YWcaSDvqaZ/RmZokHfX1G76A3s4wlHfT10o2/MGVmOUs66I8ejHXQm1nGkg76qks3ZmZpB33NM3ozMwe9mVnqkg76Sp8Al27MLG9JB70kapU+DjjozSxjSQc9FOfSTxzyrQTNLF/pB32lj4OHffVKM8tX+kHf3+eDsWaWteSDvloRE4ddujGzfCUf9J7Rm1nu0g/6Sr+vXmlmWUs/6PvlGb2ZZS39oK+4dGNmeUs+6Kv9ff5mrJllre2gl3ShpPUNjz2SPjJpmysk7W7Y5pMd9/gUFefRO+jNLF9Tujl4MxHxJLAcQFI/sA24s8mm90XENe3up1M+68bMctet0s3bgZ9HxFNder+uqXpGb2aZ61bQXwfc0WLdmyU9LOk7kt7Q6g0krZI0JmlsfHy8S92CAc/ozSxzHQe9pBrwTuCvmqx+CFgaERcDnwf+ptX7RMTqiBiNiNGRkZFOu3WUD8aaWe66MaO/GngoIp6dvCIi9kTES+XyWqAqaWEX9jllPr3SzHLXjaBfSYuyjaRXS1K5fGm5v+e7sM8pc9CbWe7aPusGQNIc4NeA321o+wBARNwGvBv4oKRDwD7guoiY0SuMFaUbX9TMzPLVUdBHxMvA2ZPabmtYvhW4tZN9dKp+Hn1EUP5xYWaWleS/GVvrr9831rN6M8tT+kFfKYboc+nNLFfpB31/GfQ+IGtmmUo+6KvljN7n0ptZrpIPes/ozSx36Qe9a/Rmlrn0g94zejPLXPpBX3HQm1nekg/6ar8PxppZ3pIPes/ozSx32QT9Ac/ozSxT6Qd9vXTjGb2ZZSr9oPfplWaWueSD3gdjzSx3yQe9D8aaWe7SD3p/YcrMMpdN0B9w0JtZppIP+sGag97M8pZ80Nf6++gT7Dt4uNddMTPriY6DXtIWSY9KWi9prMl6SbpF0iZJj0i6pNN9nmL/GK5V2DfhoDezPHV0c/AGb42I51qsuxq4oHy8Cfhi+TxjBqv97PWM3swyNROlm2uBr0bhAeAsSefOwH6PGqr1sd8zejPLVDeCPoC7JT0oaVWT9YuAXzS83lq2vYKkVZLGJI2Nj493oVvHDFX7XaM3s2x1I+gvj4hLKEo0H5L0lnbeJCJWR8RoRIyOjIx0oVvHDFX7XaM3s2x1HPQRsa183gHcCVw6aZNtwPkNrxeXbTNmqOYZvZnlq6OglzRH0tz6MnAlsGHSZncB7y/PvrkM2B0R2zvZ76nyjN7MctbpWTfnAHdKqr/XX0TEdyV9ACAibgPWAiuATcBe4Hc63OcpG6o56M0sXx0FfURsBi5u0n5bw3IAH+pkP50a9MFYM8tY8t+MhaJ049MrzSxXWQT9cM1fmDKzfGUR9PWDsUUVycwsL1kE/WCtH/AVLM0sT1kE/VC1CHofkDWzHGUR9MPljN6nWJpZjrII+sFyRu8DsmaWoyyCvl668SmWZpajPILepRszy1geQe+DsWaWsTyCvuYavZnlK4+gd43ezDKWR9C7Rm9mGcsj6F2jN7OMZRH09fPoPaM3sxxlEfQDlT765Bm9meUpi6CX5NsJmlm2sgh68O0EzSxfbQe9pPMl/UDS45Iek/ThJttcIWm3pPXl45Oddbd9cwYqvLT/UK92b2bWM53cM/YQ8NGIeEjSXOBBSesi4vFJ290XEdd0sJ+uOGu4xq69B3vdDTOzGdf2jD4itkfEQ+Xyi8ATwKJudazbFgxXHfRmlqWu1OglLQN+BfhRk9VvlvSwpO9IesMJ3mOVpDFJY+Pj493o1ivMH66x6+WJrr+vmdls13HQSzoD+GvgIxGxZ9Lqh4ClEXEx8Hngb1q9T0SsjojRiBgdGRnptFvHmT/HpRszy1NHQS+pShHyX4+Ib01eHxF7IuKlcnktUJW0sJN9tmv+cJW9Bw/7ejdmlp1OzroR8GXgiYj4kxbbvLrcDkmXlvt7vt19dmL+nBoAL+x1+cbM8tLJWTf/Gngf8Kik9WXbfwOWAETEbcC7gQ9KOgTsA66LiOhgn22bP1wE/a69B3n1mYO96IKZWU+0HfQR8UNAJ9nmVuDWdvfRTUeD/mXX6c0sL9l8M3b+nCoAu1y6MbPMZBP0C8oZ/U6feWNmmckm6M8qg/4Fl27MLDPZBH2t0scZAxXP6M0sO9kEPRR1ep9eaWa5ySvoh2vsdOnGzDLjoDczS1xWQb9o/hBbd+3tdTfMzGZUVkG/ZMEwu/ZOsGe/6/Rmlo+sgn7pgmEA/vl5z+rNLB9ZBf2Ss8ug3+mgN7N8ZBX0S8+eA8BTntGbWUayCvozBiqcPafGP+98udddMTObMVkFPRTlG8/ozSwn2QX90gUOejPLS3ZB/0vnzmPbC/u4b2P3b0BuZjYbZRf01//qMl43MoeP/dXDvHTgUK+7Y2Y27bIL+sFqP5/5rTfy7J4DfOuhrb3ujpnZtMsu6AFGly3gjYvP5Gv3P0WPbmFrZjZjOgp6SVdJelLSJkk3NVk/IOmb5fofSVrWyf666X2XLWXjjpf4/hM7et0VM7Np1XbQS+oHvgBcDVwErJR00aTNbgB2RcTrgT8FPtPu/rrt3158Hv/i1XP56F+u54HNz3PkiGf2ZpamSgc/eymwKSI2A0j6BnAt8HjDNtcCN5fL/we4VZJiFtRLBqv9fOn9o7zrz/6R61Y/ABR3oRooH5JQua0EQuVzvU3H1pWNjdscXT9zQ5q6Wdmp2dmt+uc428zOXlmn5g/X+MsPvLnr79tJ0C8CftHweivwplbbRMQhSbuBs4HnJr+ZpFXAKoAlS5Z00K2pO3/BMD/42BWse/xZnnp+LwcOHWH/xGEOHDoCFL+LIsoHQf3XU3CsjVe0xdF1HH2H2WUW/I5talb2alZ2qvx/Z0maN1idlvftJOi7KiJWA6sBRkdHZ+x/8tzBKr95yeKZ2p2Z2Yzr5GDsNuD8hteLy7am20iqAGcCz3ewTzMzO0WdBP1PgAskvUZSDbgOuGvSNncB15fL7wb+bjbU583MctJ26aasud8IfA/oB9ZExGOSPg2MRcRdwJeBr0naBOyk+GVgZmYzqKMafUSsBdZOavtkw/J+4N91sg8zM+tMlt+MNTPLiYPezCxxDnozs8Q56M3MEqfZeLajpHHgqTZ+dCFNvnWbKI81TR5rmmZirEsjYqTZilkZ9O2SNBYRo73ux0zwWNPksaap12N16cbMLHEOejOzxKUW9Kt73YEZ5LGmyWNNU0/HmlSN3szMjpfajN7MzCZx0JuZJS6JoD/ZTcpPd5K2SHpU0npJY2XbAknrJG0sn+f3up/tkLRG0g5JGxramo5NhVvKz/kRSZf0ruenrsVYb5a0rfxs10ta0bDu4+VYn5T0673pdXsknS/pB5Iel/SYpA+X7cl9ticY6+z5bCPitH5QXCL558BrgRrwMHBRr/vV5TFuARZOavsj4KZy+SbgM73uZ5tjewtwCbDhZGMDVgDfobhl6mXAj3rd/y6M9WbgY022vaj8vzwAvKb8P97f6zGcwljPBS4pl+cCPyvHlNxne4KxzprPNoUZ/dGblEfEQaB+k/LUXQvcXi7fDvxG77rSvoi4l+JeBY1aje1a4KtReAA4S9K5M9LRLmgx1lauBb4REQci4v8Bmyj+r58WImJ7RDxULr8IPEFxD+nkPtsTjLWVGf9sUwj6ZjcpP9E/8ukogLslPVjeRB3gnIjYXi4/A5zTm65Ni1ZjS/WzvrEsV6xpKMElM1ZJy4BfAX5E4p/tpLHCLPlsUwj6HFweEZcAVwMfkvSWxpVR/D2Y5HmyKY+t9EXgdcByYDvw2Z72pssknQH8NfCRiNjTuC61z7bJWGfNZ5tC0E/lJuWntYjYVj7vAO6k+DPv2fqftuXzjt71sOtajS25zzoino2IwxFxBPgSx/6EP+3HKqlKEXxfj4hvlc1JfrbNxjqbPtsUgn4qNyk/bUmaI2lufRm4EtjAK2+8fj3w7d70cFq0GttdwPvLMzQuA3Y3lAFOS5Pq0O+i+GyhGOt1kgYkvQa4APjxTPevXZJEcc/oJyLiTxpWJffZthrrrPpse33EuktHvVdQHOn+OfCJXveny2N7LcUR+oeBx+rjA84G7gE2At8HFvS6r22O7w6KP2snKGqVN7QaG8UZGV8oP+dHgdFe978LY/1aOZZHKALg3IbtP1GO9Ung6l73/xTHejlFWeYRYH35WJHiZ3uCsc6az9aXQDAzS1wKpRszMzsBB72ZWeIc9GZmiXPQm5klzkFvZpY4B71lSdLhhqsKru/mVU8lLWu8QqVZr1V63QGzHtkXEct73QmzmeAZvVmD8tr/f1Re///Hkl5fti+T9HflBarukbSkbD9H0p2SHi4fv1q+Vb+kL5XXJ79b0lDPBmXZc9BbroYmlW7e07Bud0T8S+BW4HNl2+eB2yPijcDXgVvK9luAf4iIiymuNf9Y2X4B8IWIeAPwAvBb0zoasxPwN2MtS5JeiogzmrRvAd4WEZvLC1U9ExFnS3qO4ivsE2X79ohYKGkcWBwRBxreYxmwLiIuKF//HlCNiD+YgaGZHcczerPjRYvlU3GgYfkwPh5mPeSgNzveexqe7y+X/4niyqgA7wXuK5fvAT4IIKlf0pkz1UmzqfIsw3I1JGl9w+vvRkT9FMv5kh6hmJWvLNv+E/C/JP0XYBz4nbL9w8BqSTdQzNw/SHGFSrNZwzV6swZljX40Ip7rdV/MusWlGzOzxHlGb2aWOM/ozcwS56A3M0ucg97MLHEOejOzxDnozcwS9/8B8PCiSxr69NUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9077368638808441"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic5-2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/context/\"\n",
    "model_name = \"topic5-{0}.pickle\".format(length)\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/context/topic5-2.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
