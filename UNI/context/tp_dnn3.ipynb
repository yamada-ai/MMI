{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from datatools.analyzer import *\n",
    "from utterance.error_tools import *\n",
    "\n",
    "from datatools.maneger import DataManager\n",
    "from datatools.preproc import Preprocessor\n",
    "\n",
    "sys.path.append(\"../response/\")\n",
    "from feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "emb_dim = nlp(\"形態素\").vector.shape[0]\n",
    "emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_convs(path, filename):\n",
    "    with open(path+filename, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "        # length = json_data[\"length\"]\n",
    "        convs = json_data[\"convs\"]\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_dir = \"continue/\"\n",
    "cont_path = \"../../corpus/nucc/\" + cont_dir\n",
    "length = 3\n",
    "filename = \"cont{0}.json\".format(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = load_convs(cont_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str = [conv for data in convs for conv in data]\n",
    "X_utt = [utt for data in convs for conv in data for utt in conv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "def fake_error(convs):\n",
    "    utt_list = [conv[-1] for conv in convs]\n",
    "    # print(utt_list)\n",
    "    new_conv = copy.deepcopy(convs)\n",
    "    for conv in new_conv:\n",
    "        conv[-1] = random.choice(utt_list)\n",
    "    return new_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UT: 575\n",
      "UT_NO: 5177\n"
     ]
    }
   ],
   "source": [
    "UT = fake_error( X_str[:int( len(X_str)*0.1) ] ) \n",
    "UT_NO = X_str[int( len(X_str)*0.1):]\n",
    "print(\"UT: {0}\".format(len(UT)))\n",
    "print(\"UT_NO: {0}\".format(len(UT_NO)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load : ../X_y_data/response/forback1.pickle\n",
      "300\n",
      "success load : ../models/response/forback_clf.pickle\n"
     ]
    }
   ],
   "source": [
    "F1_path = \"../X_y_data/response/\"\n",
    "F1_name = \"forback1.pickle\"\n",
    "featureM1 = DataManager(F1_path)\n",
    "\n",
    "F_fb = featureM1.load_data(F1_name)\n",
    "F_fb.set_preprocessor(Preprocessor())\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model1_path = \"../models/response/\"\n",
    "model1_name = \"forback_clf.pickle\"\n",
    "modelM1 = DataManager(model1_path)\n",
    "\n",
    "clf_fb = modelM1.load_data(model1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sets = set(\"名詞 代名詞 動詞 形容詞 接続詞 連体詞\".split() )\n",
    "def utt2vecter_s(utt):\n",
    "    vector = np.zeros(300)\n",
    "    remains = 0\n",
    "    for token in nlp(utt):\n",
    "        tag = token.tag_.split(\"-\")[0]\n",
    "        if tag in pos_sets:\n",
    "            vector += token.vector\n",
    "            remains += 1\n",
    "    \n",
    "    if remains < 2:\n",
    "        return vector\n",
    "    else:\n",
    "        return vector/remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def make_X_ginza(convs, max_len):\n",
    "    emb_dim = nlp(\"形態素\").vector.shape[0]\n",
    "    X_data = []\n",
    "    \n",
    "    for conv in tqdm(convs) :\n",
    "        # vec_list = np.zeros( (max_len, 2*emb_dim+2) )\n",
    "        vec_list = []\n",
    "        # sentence_vectors = Nmodel.encode(conv)\n",
    "        \n",
    "        for i, ut in enumerate(conv):\n",
    "            vector = utt2vecter_s(ut)\n",
    "            f = F_fb.featurization(ut)\n",
    "            fb = clf_fb.predict_proba(f.reshape(1, -1))[0]\n",
    "            if i==0:\n",
    "                # print( vector.shape, np.zeros(emb_dim).shape, fb )\n",
    "                vec_list.append(np.concatenate([vector, np.zeros(emb_dim), fb])  )\n",
    "            else:\n",
    "                vec_list.append(np.concatenate([vector, prev_vector-vector, fb])  )\n",
    "            prev_vector = vector\n",
    "        X_data.append(vec_list)\n",
    "\n",
    "    return np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic5-3.pickle\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../X_y_data/context/\"\n",
    "data_name = \"topic5-{0}.pickle\".format(length)\n",
    "dataM = DataManager(data_path)\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5752/5752 [38:31<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../X_y_data/context/topic5-3.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if dataM.is_exist(data_name):\n",
    "    \n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X = DATA_Xy[0]\n",
    "    y = DATA_Xy[1]\n",
    "else:\n",
    "    # X_data, y_data = pre.extract_X_y(df, error_types, seq_len)\n",
    "    X = make_X_ginza(UT + UT_NO, length)\n",
    "    y = np.concatenate( [ np.ones(len(UT)), np.zeros(len(UT_NO)) ] )\n",
    "    dataM.save_data(data_name, [X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n",
      "1, 2, 4, 5, 8, 10, 20, 23, 25, 40, 46, 50, 92, 100, 115, 184, 200, 230, 460, 575, 920, 1150, 2300, 4600, "
     ]
    }
   ],
   "source": [
    "X_train = X_train[:-1]\n",
    "y_train = y_train[:-1]\n",
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 230\n",
    "epoch_ = 600\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5752, 3, 602)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM?\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True, bidirectional=True )\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        bilstm_out = torch.cat([hidden_layer[0][0], hidden_layer[0][1]], dim=1)\n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        y = self.hidden2tag(bilstm_out)\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602 1800 2\n"
     ]
    }
   ],
   "source": [
    "# EMBEDDING_DIM = emb_dim\n",
    "EMBEDDING_DIM = 2*300+2\n",
    "HIDDEN_DIM = 300*6\n",
    "OUTPUT_DIM = 2\n",
    "seq_len = length\n",
    "print(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 \t loss 0.006295343657257035\n",
      "epoch 100 \t loss 0.0004964827521689585\n",
      "epoch 150 \t loss 0.00016983296836770023\n",
      "epoch 200 \t loss 7.640070464276505e-05\n",
      "epoch 250 \t loss 3.859445570242315e-05\n",
      "epoch 300 \t loss 2.0754469886696825e-05\n",
      "epoch 350 \t loss 1.1590658189675196e-05\n",
      "epoch 400 \t loss 6.549736795591343e-06\n",
      "epoch 450 \t loss 3.7716667264930948e-06\n",
      "epoch 500 \t loss 2.2095143847877807e-06\n",
      "epoch 550 \t loss 1.3050812306403259e-06\n",
      "epoch 600 \t loss 7.670854422769935e-07\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        # y_t_tensor = torch.tensor(data[1].reshape(batch_size, 1), device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8705473501303215"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic5-3.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/context/\"\n",
    "model_name = \"topic5-{0}.pickle\".format(length)\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success save : ../models/context/topic5-3.pickle\n"
     ]
    }
   ],
   "source": [
    "modelM.save_data(model_name, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
