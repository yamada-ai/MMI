{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_num_participants_file(path, num):\n",
    "    filenames = os.listdir(path)\n",
    "    num_files = []\n",
    "    for file_ in filenames:\n",
    "        participants = 0\n",
    "        if \".\" not in file_:\n",
    "            continue\n",
    "        with open(path+file_, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                if \"参加者の関係\" in line:\n",
    "                    if participants == num:\n",
    "                        num_files.append(file_)\n",
    "                        break\n",
    "                if \"参加者\" in line:\n",
    "                    participants += 1\n",
    "    num_files.sort() \n",
    "    return num_files"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def txt2json(path, file_, participants):\n",
    "\n",
    "    # for file_ in plist:\n",
    "    print(file_)\n",
    "    file_n = file_.split(\".\")[0]\n",
    "    conv = {\n",
    "            \"data\":file_n,\n",
    "            \"turns\":[]\n",
    "    }\n",
    "    users = {}\n",
    "    with open(path+file_, \"r\") as f:\n",
    "        conv_data_all = f.readlines()\n",
    "        for line in conv_data_all:\n",
    "            if \"参加者の関係\" in line:\n",
    "                break\n",
    "            if \"参加者\" in line:\n",
    "                    user = line.split(\"：\")[0][4:]\n",
    "                    users[user] = \"user{0}\".format(len(users))\n",
    "        # ここから互いの発話\n",
    "        # 変なやつが紛れてる場合がある(data007)\n",
    "        i = 0\n",
    "        while True:\n",
    "            conv_data = conv_data_all[4+participants+i:]\n",
    "                \n",
    "            line = conv_data[0]\n",
    "                \n",
    "            user_name = line.split(\"：\")[0]\n",
    "            # print(user_name)\n",
    "            \n",
    "            if user_name not in users.keys():\n",
    "                print(\"init key error\", user_name)\n",
    "                i += 1\n",
    "            else:\n",
    "                user = users[user_name]\n",
    "                utterrance = line.split(\"：\")[1].strip()\n",
    "                break\n",
    "            \n",
    "        for line in conv_data[1:]:\n",
    "            # 話者確定\n",
    "            if \"：\" in line:\n",
    "                splited = line.split(\"：\")\n",
    "                user_name = splited[0]\n",
    "                # 変なヤツ引いたら一旦飛ばす\n",
    "                if user_name not in users.keys():\n",
    "                    print(\"key error\", user_name)\n",
    "                    continue\n",
    "\n",
    "                # これまでのデータを保存\n",
    "                turn_data = {\n",
    "                    \"user\" : user,\n",
    "                    \"utterance\" : clean_utterance(utterrance)\n",
    "                }\n",
    "                conv[\"turns\"].append(turn_data)\n",
    "\n",
    "                    # 話者情報初期化\n",
    "                \n",
    "                utterrance = splited[1].strip()\n",
    "                \n",
    "                user = users[user_name]\n",
    "                # 改行で連続発話\n",
    "            else:\n",
    "                utterrance += line.strip()\n",
    "    \n",
    "    return conv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 発話を整形したい\n",
    "nlp =  spacy.load('ja_ginza')\n",
    "def clean_utterance(utt):\n",
    "    doc = nlp(utt)\n",
    "    valid_utt = \"\"\n",
    "    is_continue_parren = False\n",
    "    is_continue_moved = False\n",
    "    for token in doc:\n",
    "        if \"＜\" in token.orth_ or \"（\" in token.orth_:\n",
    "            is_continue_parren = True\n",
    "            continue\n",
    "        \n",
    "        if \"＞\" in token.orth_ or \"）\" in token.orth_:\n",
    "            is_continue_parren = False\n",
    "            continue\n",
    "        \n",
    "        # 括弧が優先\n",
    "        if is_continue_parren:\n",
    "            continue\n",
    "\n",
    "        if \"感動詞\" in token.tag_:\n",
    "            is_continue_moved = True\n",
    "            continue\n",
    "\n",
    "        if is_continue_moved and \"補助記号-読点\" in token.tag_:\n",
    "            is_continue_moved = False\n",
    "            continue\n",
    "            \n",
    "        if is_continue_moved:\n",
    "            continue\n",
    "        \n",
    "        valid_utt += token.orth_\n",
    "    return valid_utt\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "nucc_path = \"../../corpus/nucc/law/\"\n",
    "participants = 2\n",
    "num_files = get_num_participants_file(nucc_path, participants)\n",
    "filepath = \"conv{0}/\".format(participants)\n",
    "output = \"../../corpus/nucc/\" + filepath"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# clean_utterance(\"あー、半強制やろ（笑い）＜名詞＞。行きたくない、もー。\")\n",
    "doc = nlp(\"あー、半強制やろ（笑）＜名詞＞。行きたくない、もー。\")\n",
    "for token in doc:\n",
    "    print(token.orth_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "あー\n",
      "、\n",
      "半\n",
      "強制\n",
      "やろ\n",
      "(\n",
      "笑\n",
      ")\n",
      "＜\n",
      "名詞\n",
      "＞\n",
      "。\n",
      "行き\n",
      "たく\n",
      "ない\n",
      "、\n",
      "もー\n",
      "。\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "completed = len( os.listdir(output) )\n",
    "print(\"completed:\", completed)\n",
    "for file_ in num_files[completed:]:\n",
    "    # print(file_)\n",
    "    conv = txt2json(nucc_path, file_, participants)\n",
    "    file_n = file_.split(\".\")[0]\n",
    "    with open(output+\"{0}.json\".format(file_n), \"w\") as f:\n",
    "        json.dump(conv, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completed: 91\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "nlp =  spacy.load('ja_ginza')\n",
    "emb_dim = nlp(\"形態素\").vector.shape[0]\n",
    "\n",
    "def _in(arg1, arg2):\n",
    "    result = False\n",
    "    if isinstance(arg1, list):\n",
    "        for element in arg1:\n",
    "            if isinstance(element, str):\n",
    "                if element in arg2:\n",
    "                    result = True\n",
    "                    break\n",
    "            else:\n",
    "                if element == arg2:\n",
    "                    result = True\n",
    "                    break\n",
    "    else:\n",
    "        if isinstance(arg1, str):\n",
    "            if arg1 in arg2:\n",
    "                result = True\n",
    "        else:\n",
    "            if arg1 == arg2:\n",
    "                result = True\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_pos_vector(pos_list, text):\n",
    "    vec = np.zeros( (emb_dim) )\n",
    "    doc = nlp(text, disable=['ner'])\n",
    "    for token in doc:\n",
    "        # print(token, token.tag_)\n",
    "        if \"助動詞\" not in token.tag_ and _in(pos_list, token.tag_):\n",
    "            # print(token, token.tag_)\n",
    "            vec += token.vector\n",
    "    return vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 5文連続で有効そうな対話を取得\n",
    "\n",
    "def extract_continue_convs(pos_list, conv, length):\n",
    "    new_convs = []\n",
    "    continue_conv = []\n",
    "    for turn in conv:\n",
    "        utterance = turn[\"utterance\"]\n",
    "        # print(utterance)\n",
    "        vecter = filter_pos_vector(pos_list, utterance)\n",
    "        # 有効そうな発話だったら\n",
    "        mo_len = len(nlp(utterance)) \n",
    "        if mo_len > 5 and mo_len <= 30 and np.linalg.norm(vecter) > 0:\n",
    "            continue_conv.append(utterance)\n",
    "        else:\n",
    "            continue_conv  = []\n",
    "        \n",
    "        if len(continue_conv) >= length:\n",
    "            new_convs.append( continue_conv[-length:] )\n",
    "            # print(continue_conv[-length:])\n",
    "    \n",
    "    return new_convs\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pos_list = \"名詞 代名詞 動詞 形容詞 副詞 接続詞 感動詞 連体詞\".split()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def load_json(path, file_):\n",
    "    with open(path+file_, \"r\") as f:\n",
    "        conv = json.load(f)\n",
    "    return conv\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "files = os.listdir(output)\n",
    "files.sort()\n",
    "\n",
    "length = 2\n",
    "conv_json = {\n",
    "    \"length\" : length,\n",
    "    \"convs\" : []\n",
    "}\n",
    "from tqdm import tqdm\n",
    "for file_ in tqdm( files ) :\n",
    "    conv = load_json(output, file_)\n",
    "\n",
    "    new_conv = extract_continue_convs(pos_list, conv[\"turns\"],  length)\n",
    "    conv_json[\"convs\"].append(new_conv)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 91/91 [09:07<00:00,  6.02s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "cont_dir = \"continueV2/\"\n",
    "cont_path = \"../../corpus/nucc/\" + cont_dir\n",
    "cont_path"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../../corpus/nucc/continue/'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "with open(cont_path+\"cont{0}.json\".format(length), \"w\") as f:\n",
    "    json.dump(conv_json, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}