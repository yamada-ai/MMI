{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from datatools.analyzer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(model_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Repetition\"\n",
    "import csv\n",
    "import Levenshtein\n",
    "import random\n",
    "def make_X_y_csv(filename=\"repetition.csv\"):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    y = []\n",
    "    all_data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_ = csv.reader(f)\n",
    "        for d in data_:\n",
    "          all_data.append(d)\n",
    "    \n",
    "    for d in all_data:\n",
    "        y.append(int(d[0]))\n",
    "        X1.append(d[1])\n",
    "        X2.append(d[2])\n",
    "\n",
    "    # u1_l = [d[1]  for d in all_data]\n",
    "    # u2_l = [d[2]  for d in all_data]\n",
    "    X1_vec = Nmodel.encode(X1)\n",
    "    X2_vec = Nmodel.encode(X2)\n",
    "    X = [ x1-x2 for x1, x2 in zip(X1_vec, X2_vec) ]\n",
    "    for x1, x2 in zip( random.choices(X1_vec, k=len(X1)), random.choices(X2_vec, k=len(X2)) ):\n",
    "        X.append(x1-x2)\n",
    "        y.append(0)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' わあ！いいですね！', ' 映画が楽しみです', ' 海は素晴らしいですね', ' 今は、してないですね。', ' 食べるのがいいですね', ' 歯ごたえが抜群にいいですね', ' スイカは大好きですね', ' メロンは好きですね', ' 泳ぐを繰り返すのです', ' 泳ぐを覚えるのです', ' 泳ぐを繰り返すようですよ', ' 泳ぐを覚えますよねー', ' 猛暑は欲しいですよねー', ' 冷夏が多いです', ' 元気ですかは元気か', ' 熱中症で死者が出てるんでしょうか', ' 実写映画を含めるのです', ' 実写映画を問わないなあ', ' ありがとうございます', ' うんどういたしまして', ' ゴーヤチャンプルは美味しいですね', ' 予防を怠ります', ' 普段から予防を行いますよねー', ' 海は楽しいですね', ' 湖が綺麗ですね', ' 果物が良いですね', ' ありがとう', ' スイカは大好きですね', ' 趣味は写真を撮るのです', ' 機嫌をなおすのです', ' 嫌はいいのが救いですけどねぇ', ' 元気ですかは元気ですね', ' ありがとうございます', ' 代替わりしていますね', ' あはは本当ですね。（←大阪の友達に怒られそう…）東京のもんじゃも似てますけどね。', ' こんにちは。', ' 好きな食べ物は？', ' そうですねえ', ' ペットは飼ってませんが動物は好きです。', ' 最近だと旅行にはまりだしたので、旅代で消えてしまいますね。', ' ヨガちょっとやってみたいです！やっぱりやり始めてからは体の調子いいですか？お風呂上がりにストレッチやろうと思うのですが、なかなか続かなくて。', ' 猫は好きですか。', ' いま住んでるところから駅がちょっと遠いので駅までは車ですよ。', \" 慣れてる方ですね'\", ' 当たらないだろうと思って出していません。', ' 地下鉄で。名大前ができてすごく便利になりましたね、ここの大学。']\n"
     ]
    }
   ],
   "source": [
    "X, y = make_X_y_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=10000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[11  1]\n",
      " [ 0 16]]\n",
      "accuracy =  0.9642857142857143\n",
      "precision =  0.9411764705882353\n",
      "recall =  1.0\n",
      "f1 score =  0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [10:40<00:00,  3.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "y_pred = []\n",
    "for conv in tqdm(convs):\n",
    "    ngram_sets = []\n",
    "    history = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        y_pred.append(0)\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) <= 3:\n",
    "                # print(sent)\n",
    "                continue\n",
    "            vec = Nmodel.encode(sent.text)[0]\n",
    "            for prev in history:\n",
    "                if clf.predict(vec.reshape(1, -1)) == 1:\n",
    "                    y_pred[-1] = 1\n",
    "            history.append(vec)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Repetition\"\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        # \n",
    "        if ut.is_error_included(error):\n",
    "            # print(ut.errors)\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1788  364]\n",
      " [  38   10]]\n",
      "accuracy =  0.8172727272727273\n",
      "F-measure:  0.04739336492890995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print('F-measure: ', metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
