{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True, bidirectional=True )\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        bilstm_out = torch.cat([hidden_layer[0][0], hidden_layer[0][1]], dim=1)\n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        y = self.hidden2tag(bilstm_out)\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "Nmodel_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(Nmodel_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(convs, max_len):\n",
    "    # emb_dim = nlp(\"形態素\").vector.shape\n",
    "    X_data = []\n",
    "    \n",
    "    for conv in convs :\n",
    "        # vec_list = np.zeros( (max_len, emb_dim[0]) )\n",
    "        sentence_vectors = Nmodel.encode(conv)\n",
    "        # for i, ut in enumerate(conv):\n",
    "        #     doc = nlp(ut)\n",
    "        #     vec_list[i] = doc.vector\n",
    "        X_data.append(sentence_vectors)\n",
    "    return np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "\n",
    "output = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_with_NoErr(path:str, datalist:list) -> pd.DataFrame:\n",
    "    cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "    df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "    for p in datalist:\n",
    "        datapath = Path(path + p + '/')\n",
    "        for file in datapath.glob(\"*.json\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                did = json_data[\"did\"]\n",
    "                for t in json_data[\"turns\"]:\n",
    "                    if t[\"turn-index\"] == 0:\n",
    "                        continue\n",
    "                    if t[\"speaker\"] == \"U\":\n",
    "                        usr = t[\"utterance\"]\n",
    "                        continue\n",
    "                    if t[\"speaker\"] == \"S\" :\n",
    "                        tid = t[\"turn-index\"]\n",
    "                        sys = t[\"utterance\"]\n",
    "                        if t[\"error_category\"]:\n",
    "                            ec = t[\"error_category\"]\n",
    "                        else:\n",
    "                            ec = [\"No-Err\"]\n",
    "                        df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_json_with_NoErr(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_continue_convs_with_error(df, length, errors):\n",
    "    new_convs = []\n",
    "    continue_conv = []\n",
    "    did = 0\n",
    "    for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "        # did が変化すれば，別の対話\n",
    "        if d != did:\n",
    "            continue_conv = []\n",
    "            did = d\n",
    "        continue_conv .append(u)\n",
    "        continue_conv .append(s)\n",
    "        for err in errors:\n",
    "            if len(continue_conv) >= length and err in e:\n",
    "                new_convs.append( continue_conv[-length:] )\n",
    "    \n",
    "    return new_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "# errors = [\"Lack of information\"]\n",
    "errors = [\"Topic transition error\"]\n",
    "# errors = [\"Unclear intention\"]\n",
    "length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic4-3.pickle\n",
      "success load : ../models/context/topic4-3.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/context/\"\n",
    "model_name = \"topic4-{0}.pickle\".format(length)\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)\n",
    "if modelM.is_exist(model_name):\n",
    "    model = modelM.load_data(model_name)\n",
    "    model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real test\n",
    "leng_c = 2\n",
    "def make_X_str_y(df, errors):\n",
    "    X_str = []\n",
    "    y = []\n",
    "    y = np.zeros(len(df))\n",
    "    continue_conv = []\n",
    "    did = 0\n",
    "    for i, (d, u, s, e) in enumerate(zip(df.did, df.usr, df.sys, df.ec)):\n",
    "        # sentence_vectors = Nmodel.encode([u, s])\n",
    "        if d != did:\n",
    "            continue_conv = [\"\"]*leng_c\n",
    "            did = d\n",
    "        continue_conv.append(u)\n",
    "        continue_conv.append(s)\n",
    "        X_str.append( continue_conv[-leng_c:] )\n",
    "        for err in errors:\n",
    "            if err in e:\n",
    "                y[i] = 1\n",
    "\n",
    "    return X_str, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str, y = make_X_str_y(df, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_X(X_str, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     X_tensor = torch.tensor(X, device='cuda:0').float()\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    # y_tensor = torch.tensor(y, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "#     y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)\n",
    "    y_pred= np.array(model(X_tensor)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1675  133]\n",
      " [ 176   16]]\n",
      "accuracy =  0.8455\n",
      "F-measure:  0.093841642228739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print('F-measure: ', metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, np.zeros(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['そうですか', 'あはい']\n",
      "['こんにちは。お名前は？', 'お名前はco.jpに添えます']\n",
      "['どこにいったの？', '旅行を検討しちゃった']\n",
      "['一緒に遊ぼう！', 'しりとりでもしましょうか']\n",
      "['海外旅行されたことありますか？', '最近興味のあることは何ですか？']\n",
      "['こんにちは', 'こんにちは']\n",
      "['海外旅行の経験はありますか？', '海外旅行も楽しいかな。']\n",
      "['興味があるスポーツは何ですか？', '今は、してないですね。']\n",
      "['学生時代に何かスポーツしていましたか？', '今は、してないですね。']\n",
      "['こんにちは！元気ですか？', '元気ですかは元気です']\n",
      "['そうですね。', 'うん']\n",
      "['インターネットはいまではだれでも使っていますよね。', '日本の国花は桜だけど、法律で決められたものじゃないんだよ。']\n",
      "['こんばんは', 'こんばんわ']\n",
      "['そうですね', 'うむ']\n",
      "['お笑いの番組はよく見るよ。', 'お笑いはくだらないですね']\n",
      "['嬉しいですね', 'なでなで']\n",
      "['物知りなんですね', 'そのとおり']\n",
      "['こんにちは。', 'こんにちはー']\n",
      "['こんにちはー', 'こんにちはですー']\n",
      "['それはゲイということですか？', 'お金がかかるのです']\n",
      "['だれがですか？', 'お金にルーズなようですよ']\n",
      "['だれがルーズなんですか？', '銀行からお金を借りるらしいですよ']\n",
      "['競泳で金メダルでましたね', '金メダルは無理です']\n",
      "['こんにちは。', 'こんにちは']\n",
      "['そうですね。', 'うむ']\n",
      "['次はいつ登山に行く予定なんですか？', 'インターネットで調べたところ、一位は、GWです。']\n",
      "['あら、教えてもらえなくて残念です。', 'インターネットで調べたところ、一位は、レンズ豆です。']\n",
      "['むしろ「じょうおう」って読み方を知らなかったよ。', '針のむしろのむしろが藁や竹で編んだマットのようなものと知っている人少ないですね。']\n",
      "['そうだね。エントリーっていうのがちょっとよくわからないけど。', '防水はいいそうです']\n",
      "['仲良しな感じがするね。', '仲良しの人同士なら、すぐに時間がすぎちゃってびっくりするよね。']\n",
      "['こんにちは', 'こんにちは']\n",
      "['こんにちは。今日の予定は何ですか？', '「こんにちは」をWikipediaで検索した結果は、「久しぶりこんにちは']\n",
      "['おはよう', 'おはおはーっ']\n",
      "['犬に仕込んだんですか？', '犬は実家で飼います']\n",
      "['こんにちは', 'ああこんにちは']\n",
      "['そうですか？', 'そうだよ']\n",
      "['じゃあ、用意して待ってます。', '車で買い物に行くのです']\n",
      "['運転が疲れた時は車を止めて少し休みましょう。', '通り道をふさぎます']\n",
      "['よくわからないです。', 'ごめんなさい、話を変えましょう。買い物することが多いって言ってましたよね。よく行くお店があるんですか？']\n",
      "['今はインターネットでの買い物が多いです。', '情報が多いですね']\n",
      "['国際紛争のニュースが一番気になります。', '国際紛争が起きてますか？']\n",
      "['こんにちは', 'こんにちは?']\n",
      "['わかります', 'わかりますか']\n",
      "['こんにちは', 'おはよ']\n",
      "['そうですか', 'ごめんね']\n",
      "['どういたしまして', 'はい']\n",
      "['私は苦手だなあ', '真夏日はスイカがうまいですね']\n",
      "['外国に慣れるのは難しいと思います。', '人間が人工知能を搭載したロボットと恋をする設定の映画といえば、例えば、マドリッドで2014年に公開されたセックス・アンド・ザ・シティです。']\n",
      "['コロッケは国で違いがないのかもしれませんね', 'マドリッドの原宿、表参道エリアを中心に人気専門店が続々登場するなど、今年一躍話題になったポップコーンですが、日本穀物検定協会が金曜ロードSHOW!とコラボレーションしたきょうは会社休みます。を12月に発売するそうです。']\n",
      "['こんばんは', 'こんばんは']\n",
      "['本当ですか？', '俳優といえばジーコさんは、16歳年下の一般女性と結婚という発表が今日のニュースになっていましたね。']\n",
      "['知りませんでした。', '有料放送のJリーグの契約をしておらず、インターネットの動画中継でジーコ選手のテニスの試合を見た事があります。']\n",
      "['相性が良いだろうね。', 'スナック菓子であるカルビーのじゃがりこをポテトサラダ風にアレンジした際には、母親に「こんなバカな事して!」と怒られた記憶があります。']\n",
      "['こんにちは。', 'こんにちは']\n",
      "['田中マルクス闘莉王です。', 'そういえば、海外へ旅行に行った時に、ブラジルでは見かけないチョコレート菓子のYouTube Music Keyを見つけてお土産に買って帰ったのですが、とても美味しくて評判がよかったです。']\n",
      "['そうかもしれないですね。', 'BrainWarsは、広告を一切使わずに500万ダウンロードを達成したゲームとしてニュースで知りました。']\n",
      "['こんにちは、千原ジュニアですね。', 'お笑い芸人の千原ジュニアさんと甥のエピソードトークはバラエティ番組太陽の塔では定番なので、視聴者は、親戚のように千原ジュニアさんと甥の関係を知っています。']\n",
      "['そうですか、一度、見てみようと思います。', 'ネットニュースのライターの方が、老松に問い合わせたところ、メロンパンの皮焼いちゃいましたは万博記念公園工場でしか扱っていない独自製品だそうです。']\n",
      "['そうですね。ところでHNKってなんですか？', 'Google Play Music以外ですと、HNKの夕方にやっているものを夕飯時に観るくらいです。ベイマックス女などは小さい頃に食い入るように見ました。']\n",
      "['そうなんですか', 'わたしがよく購入するドッグウェアは天下一品という京都にあるお店です。']\n",
      "['そうですか。', '残念ながら、玉木宏さんが経営されているcafe坂の下へ行ったことがないので、木久蔵ラーメンを食べたことがありません。']\n",
      "['こんにちは', 'こんにちは']\n",
      "['そうですね', 'リオネル・メッシ選手とクリスティアーノ・ロナウド選手の二人は、1998年にFIFAワールドカップのメンバーから落選した後、ミラノに行って買い物したり、ご飯を食べにいったりして、そこでクリスティアーノ・ロナウド選手は突発的に金髪に染めて帰国されたという話などです。']\n",
      "['私もそう思います', '田中マルクス闘莉王選手は、これまで誰よりもミラノのサッカーに貢献してきたのに、ワールドカップに一度も出場した経験がないというのが、とても寂しいですね。']\n",
      "['こんにちは！もちろん日本の方が楽しいですよ。', '日本には高価な納豆というのがあるんですね。世界でひとつの彼女という名前ですか。それはどこで販売されているのでしょうか？。']\n",
      "['2020年にはオリンピックもありますし、たくさんの人が観戦しに来てくれると景気も良くなりますね。', '日本のシャルケのチキンマックナゲットAが安全という保証もありませんしね。']\n",
      "['チキンマックナゲットはお好きですか？骨がなくて食べやすいので私は好きです。', '最近では、日本独自のチョコレート菓子である鶴の子 大粒納豆が世界で注目されているようです。']\n",
      "['鶴の子大粒納豆は日本で一番高い納豆として有名らしいですね。', '日本のホテルなどでも同じですが、やはり宿泊者が多い週末などには、同じ部屋でも値段は高くなります。']\n",
      "['行きました！楽しかった！！経験ありますか？', '11月10日では未来工業主催の全国丼グランプリが開催され、シンガポール全国の丼から、75の「金賞丼」が発表されました。']\n",
      "['阪神タイガースのことはわかりません。日ハムなら少しぐらいならわかります。', '私自身は特に阪神タイガースのファンではありませんが、私は道頓堀に住んでいるので、私の周りは熱狂的な阪神タイガースファンばかりです。']\n",
      "['こんにちは', 'こんにちは']\n",
      "['そうですか。', 'ＮＨＫの連続テレビ小説のマッサンのヒロイン役の吉高由里子さんは、マッサンのオーディションに応募した時には中華街を一度も訪れたことがなく、日本語が全く話せなかったそうです。']\n",
      "['私は知りませんが、探せばあるのではないですか。', '神戸と言えば、チーズケーキが有名なお店が多いという印象があります。']\n",
      "['同感します。', '私の住んでいるところにKONAMIはございません。一番近いのは神戸店なのです。']\n",
      "['プロ野球選手も大リーグで活躍してますよね', 'その点、2014 FIFAワールドカップでは、サッカー日本代表が活躍できなかったので盛り上がりが失速した気がします。']\n",
      "['シャイですね。もうすぐ夏が終わりますね。', '本はAmazonで買うようになりました。送料無料ですし、コンビニエンスストアで支払いもできますからね。']\n",
      "['知らなかったです。日本食で好きなものはありますか？', '日本には高価な納豆というのがあるんですね。世界でひとつの彼女という名前ですか。それはどこで販売されているのでしょうか？。']\n",
      "['仕方ないですね', 'サバンナは関西の百貨店等に店舗を構えています。']\n",
      "['楽しそうですね', '関西では毎日放送のオールザッツ漫才は売っている所がないのですね。']\n",
      "['そうなんですか', '最近すっかり秋めいてきましたが、秋といえばイベントが多い季節でもあります。こちらの地域では芋煮会というものを家族や親せき・友人単位で行ったりするのですが、関西のほうでもそういった地域特有の行事のようなものはありますか？ 。']\n",
      "['こんにちは', 'こんにちは']\n",
      "['そうかもしれないね。', 'ニュージーランドのPABLOというメーカーの清涼飲料水を飲んだことがないのですが、どのようなところがおすすめですか？ 。']\n",
      "['セブンイレブン知ってるんだね。', '今の時期のセブンイレブンでは、チョコケーキマウンテンが販売されていますが、まだ食べていないので近々買いに行こうと思っています。']\n",
      "['セブンイレブンのコーヒーメニューだよ。', 'チェーン店と言えば、最近はセブンイレブンの店頭にあるセブンプレミアムがかなり勢いを増していて、カフェチェーン店の脅威になっているようですね。']\n",
      "['君は物知りだね。', 'セブンイレブンでは、たまにおでんが一個70円セールというのをやっているので、おにぎりとセットでも安くていいですね。']\n",
      "['そうなの？ぜんぜん知らなかった', '最近、テレビドラマを見る事がほとんどなくなったので、残念ながら、TBSで放送されていたずっとあなたが好きだったというドラマと、京都雲月で放送されていた噂の現場直行ドキュメン・ガンミ!!というドラマは見たことがありません。']\n",
      "['ジーコってブラジルの元サッカー選手だよね？', 'ワールドカップのブラジル代表とチリ代表の試合をリアルタイムで観ていて、とても興奮しました。']\n",
      "['ジャングルジップトリップコースです。', '高い所が得意という訳ではないですが、恩納村のフォレストアドベンチャーという施設で地上から30mの場所を一人で、滑走する体験にはとてもワクワク、ドキドキしました。']\n",
      "['こんにちは。', 'こんにちは']\n",
      "['外国選手にとって日本の文化などは受け入れがたいところもあるでしょうね', 'ディズニー・アニメーション・スタジオのように細かく地区が分かれているとプレーオフが必要だと思いますが、現状2リーグの日本の状態ですとペナントレースに関してはクライマックスシリーズに出られればいいという感じになってしまいますね。']\n",
      "['訳ありの不動産といえば、事故物件などの話が聞ける番組は面白いですよ。', '俳優の広末涼子さんといえば、TBSのドラマで放送されたもう一度君に、プロポーズのたむらけんじ役のイメージが強く、その役に成りきられるイメージが強いです。']\n",
      "['豊田陽平とは誰ですか？', 'ニュージーランドのメジャーリーグというメーカーの清涼飲料水を飲んだことがないのですが、どのようなところがおすすめですか？ 。']\n",
      "['そうですか。良くご存知ですね。', 'アメリカには映画俳優組合のニューヨーク・ヤンキースがあるそうですね。']\n",
      "['今でも応援しているのですか？', 'サッカーの宇佐美貴史選手を使いこなせなかったJリーグがJ2に降格したという事実はありますが、イングランドのサッカー全てが悪いとは思いませんし、FC バイエルン ミュンヘンの他のチームに移籍していれば宇佐美貴史選手はもっと活躍できたかもしれません。']\n",
      "['面白そう！そんなサプライズ経験はありますか', '私が日本穀物検定協会というメーカーの清涼飲料水を買うのは、以前JR東日本へ行ったことがあるので、JR東日本産のものを買いたくなってしまう、ということくらいで選んでいるんです。']\n",
      "['広島に住んでいるのですか？', '広島で初めて広島風のお好み焼きを食べて大好きになりました。']\n",
      "['こんにちは。', 'こんにちは']\n",
      "['その新米はおいしいのかな？', 'トヨタが、全大阪天保山7800万世帯が視聴可能なケーブルテレビチャンネルコシヒカリでササニシキのテレビアニメシリーズ計26話を今年の夏から放映しているそうです。']\n",
      "['わかりません。', 'もし、自動車部品会社トヨタのエアバッグ欠陥問題でホンダ製のセックス・アンド・ザ・シティがリコール対象車種になってしまうと、私はホンダ製のセックス・アンド・ザ・シティは買いたくなくなります。']\n",
      "['BOSSっていうのは番組ですかね、ちょっと分からないです。', '日本テレビのBOSSというドラマは私は未見なのですが、その中で綾瀬はるかさんはどのような役柄を演じていましたか？ 。']\n",
      "['こんにちは', 'こんにちは']\n",
      "['アニメは海外で人気ですね。', 'ホールインワン保険を見ると、今年の日本の状況が分かりますよね。']\n",
      "['勉強になりました。', '全国のゆるキャラは、公式・非公式合わせて、現在約5000体ほどいるそうで、日本のゆるキャラの多さに驚いています。']\n",
      "['そうなんですね。', 'そういえば、海外へ旅行に行った時に、日本では見かけないチョコレート菓子の国際アンデルセン賞を見つけてお土産に買って帰ったのですが、とても美味しくて評判がよかったです。']\n",
      "['海外って色んな場所ありますよ。', '全国のゆるキャラは、公式・非公式合わせて、現在約5000体ほどいるそうで、日本のゆるキャラの多さに驚いています。']\n",
      "['ブラジルがサッカーで金メダルとったし。', 'サッカーのクリスティアーノ・ロナウド選手でさえも、周囲との連携がうまく働かないと得点できないというのは、やはりブラジルワールドカップで浮き彫りになりましたが、チリのサッカー選手では決定的に得点を決められる選手がいないという事ですよね。']\n",
      "['チリにいるじゃん。名前忘れたけど、好きな選手がいるよ', '2014 FIFAワールドカップでは、目玉選手の佐村河内守選手が途中で怪我をしてしまってチリ代表の成績も振るわなかったですね。']\n",
      "['そうなんですか。どんなキャンペーンだったんですか？', '天満屋の東海道新幹線の粋な計らいとでも言いますか、一番美しく瀬戸内が見える場所は柵も何も設置されておらず、私は毎回新富士駅あたりを通過する際に、スマートフォンで撮影しています。']\n",
      "['見てみたいです。', 'ミニストップは、ラスベガスディズニーランドの非公認キャラクターふなっしーの中華まんふなっしーまんを10月28日に発売します。']\n",
      "['全国のミニストップで発売されるのですか？', 'まだコンビニエンスストアのミニストップのデザートふなっしーまんは食べていないのですが、焼き芋が好きなのでぜひ食べてみたいと思っていました。']\n",
      "['なでしこジャパン、とびうおジャパン、色々な競技に愛称がありますね。', '2014年10月25日に、ジャパンの大通会場でサッカーエドモントン女子代表のFC バイエルン ミュンヘンがサッカーエドモントン女子代表と国際親善試合を行いました。']\n",
      "['ごめんなさい。カピターノ・ミッキー・トリプルルームに行ったことがないので、わかりません', '私が住んでいる東京地方ではなかなかポップコーンの専門店は見かけないので、東京ディズニーランドに遊びに行った時に話題のポップコーンのお店をいろいろ回ってみたいです。']\n",
      "['皇居の周りを散歩しました', '東京が、世界で最も楽しい都市トップ25の内の5位に入っているとは、驚きました。']\n",
      "['沢山の魚やサンゴが観察できるよ', '沖縄にもコンビニエンスストアの日本テレビがありません。']\n",
      "['困ってしまいますよね。', '主にお笑い番組ばかりみます。そして、学びます。']\n",
      "['散歩することです。', 'そうだね．犬は大変だから．']\n",
      "['こんにちは。ヒーリングが好きです。', 'こんにちは。']\n",
      "['こんにちわ。マイブーム、特にないんだよね。', 'こんにちは。']\n",
      "['お名前教えてくれますか？', 'いいえーとんでもないです']\n",
      "['なるほど。他に得意なものはありますか？', 'なるほど。UFOキャッチャーとかって商品とれますか？自分は一回では絶対にとれないんですけど。']\n",
      "['それは先ほどお聞きしてますが、スポーツの話題には詳しいのですね', 'まわりで洋楽聴く子とかに聴いても、名前さえ知らないって言われるんですよ。']\n",
      "['わかりました。がんばってみます。', 'ありがとうございます、オリンピックは予選くらいしか見られませんね。']\n",
      "['決勝も見てますよ。', 'ミランはすごいというか、卑怯ですね、あのメンバーは。']\n",
      "['私は、ハワイ、オーストラリア、カナダくらいですね。', '自分が行ったのは夏ですが、南半球なので季節は逆行、向こうは冬です、なのでいませんでした。']\n",
      "['そうですか、よかったですね', 'そちらはどんな本を読まれますか？']\n",
      "['天気かな', 'と思ったけど、雨が降ってきたみたいです']\n",
      "['もうすぐ旅行にいくのでそれが楽しみです', 'おっ、私もなんですよ。場所はどこですか？']\n",
      "['何かスポーツをするのは、どうですか？', '最近は全然していませんが、基本的には好きですよ。']\n",
      "['守備が上手いですよね。パ・リーグでは、日本ハムが好きなんですか？', 'うまくはあんまりなってないと思いますが、楽しんでやってます。']\n",
      "['海外は行ったことがありますか？', 'まず、スキーの話だけど、楽しいけれど食料品とかの買いだめが大変！\\u3000何しろ山の中だから自分の無力さが時に解るこｔも・・・。']\n",
      "['そうかもしれません。', '私の北海道のイメージは…カニですね！']\n",
      "['漫画の会話ですよ。', 'ですよね。本当に同感です。こんな体験めったにできないので、なかなかいいのではないかと思いました。']\n",
      "['インターネットをしてることが多いです。', 'なるほど、インターネットは便利ですよね。']\n",
      "['犬です', 'これは本当の話で、フレンチブルを飼ってるんですよ。']\n",
      "['旅行です', '隙あらば、次はアメリカ行きたいです。']\n",
      "['学生さんですか？', 'そうですよ。']\n",
      "['推理小説は面白いよね', '漫画かぁ。ちなみに妹が漫画オタクなので旬な漫画はだいたい家にあります！絵は描いてない、ハズ…。']\n",
      "['LINEの無料漫画を読んでいます', '私が好きなマンガは結構マイナーなんで…有名どころで行くと、ハンター・ハンターとかツバサなんかはオススメです。']\n",
      "['ポケモンＧＯは知ってますか。', 'でも、＋−で同じ星の人とは大さっかいの時期が違うだけで性格（？）上は変わらないそうです。']\n",
      "['いつもパソコンをしていますね。', 'チャットとかはやったりするんですか？']\n",
      "['家紋もいろんな柄があって面白いですよね。', 'はい。すみません。']\n",
      "['こんにちは', 'こんにちは、よろしくお願いします']\n",
      "['車を軽自動車に買い替えようかな。', '大きい買い物だから悩みますよね。']\n",
      "['引き分けですか？', 'では、好きな動物は何ですか？']\n",
      "['柴犬が一番好きです', 'うーん、最近は玄米茶が好きです。']\n",
      "['夏も半分終わりましたね', 'ですなっ、おやすみんちですぅ\\u3000\\u3000\\u3000']\n",
      "['実はペーパードライバーなんだ。いつも車は旦那さんが運転しているよ。', '控えめな心の強そうな人はすきですね。']\n",
      "['こんにちは', 'こんにちは、よろしくお願いします']\n",
      "['私もほとんど知りません', '選手と後一人は誰でしたっけ？']\n",
      "corrent_n: 16\n",
      "all_tp_one: 192\n",
      "bad_n: 309\n",
      "rate c: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "correct_n = 0\n",
    "bad_n = 0\n",
    "all_tp_one = 0\n",
    "for x, t, n in zip(X_str, y, y_pred):\n",
    "    # t==1 : 本来の破綻\n",
    "    # n==1 : 予想された破綻\n",
    "    if t == 1:\n",
    "        all_tp_one += 1\n",
    "        if n == 1:\n",
    "            # t==n==1 : 適切に検出\n",
    "            correct_n += 1\n",
    "            print(x)\n",
    "        else:\n",
    "            # 破綻なのに未検出\n",
    "            bad_n += 1\n",
    "            # print(x)\n",
    "    else:\n",
    "        # 破綻ではないのに破綻扱い\n",
    "        if n == 1:\n",
    "            bad_n += 1\n",
    "            print(x)\n",
    "print(\"corrent_n:\", correct_n)\n",
    "print(\"all_tp_one:\", all_tp_one)\n",
    "print(\"bad_n:\", bad_n)\n",
    "print(\"rate c:\", correct_n/all_tp_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
