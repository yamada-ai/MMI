{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, batch_first=True, bidirectional=True )\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        bilstm_out = torch.cat([hidden_layer[0][0], hidden_layer[0][1]], dim=1)\n",
    "        # y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "\n",
    "        y = self.hidden2tag(bilstm_out)\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "Nmodel_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "Nmodel = SentenceTransformer(Nmodel_path, show_progress_bar=False)\n",
    "emb_dim = Nmodel.encode([\"お辞儀をしている男性会社員\"])[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(convs, max_len):\n",
    "    # emb_dim = nlp(\"形態素\").vector.shape\n",
    "    X_data = []\n",
    "    \n",
    "    for conv in convs :\n",
    "        # vec_list = np.zeros( (max_len, emb_dim[0]) )\n",
    "        sentence_vectors = Nmodel.encode(conv)\n",
    "        # for i, ut in enumerate(conv):\n",
    "        #     doc = nlp(ut)\n",
    "        #     vec_list[i] = doc.vector\n",
    "        X_data.append(sentence_vectors)\n",
    "    return np.array(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "\n",
    "output = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_with_NoErr(path:str, datalist:list) -> pd.DataFrame:\n",
    "    cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "    df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "    for p in datalist:\n",
    "        datapath = Path(path + p + '/')\n",
    "        for file in datapath.glob(\"*.json\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                did = json_data[\"did\"]\n",
    "                for t in json_data[\"turns\"]:\n",
    "                    if t[\"turn-index\"] == 0:\n",
    "                        continue\n",
    "                    if t[\"speaker\"] == \"U\":\n",
    "                        usr = t[\"utterance\"]\n",
    "                        continue\n",
    "                    if t[\"speaker\"] == \"S\" :\n",
    "                        tid = t[\"turn-index\"]\n",
    "                        sys = t[\"utterance\"]\n",
    "                        if t[\"error_category\"]:\n",
    "                            ec = t[\"error_category\"]\n",
    "                        else:\n",
    "                            ec = [\"No-Err\"]\n",
    "                        df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_json_with_NoErr(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_continue_convs_with_error(df, length, errors):\n",
    "    new_convs = []\n",
    "    continue_conv = []\n",
    "    did = 0\n",
    "    for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "        # did が変化すれば，別の対話\n",
    "        if d != did:\n",
    "            continue_conv = []\n",
    "            did = d\n",
    "        continue_conv .append(u)\n",
    "        continue_conv .append(s)\n",
    "        for err in errors:\n",
    "            if len(continue_conv) >= length and err in e:\n",
    "                new_convs.append( continue_conv[-length:] )\n",
    "    \n",
    "    return new_convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "# errors = [\"Lack of information\"]\n",
    "errors = [\"Topic transition error\"]\n",
    "# errors = [\"Unclear intention\"]\n",
    "length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic5-2.pickle\n",
      "success load : ../models/context/topic5-2.pickle\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/context/\"\n",
    "model_name = \"topic5-{0}.pickle\".format(length)\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)\n",
    "if modelM.is_exist(model_name):\n",
    "    model = modelM.load_data(model_name)\n",
    "    model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real test\n",
    "leng_c = 2\n",
    "def make_X_str_y(df, errors):\n",
    "    X_str = []\n",
    "    y = []\n",
    "    y = np.zeros(len(df))\n",
    "    continue_conv = []\n",
    "    did = 0\n",
    "    for i, (d, u, s, e) in enumerate(zip(df.did, df.usr, df.sys, df.ec)):\n",
    "        # sentence_vectors = Nmodel.encode([u, s])\n",
    "        if d != did:\n",
    "            continue_conv = [\"\"]*leng_c\n",
    "            did = d\n",
    "        continue_conv.append(u)\n",
    "        continue_conv.append(s)\n",
    "        X_str.append( continue_conv[-leng_c:] )\n",
    "        for err in errors:\n",
    "            if err in e:\n",
    "                y[i] = 1\n",
    "\n",
    "    return X_str, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str, y = make_X_str_y(df, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_X(X_str, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     X_tensor = torch.tensor(X, device='cuda:0').float()\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    # y_tensor = torch.tensor(y, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "#     y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)\n",
    "    y_pred= np.array(model(X_tensor)).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[1767   41]\n",
      " [ 190    2]]\n",
      "accuracy =  0.8845\n",
      "EM: 0.8845\n",
      "F-measure:  0.01702127659574468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "\n",
    "print('EM:', metrics.accuracy_score(y, y_pred))\n",
    "print('F-measure: ', metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, np.zeros(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['はい。あなたは海に行きますか？', '海は危険です']\n",
      "['栄養面を考えて食事をとることは大切ですよね。', '外でとります']\n",
      "['どこの海がお勧めですか？', '海は楽しいですね']\n",
      "['でも私は泳げないんです。', '海は大好きですね']\n",
      "['海外旅行されたことありますか？', '最近興味のあることは何ですか？']\n",
      "['海外旅行の経験はありますか？', '海外旅行も楽しいかな。']\n",
      "['お花見は好きですか？', 'どうだと思いますか？']\n",
      "['こんにちは。パソコンのし過ぎで目が疲れてきた', '焦点が合うんですか？']\n",
      "['嬉しいですね', 'なでなで']\n",
      "['だれがルーズなんですか？', '銀行からお金を借りるらしいですよ']\n",
      "['でも、たまに荒ぶりますよね', '海は楽しいですね']\n",
      "['わたしがですか', 'そうですね']\n",
      "['そうですか？', 'そうだよ']\n",
      "['運転が疲れた時は車を止めて少し休みましょう。', '通り道をふさぎます']\n",
      "['夏場はそうですね', '海は熱いですか']\n",
      "['私も通堂は面白くて好きです。遠いところにあるので不便ですね。', 'やはり水卜麻美選手は、自身の出身地広島でプロ野球人生を終えたいのでしょうかね。']\n",
      "['篤姫は少しだけテレビで見たことがあります。祖母は連続テレビ小説が好きで、毎日見ているんです。', '女優有働由美子さんが主演していたNHKテレビドラマ篤姫には、音楽グループニッカウヰスキーのシャーロット・ケイト・フォックスさんも婚活に励む男性役で出演していたのですが、あまりに自然体なのでニッカウヰスキーのメンバーだと後から知ってとても驚きました。']\n",
      "['マザコンの男性は意外と多いようですね。', 'TBSで放送されたテレビドラマのずっとあなたが好きだったの内容は、定時制高校を舞台としたドラマで、スナックのママとして働いていた冬彦さんさんが定時制高校の校長からスカウトされたことから始まる、テレビドラマでした。']\n",
      "['フランスです', 'セレッソ大阪のフランス料理のフルコース宅配弁当で思い出したのですが、フランス料理のシェフが出張で料理を作ってくれるというケータリングサービスはいいと思いました。']\n",
      "['それは何処にあるのですか？', 'ニュージーランドあたりでのクリスマスに登場する豊田陽平の服装は、やはり薄着なのですね。']\n",
      "['豊田陽平とは誰ですか？', 'ニュージーランドのメジャーリーグというメーカーの清涼飲料水を飲んだことがないのですが、どのようなところがおすすめですか？ 。']\n",
      "['面白そう！そんなサプライズ経験はありますか', '私が日本穀物検定協会というメーカーの清涼飲料水を買うのは、以前JR東日本へ行ったことがあるので、JR東日本産のものを買いたくなってしまう、ということくらいで選んでいるんです。']\n",
      "['ブラジルがサッカーで金メダルとったし。', 'サッカーのクリスティアーノ・ロナウド選手でさえも、周囲との連携がうまく働かないと得点できないというのは、やはりブラジルワールドカップで浮き彫りになりましたが、チリのサッカー選手では決定的に得点を決められる選手がいないという事ですよね。']\n",
      "['チリにいるじゃん。名前忘れたけど、好きな選手がいるよ', '2014 FIFAワールドカップでは、目玉選手の佐村河内守選手が途中で怪我をしてしまってチリ代表の成績も振るわなかったですね。']\n",
      "['瀬戸内ですかね。なんでその２つなんですか？', '東海道新幹線観光キャンペーンでは、瀬戸内でいちばん出荷量の多いレモンがありましたね。']\n",
      "['そうなんですか。どんなキャンペーンだったんですか？', '天満屋の東海道新幹線の粋な計らいとでも言いますか、一番美しく瀬戸内が見える場所は柵も何も設置されておらず、私は毎回新富士駅あたりを通過する際に、スマートフォンで撮影しています。']\n",
      "['なでしこジャパン、とびうおジャパン、色々な競技に愛称がありますね。', '2014年10月25日に、ジャパンの大通会場でサッカーエドモントン女子代表のFC バイエルン ミュンヘンがサッカーエドモントン女子代表と国際親善試合を行いました。']\n",
      "['散歩することです。', 'そうだね．犬は大変だから．']\n",
      "['明日の朝、起きるの大変です。', 'そうですか。']\n",
      "['お名前教えてくれますか？', 'いいえーとんでもないです']\n",
      "['保育士が夢だったんですか？', 'すごいですね！子供と接する仕事はおもしろそうですよね？']\n",
      "['ジョギングとか？', 'スポーツが好きなんですね。']\n",
      "['天気かな', 'と思ったけど、雨が降ってきたみたいです']\n",
      "['ちがいますよ', 'ニュースなんですよ']\n",
      "['あまりやりません。', 'そうなんですか。']\n",
      "['あまりやりません', 'そうなんですか。']\n",
      "['旅行です', '隙あらば、次はアメリカ行きたいです。']\n",
      "['社会人です', 'あ、そうなんですか、失礼しました。']\n",
      "['車を軽自動車に買い替えようかな。', '大きい買い物だから悩みますよね。']\n",
      "['地震に対する準備はされてますか？', '本当多いですね。地震多いですね。']\n",
      "['テナーサックスはやっていませんよ', 'そういうわけです。（この辺の話はもしわかる人がいたらと思って書いたんですけど守備範囲外だったらスルーしていいですよ）。']\n",
      "corrent_n: 2\n",
      "all_tp_one: 192\n",
      "bad_n: 231\n",
      "rate c: 0.010416666666666666\n"
     ]
    }
   ],
   "source": [
    "correct_n = 0\n",
    "bad_n = 0\n",
    "all_tp_one = 0\n",
    "for x, t, n in zip(X_str, y, y_pred):\n",
    "    # t==1 : 本来の破綻\n",
    "    # n==1 : 予想された破綻\n",
    "    if t == 1:\n",
    "        all_tp_one += 1\n",
    "        if n == 1:\n",
    "            # t==n==1 : 適切に検出\n",
    "            correct_n += 1\n",
    "            # print(x)\n",
    "        else:\n",
    "            # 破綻なのに未検出\n",
    "            bad_n += 1\n",
    "            # print(x)\n",
    "    else:\n",
    "        # 破綻ではないのに破綻扱い\n",
    "        if n == 1:\n",
    "            bad_n += 1\n",
    "            print(x)\n",
    "print(\"corrent_n:\", correct_n)\n",
    "print(\"all_tp_one:\", all_tp_one)\n",
    "print(\"bad_n:\", bad_n)\n",
    "print(\"rate c:\", correct_n/all_tp_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
