{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "\n",
    "output = \"./\"\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "emb_dim = nlp(\"形態素\").vector.shape[0]\n",
    "emb_dim"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def read_json_with_NoErr(path:str, datalist:list) -> pd.DataFrame:\n",
    "    cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "    df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "    for p in datalist:\n",
    "        datapath = Path(path + p + '/')\n",
    "        for file in datapath.glob(\"*.json\"):\n",
    "            with open(file, \"r\") as f:\n",
    "                json_data = json.load(f)\n",
    "                did = json_data[\"did\"]\n",
    "                for t in json_data[\"turns\"]:\n",
    "                    if t[\"turn-index\"] == 0:\n",
    "                        continue\n",
    "                    if t[\"speaker\"] == \"U\":\n",
    "                        usr = t[\"utterance\"]\n",
    "                        continue\n",
    "                    if t[\"speaker\"] == \"S\" :\n",
    "                        tid = t[\"turn-index\"]\n",
    "                        sys = t[\"utterance\"]\n",
    "                        if t[\"error_category\"]:\n",
    "                            ec = t[\"error_category\"]\n",
    "                        else:\n",
    "                            ec = [\"No-Err\"]\n",
    "                        df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "def extract_error(df, errors):\n",
    "    utterances = []\n",
    "    for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "        for err in errors:\n",
    "            if err in e:\n",
    "                utterances.append([u, s])\n",
    "    return utterances\n",
    "\n",
    "def no_error(df):\n",
    "    utterances = []\n",
    "    system = []\n",
    "    user = []\n",
    "    for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "        if e[0] == \"No-Err\":\n",
    "            utterances.append([ u, s])\n",
    "        # system.append(s)\n",
    "        # user.append(u)\n",
    "    return utterances\n",
    "\n",
    "def _in(arg1, arg2):\n",
    "    result = False\n",
    "    if isinstance(arg1, list):\n",
    "        for element in arg1:\n",
    "            if isinstance(element, str):\n",
    "                if element in arg2:\n",
    "                    result = True\n",
    "                    break\n",
    "            else:\n",
    "                if element == arg2:\n",
    "                    result = True\n",
    "                    break\n",
    "    else:\n",
    "        if isinstance(arg1, str):\n",
    "            if arg1 in arg2:\n",
    "                result = True\n",
    "        else:\n",
    "            if arg1 == arg2:\n",
    "                result = True\n",
    "    return result\n",
    "\n",
    "def filter_pos_vector(pos_list, text):\n",
    "    vec = np.zeros(emb_dim)\n",
    "    doc = nlp(text, disable=['ner'])\n",
    "    for token in doc:\n",
    "        # print(token, token.tag_)\n",
    "        if \"助動詞\" not in token.tag_ and _in(pos_list, token.tag_):\n",
    "            # print(token, token.tag_)\n",
    "            vec += token.vector\n",
    "    return vec\n",
    "\n",
    "def feature_extraction(pos_list, convs, mode, is_norm=False):\n",
    "    feature_list = []\n",
    "    for utt in convs:\n",
    "\n",
    "        if mode == \"dif\":\n",
    "            usr_vec = filter_pos_vector(pos_list, utt[0] )\n",
    "            sys_vec = filter_pos_vector(pos_list, utt[1] )\n",
    "            dif_vec = usr_vec - sys_vec\n",
    "            # if np.linalg.norm(dif_vec)==0:\n",
    "            #     print(utt)\n",
    "            if is_norm:\n",
    "                # print(np.linalg.norm(dif_vec))\n",
    "                if np.linalg.norm(dif_vec)==0:\n",
    "                    vec = np.concatenate( [dif_vec, dif_vec/0.05] )\n",
    "                else:\n",
    "                    vec = np.concatenate( [dif_vec, dif_vec/np.linalg.norm(dif_vec)] )\n",
    "                feature_list.append(vec)\n",
    "            else:\n",
    "                feature_list.append(dif_vec)\n",
    "    \n",
    "    return np.array(feature_list, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def remove_norm_zero(pos_list, convs):\n",
    "    convs_removed = []\n",
    "    for utt in convs:\n",
    "        usr_vec = filter_pos_vector(pos_list, utt[0] )\n",
    "        sys_vec = filter_pos_vector(pos_list, utt[1] )\n",
    "        dif_vec = usr_vec - sys_vec\n",
    "        if np.linalg.norm(dif_vec)==0 or np.linalg.norm(usr_vec)==0 or np.linalg.norm(sys_vec)==0:\n",
    "            continue\n",
    "        convs_removed.append(utt)\n",
    "    return convs_removed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df = read_json_with_NoErr(path, datalist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "errors = [\"Topic transition error\", \"Unclear intention\", \"Lack of information\"]\n",
    "errors = [\"Topic transition error\"]\n",
    "ut = extract_error(df, errors)\n",
    "ut_no = no_error(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "pos_list = [\"名詞\", \"動詞\", \"形状詞\", \"形容詞\"]\n",
    "ut_new = remove_norm_zero(pos_list, ut)\n",
    "ut_no_new = remove_norm_zero(pos_list, ut_no)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import random\n",
    "def fake_error(convs, times):\n",
    "    user_list = []\n",
    "    system_list = []\n",
    "    for utt in convs:\n",
    "        user_list.append(utt[0])\n",
    "        system_list.append(utt[1])\n",
    "    \n",
    "    fake_convs = []\n",
    "    for _ in range(times):\n",
    "        fake_convs.append( [random.choice(user_list), random.choice(system_list)] )\n",
    "    return fake_convs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "UT = ut_new + fake_error(ut_no_new, len(ut_no_new)-len(ut_new))\n",
    "UT_NO = ut_no_new\n",
    "print(\"UT: {0}\".format(len(UT)))\n",
    "print(\"UT_NO: {0}\".format(len(UT_NO)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UT: 528\n",
      "UT_NO: 528\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# LSTM?\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, batch_size):\n",
    "        # 親クラスのコンストラクタ。決まり文句\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。    \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
    "        # self.softmax = \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "        _, hidden_layer = self.lstm(x)\n",
    "        # print(hidden_layer)\n",
    "        y = self.hidden2tag(hidden_layer[0].view(batch_size, -1))\n",
    "        y = F.log_softmax(y, dim=1)\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "len_func = lambda conv: sum( [ len(nlp(c)) for c in conv] )\n",
    "max_len = max( list(map(len_func, UT+UT_NO)) )\n",
    "print(max_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "84\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class Datasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        # self.transform = transform\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "        self.datanum = len(X_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_X = self.X_data[idx]\n",
    "        out_y = self.y_data[idx]\n",
    "\n",
    "        return out_X, out_y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def make_X(convs, max_len):\n",
    "    emb_dim = nlp(\"形態素\").vector.shape\n",
    "    X_data = []\n",
    "    for conv in convs:\n",
    "        vec_list = np.zeros( (max_len, emb_dim[0]) )\n",
    "        doc = nlp(conv[0]+conv[1])\n",
    "        for i, token in enumerate(doc):\n",
    "            vec_list[i] = token.vector\n",
    "        X_data.append(vec_list)\n",
    "    \n",
    "    return np.array(X_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "def make_X2(convs):\n",
    "    emb_dim = nlp(\"形態素\").vector.shape\n",
    "    X_data = []\n",
    "    for conv in convs:\n",
    "        vec_list = []\n",
    "        doc = nlp(conv[0]+conv[1])\n",
    "        for i, token in enumerate(doc):\n",
    "            if np.linalg.norm(token.vector) > 0:\n",
    "                vec_list.append(token.vector)\n",
    "        X_data.append( torch.Tensor(vec_list) )\n",
    "    return rnn.pad_sequence( X_data, batch_first=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "EMBEDDING_DIM = emb_dim\n",
    "HIDDEN_DIM = emb_dim*2\n",
    "OUTPUT_DIM = 2\n",
    "seq_len = max_len"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def y_2array(y_):\n",
    "    ary = []\n",
    "    for a in y_:\n",
    "        b = np.zeros(OUTPUT_DIM)\n",
    "        b[int(a)] = 1\n",
    "        ary.append(b)\n",
    "    return np.array(ary)\n",
    "y_ = np.concatenate( [ np.ones(len(UT)), np.zeros(len(UT_NO)) ] )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "\n",
    "# X = make_X(UT + UT_NO, max_len)\n",
    "X = make_X2(UT + UT_NO)\n",
    "# y = y_2array(y_)\n",
    "y = y_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "# y = y_2array(y_)\n",
    "# y\n",
    "data_path = \"../X_y_data/context/\"\n",
    "data_name = \"topic1.pickle\"\n",
    "dataM = DataManager(data_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "if dataM.is_exist(data_name):\n",
    "    \n",
    "    DATA_Xy = dataM.load_data(data_name)\n",
    "    X = DATA_Xy[0]\n",
    "    y = DATA_Xy[1]\n",
    "else:\n",
    "    # X_data, y_data = pre.extract_X_y(df, error_types, seq_len)\n",
    "    X = make_X2(UT + UT_NO)\n",
    "    y = np.concatenate( [ np.ones(len(UT)), np.zeros(len(UT_NO)) ] )\n",
    "    dataM.save_data(data_name, [X, y])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success save : ../X_y_data/context/topic1.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "X_train = X_train[:-4]\n",
    "y_train = y_train[:-4]\n",
    "leng = len(y_train)\n",
    "print(leng)\n",
    "for i, v in enumerate(y_train):\n",
    "    if leng %(i+1) == 0:\n",
    "        print(i+1, end=\", \")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "840\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 15, 20, 21, 24, 28, 30, 35, 40, 42, 56, 60, 70, 84, 105, 120, 140, 168, 210, 280, 420, 840, "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "BATCH_SIZE = 70\n",
    "epoch_ = 1000\n",
    "trainset = Datasets(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BATCH_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "   model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "losses = []\n",
    "loss_border = 0.0001\n",
    "# print(\"error[{0}]\".format(error_types[error_i]))\n",
    "for epoch in range(epoch_):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    all_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_t_tensor = torch.tensor(data[0], device='cuda:0').float()\n",
    "        # y_t_tensor = torch.tensor(data[1].reshape(batch_size, 1), device='cuda:0').float()\n",
    "        y_t_tensor = torch.tensor(data[1], device='cuda:0').long()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        # print(X_t_tensor.shape)\n",
    "\n",
    "        score = model(X_t_tensor)\n",
    "        loss_ = loss_function(score, y_t_tensor)\n",
    "        loss_.backward()\n",
    "        all_loss += loss_.item()\n",
    "        optimizer.step()\n",
    "        del score\n",
    "        del loss_\n",
    "    losses.append(all_loss)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"epoch\", epoch+1, \"\\t\" , \"loss\", all_loss)\n",
    "    # if all_loss <= loss_border:\n",
    "    #     print(\"loss was under border(={0}) : train end\".format(loss_border))\n",
    "    #     break\n",
    "print(\"done\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 50 \t loss 8.334972858428955\n",
      "epoch 100 \t loss 8.445462048053741\n",
      "epoch 150 \t loss 8.28945130109787\n",
      "epoch 200 \t loss 8.261108100414276\n",
      "epoch 250 \t loss 8.235379099845886\n",
      "epoch 300 \t loss 8.307891368865967\n",
      "epoch 350 \t loss 8.31636780500412\n",
      "epoch 400 \t loss 8.321036040782928\n",
      "epoch 450 \t loss 8.323181986808777\n",
      "epoch 500 \t loss 8.320523262023926\n",
      "epoch 550 \t loss 8.323448777198792\n",
      "epoch 600 \t loss 8.317480564117432\n",
      "epoch 650 \t loss 8.317037403583527\n",
      "epoch 700 \t loss 8.3756964802742\n",
      "epoch 750 \t loss 8.287786841392517\n",
      "epoch 800 \t loss 6.3947300016880035\n",
      "epoch 850 \t loss 1.1349189970642328\n",
      "epoch 900 \t loss 0.8445930164307356\n",
      "epoch 950 \t loss 0.11301109043415636\n",
      "epoch 1000 \t loss 0.0023977498058229685\n",
      "done\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_history(losses):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    epochs = np.arange(1, len(losses) + 1)\n",
    "\n",
    "    # 損失の推移\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.plot(epochs, losses)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()\n",
    "plot_history(losses)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 368.925 277.314375\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-31T07:32:12.712662</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 277.314375 \nL 368.925 277.314375 \nL 368.925 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 239.758125 \nL 361.725 239.758125 \nL 361.725 22.318125 \nL 26.925 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me3ff917a9c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"41.838514\" xlink:href=\"#me3ff917a9c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(38.657264 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.772174\" xlink:href=\"#me3ff917a9c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(93.228424 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.705835\" xlink:href=\"#me3ff917a9c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(154.162085 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.639496\" xlink:href=\"#me3ff917a9c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(215.095746 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.573157\" xlink:href=\"#me3ff917a9c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(276.029407 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.506818\" xlink:href=\"#me3ff917a9c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <g transform=\"translate(333.781818 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epoch -->\n     <g transform=\"translate(179.014063 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m277c3a404a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m277c3a404a\" y=\"229.923278\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 233.722497)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m277c3a404a\" y=\"189.227342\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2 -->\n      <g transform=\"translate(13.5625 193.026561)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m277c3a404a\" y=\"148.531406\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(13.5625 152.330624)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m277c3a404a\" y=\"107.835469\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 6 -->\n      <g transform=\"translate(13.5625 111.634688)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m277c3a404a\" y=\"67.139533\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 8 -->\n      <g transform=\"translate(13.5625 70.938752)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m277c3a404a\" y=\"26.443597\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 30.242816)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pa6b3f65dff)\" d=\"M 42.143182 60.386912 \nL 42.752518 60.68727 \nL 43.361855 60.651269 \nL 43.666523 60.45034 \nL 43.971192 60.73043 \nL 44.27586 60.648045 \nL 44.885197 60.970193 \nL 45.189865 61.63531 \nL 45.494533 59.240658 \nL 45.799201 60.662216 \nL 46.10387 60.737535 \nL 46.408538 60.632131 \nL 46.713206 60.767252 \nL 47.017875 60.648825 \nL 49.455221 60.903847 \nL 50.064558 61.019114 \nL 50.369226 61.153031 \nL 50.673894 60.593288 \nL 50.978563 61.069933 \nL 51.283231 61.170777 \nL 51.587899 61.104769 \nL 51.892568 61.252722 \nL 52.197236 53.596458 \nL 52.501904 59.323685 \nL 52.806572 59.770627 \nL 53.111241 59.490724 \nL 53.720577 60.000431 \nL 54.025246 60.040858 \nL 54.329914 60.824233 \nL 54.939251 60.060684 \nL 55.243919 59.726133 \nL 55.548587 60.035611 \nL 55.853256 60.6879 \nL 56.157924 59.990715 \nL 56.462592 60.375193 \nL 56.76726 60.065463 \nL 57.376597 60.560571 \nL 57.681265 60.272504 \nL 58.290602 60.191203 \nL 58.59527 60.413316 \nL 58.899939 60.385076 \nL 59.204607 60.706745 \nL 59.509275 60.468427 \nL 59.813943 60.513158 \nL 60.118612 60.338239 \nL 60.42328 60.367114 \nL 60.727948 60.6734 \nL 61.032617 60.440281 \nL 61.337285 60.707995 \nL 61.641953 60.670025 \nL 61.946622 60.378266 \nL 62.555958 60.503717 \nL 62.860627 59.426474 \nL 63.165295 60.643959 \nL 64.0793 60.507025 \nL 64.383968 60.616682 \nL 64.688636 60.554227 \nL 64.993305 60.257634 \nL 65.297973 60.534674 \nL 67.125983 60.757487 \nL 67.735319 60.47451 \nL 68.039988 60.799291 \nL 68.344656 60.875666 \nL 68.953993 60.263229 \nL 69.563329 60.496691 \nL 69.867998 60.762715 \nL 70.172666 59.741464 \nL 70.477334 59.771222 \nL 70.782002 58.467484 \nL 71.086671 39.698161 \nL 71.391339 55.884551 \nL 71.696007 56.20809 \nL 72.000676 58.839598 \nL 72.305344 58.075286 \nL 72.610012 55.240989 \nL 72.914681 58.501576 \nL 73.219349 53.234432 \nL 73.524017 58.618014 \nL 73.828686 32.201761 \nL 74.133354 40.141726 \nL 74.438022 59.526659 \nL 74.74269 47.311428 \nL 75.047359 47.012712 \nL 75.656695 56.881966 \nL 75.961364 60.207477 \nL 76.266032 60.524986 \nL 76.5707 59.848178 \nL 76.875369 40.147819 \nL 77.180037 56.690732 \nL 77.484705 59.720875 \nL 77.789373 58.612093 \nL 78.094042 60.430139 \nL 78.39871 60.451674 \nL 78.703378 60.597498 \nL 79.008047 60.989243 \nL 79.312715 60.861381 \nL 79.617383 58.816448 \nL 79.922052 59.119193 \nL 80.22672 61.322283 \nL 80.531388 61.128909 \nL 80.836057 61.400426 \nL 81.140725 61.309065 \nL 82.05473 61.496421 \nL 82.359398 61.5319 \nL 82.664066 61.690743 \nL 82.968735 61.572589 \nL 83.578071 61.577144 \nL 83.88274 61.81412 \nL 84.492076 61.501329 \nL 84.796744 61.606609 \nL 85.101413 61.160933 \nL 86.015418 61.436799 \nL 86.320086 60.441207 \nL 86.624754 61.289015 \nL 86.929423 61.292711 \nL 87.234091 58.866036 \nL 87.538759 61.249787 \nL 87.843428 60.111335 \nL 88.148096 60.187257 \nL 88.452764 60.889661 \nL 89.062101 61.280427 \nL 89.366769 60.539607 \nL 89.671437 61.892733 \nL 89.976106 61.287137 \nL 90.280774 61.717363 \nL 90.585442 61.302496 \nL 90.890111 61.178827 \nL 91.194779 60.04206 \nL 91.804115 59.359674 \nL 92.108784 61.011239 \nL 92.71812 60.343478 \nL 93.022789 60.835203 \nL 93.327457 60.985546 \nL 93.632125 60.804337 \nL 93.936794 60.486539 \nL 94.241462 61.089075 \nL 94.54613 60.704407 \nL 94.850799 61.728039 \nL 95.155467 61.28689 \nL 95.460135 61.560403 \nL 95.764803 61.635819 \nL 96.37414 61.472866 \nL 96.678808 61.754509 \nL 97.897482 61.641916 \nL 98.20215 61.773797 \nL 98.506818 61.703085 \nL 98.811486 61.387256 \nL 99.116155 61.820862 \nL 99.420823 61.604071 \nL 99.725491 61.787348 \nL 100.03016 61.287321 \nL 100.639496 61.690892 \nL 101.248833 61.704959 \nL 101.553501 61.392622 \nL 101.85817 61.660526 \nL 102.162838 61.625133 \nL 102.467506 61.878156 \nL 102.772174 61.826514 \nL 103.076843 61.635488 \nL 103.381511 61.87684 \nL 103.990848 61.514677 \nL 104.295516 62.016525 \nL 104.904853 61.838201 \nL 105.209521 61.946951 \nL 105.514189 61.742233 \nL 106.123526 61.669801 \nL 106.428194 60.937448 \nL 106.732862 62.01729 \nL 107.037531 61.848323 \nL 107.342199 61.997443 \nL 107.646867 61.943522 \nL 107.951536 62.020297 \nL 108.256204 61.869606 \nL 108.560872 62.060334 \nL 108.865541 61.37706 \nL 109.170209 61.002825 \nL 109.474877 61.950439 \nL 110.084214 61.901594 \nL 110.388882 62.055054 \nL 110.998219 61.709663 \nL 111.302887 61.993413 \nL 111.607555 61.904911 \nL 112.52156 62.161621 \nL 112.826229 61.790274 \nL 113.130897 62.023351 \nL 113.435565 62.071085 \nL 113.740233 61.765397 \nL 114.044902 62.290261 \nL 114.34957 62.171884 \nL 114.958907 62.272526 \nL 115.568243 61.824206 \nL 115.872912 61.771214 \nL 116.17758 62.057956 \nL 116.482248 61.991401 \nL 116.786916 62.316097 \nL 117.091585 62.080246 \nL 117.700921 62.394574 \nL 118.310258 62.248527 \nL 118.614926 60.828683 \nL 118.919595 61.588406 \nL 119.224263 50.379035 \nL 119.528931 59.719788 \nL 119.8336 55.328581 \nL 120.138268 58.097672 \nL 120.442936 59.751647 \nL 120.747604 60.264588 \nL 121.052273 60.187182 \nL 121.356941 59.934277 \nL 121.661609 59.157166 \nL 121.966278 59.769842 \nL 122.270946 59.712827 \nL 122.575614 58.232987 \nL 122.880283 60.533983 \nL 123.489619 59.442071 \nL 123.794287 57.861136 \nL 124.098956 59.355663 \nL 124.403624 60.070518 \nL 124.708292 60.414639 \nL 125.012961 60.063408 \nL 125.317629 60.080806 \nL 125.622297 60.392278 \nL 125.926966 60.18698 \nL 126.231634 59.578412 \nL 126.536302 60.297798 \nL 126.840971 60.73072 \nL 127.450307 59.850555 \nL 127.754975 60.635313 \nL 128.364312 59.045486 \nL 128.973649 59.15071 \nL 129.278317 60.023008 \nL 129.582985 60.069224 \nL 129.887654 60.288333 \nL 130.192322 60.692324 \nL 130.801658 60.51426 \nL 131.106327 60.159802 \nL 131.410995 60.250403 \nL 131.715663 60.465587 \nL 132.020332 60.137236 \nL 132.325 60.262323 \nL 132.629668 58.462348 \nL 132.934337 60.268644 \nL 133.239005 60.874569 \nL 133.543673 60.418907 \nL 133.848342 59.351952 \nL 134.15301 60.344977 \nL 134.457678 60.315001 \nL 134.762346 60.706806 \nL 135.067015 60.728261 \nL 135.371683 60.388387 \nL 135.98102 60.36794 \nL 136.285688 60.70173 \nL 136.895025 60.880904 \nL 137.199693 59.654333 \nL 137.504361 59.074827 \nL 137.809029 59.86417 \nL 138.113698 59.477155 \nL 138.418366 59.355248 \nL 138.723034 58.569205 \nL 139.027703 59.369678 \nL 139.637039 60.410644 \nL 139.941708 60.075651 \nL 140.246376 60.529575 \nL 140.551044 59.238686 \nL 140.855713 60.465475 \nL 141.160381 60.527287 \nL 141.465049 58.974437 \nL 141.769717 59.12655 \nL 142.074386 60.737106 \nL 142.379054 60.765833 \nL 142.683722 60.253061 \nL 142.988391 60.310997 \nL 143.293059 60.48763 \nL 143.597727 60.305748 \nL 143.902396 60.48295 \nL 144.207064 60.882773 \nL 144.8164 60.187252 \nL 145.121069 60.294158 \nL 145.425737 58.565988 \nL 145.730405 59.795229 \nL 146.035074 60.433877 \nL 146.64441 60.745347 \nL 147.253747 60.388585 \nL 147.558415 60.060447 \nL 147.863084 60.481086 \nL 148.47242 60.702091 \nL 148.777088 60.530085 \nL 149.081757 60.593691 \nL 149.386425 59.625645 \nL 149.691093 60.792336 \nL 149.995762 60.658479 \nL 150.605098 60.875488 \nL 150.909767 60.618697 \nL 151.214435 60.883282 \nL 151.519103 59.896587 \nL 151.823771 59.713624 \nL 152.12844 60.357671 \nL 152.433108 59.588184 \nL 152.737776 60.039575 \nL 153.042445 58.743677 \nL 153.347113 60.487565 \nL 153.651781 60.535407 \nL 153.95645 60.387969 \nL 154.870455 60.61926 \nL 155.479791 60.446068 \nL 155.784459 60.414419 \nL 156.089128 60.665743 \nL 156.393796 60.448713 \nL 156.698464 60.499402 \nL 157.612469 60.375339 \nL 157.917138 60.655879 \nL 158.221806 60.604354 \nL 158.526474 60.205072 \nL 158.831143 60.507706 \nL 159.135811 60.125599 \nL 159.440479 60.453766 \nL 159.745147 60.183747 \nL 160.354484 60.689869 \nL 160.659152 59.842869 \nL 160.963821 60.648656 \nL 161.268489 59.647165 \nL 161.573157 60.652467 \nL 162.182494 60.312338 \nL 162.487162 60.567736 \nL 162.79183 60.284904 \nL 163.096499 60.657477 \nL 163.401167 60.524638 \nL 163.705835 60.607102 \nL 164.010504 60.15186 \nL 164.315172 60.362996 \nL 164.924509 60.451106 \nL 165.533845 60.675161 \nL 165.838514 60.567458 \nL 166.143182 60.635856 \nL 166.44785 60.49831 \nL 166.752518 60.618623 \nL 167.057187 60.535328 \nL 167.361855 60.585186 \nL 167.666523 60.370284 \nL 167.971192 60.618766 \nL 168.580528 60.364329 \nL 168.885197 60.666717 \nL 169.494533 59.888443 \nL 169.799201 60.343567 \nL 170.10387 60.488143 \nL 170.408538 60.374385 \nL 170.713206 60.672661 \nL 171.017875 60.444519 \nL 171.322543 60.665779 \nL 171.627211 60.590734 \nL 171.93188 60.243426 \nL 172.236548 60.248384 \nL 172.845885 60.517669 \nL 173.150553 60.552048 \nL 173.455221 60.456182 \nL 174.369226 60.658833 \nL 174.978563 60.427454 \nL 175.283231 60.295859 \nL 175.587899 60.566608 \nL 176.501904 60.674961 \nL 176.806572 60.422069 \nL 177.415909 60.635398 \nL 177.720577 60.564848 \nL 178.025246 60.618434 \nL 178.329914 60.397675 \nL 178.634582 60.632402 \nL 179.548587 60.466286 \nL 179.853256 60.625808 \nL 180.462592 60.617323 \nL 180.76726 60.72151 \nL 181.681265 60.580298 \nL 181.985934 60.686874 \nL 182.290602 60.639046 \nL 182.59527 60.328816 \nL 182.899939 60.374157 \nL 183.204607 60.678016 \nL 183.509275 60.430863 \nL 183.813943 60.609064 \nL 184.727948 60.548184 \nL 185.337285 60.620725 \nL 187.774631 60.603106 \nL 188.0793 60.66473 \nL 188.383968 60.497776 \nL 188.688636 60.660043 \nL 189.602641 60.499497 \nL 189.90731 60.659835 \nL 190.211978 60.586329 \nL 190.516646 60.683052 \nL 190.821314 60.533396 \nL 191.125983 60.636593 \nL 191.735319 60.544747 \nL 192.649324 60.662514 \nL 193.563329 60.573764 \nL 193.867998 60.492894 \nL 194.477334 60.68807 \nL 194.782002 60.582294 \nL 195.086671 60.674579 \nL 195.391339 60.635273 \nL 195.696007 60.477487 \nL 196.000676 60.624936 \nL 196.305344 60.55554 \nL 197.219349 60.590091 \nL 197.828686 60.673676 \nL 198.133354 60.505094 \nL 198.74269 60.656865 \nL 199.352027 60.351678 \nL 199.656695 60.689217 \nL 200.5707 60.701713 \nL 200.875369 60.615718 \nL 201.180037 60.671296 \nL 201.484705 60.448448 \nL 201.789373 60.659008 \nL 202.094042 60.499362 \nL 202.39871 60.656846 \nL 202.703378 60.50581 \nL 203.008047 60.650659 \nL 203.617383 60.663103 \nL 204.22672 60.602569 \nL 204.531388 60.484902 \nL 205.140725 60.62994 \nL 207.578071 60.663835 \nL 208.187408 60.638622 \nL 208.796744 60.651697 \nL 209.101413 60.478484 \nL 209.710749 60.604217 \nL 211.538759 60.601253 \nL 212.148096 60.686514 \nL 212.452764 60.590403 \nL 212.757432 60.716517 \nL 213.366769 60.648193 \nL 214.585442 60.64349 \nL 214.890111 60.57159 \nL 215.194779 60.646026 \nL 215.499447 60.563731 \nL 215.804115 60.69741 \nL 217.022789 60.609292 \nL 217.327457 60.715388 \nL 217.632125 60.563174 \nL 217.936794 60.704846 \nL 219.155467 60.783571 \nL 219.460135 60.826526 \nL 219.764803 60.078792 \nL 220.37414 60.232237 \nL 220.678808 60.210225 \nL 220.983477 58.579385 \nL 221.288145 59.920997 \nL 221.592813 58.196512 \nL 221.897482 58.242583 \nL 222.20215 60.126374 \nL 222.506818 59.842412 \nL 222.811486 59.045504 \nL 223.116155 60.265136 \nL 223.420823 60.630276 \nL 223.725491 60.217228 \nL 224.03016 60.613261 \nL 224.334828 60.22536 \nL 224.639496 60.679449 \nL 224.944165 60.400695 \nL 225.85817 60.652187 \nL 226.162838 60.481252 \nL 226.467506 60.568974 \nL 226.772174 60.46643 \nL 227.076843 60.68739 \nL 227.381511 59.815927 \nL 227.686179 60.733194 \nL 227.990848 60.516524 \nL 228.295516 60.632568 \nL 228.600184 60.548177 \nL 228.904853 60.629183 \nL 229.209521 60.347846 \nL 229.514189 60.577374 \nL 229.818857 60.497241 \nL 230.123526 60.604132 \nL 230.428194 60.521664 \nL 230.732862 60.651231 \nL 231.037531 60.576603 \nL 231.342199 60.630779 \nL 231.951536 60.412966 \nL 232.256204 60.63632 \nL 232.865541 60.611445 \nL 233.170209 60.363513 \nL 233.474877 60.494384 \nL 233.779545 60.365291 \nL 234.084214 60.642314 \nL 234.388882 60.65805 \nL 234.69355 60.509304 \nL 235.302887 60.651571 \nL 235.607555 60.527506 \nL 236.216892 60.640766 \nL 236.52156 60.505656 \nL 237.435565 60.689525 \nL 238.958907 60.599156 \nL 241.700921 60.6309 \nL 242.310258 60.579098 \nL 242.614926 60.717919 \nL 242.919595 60.360162 \nL 243.528931 60.673198 \nL 243.8336 60.602581 \nL 244.442936 60.625939 \nL 245.661609 60.676311 \nL 246.270946 60.624053 \nL 246.575614 60.722002 \nL 249.012961 60.665627 \nL 249.317629 60.692368 \nL 249.622297 60.598508 \nL 250.536302 60.662914 \nL 250.840971 60.570733 \nL 251.145639 60.664748 \nL 251.450307 60.607147 \nL 251.754975 60.684817 \nL 252.66898 60.562567 \nL 252.973649 60.674064 \nL 253.887654 60.577269 \nL 254.192322 60.461955 \nL 254.49699 59.726651 \nL 254.801658 59.29973 \nL 255.106327 59.494873 \nL 255.410995 54.775388 \nL 256.020332 60.583317 \nL 256.325 60.412251 \nL 256.629668 59.831165 \nL 256.934337 60.303247 \nL 257.543673 59.716519 \nL 257.848342 60.149821 \nL 258.15301 57.451346 \nL 258.457678 60.854963 \nL 258.762346 60.805077 \nL 259.067015 59.063198 \nL 259.98102 61.984476 \nL 260.285688 62.187652 \nL 260.895025 58.702106 \nL 261.199693 60.729894 \nL 261.504361 60.761475 \nL 261.809029 60.337832 \nL 262.113698 60.961853 \nL 262.723034 60.903061 \nL 263.027703 60.40612 \nL 263.332371 57.637062 \nL 263.637039 59.336631 \nL 263.941708 58.929836 \nL 264.246376 60.293981 \nL 264.551044 60.567555 \nL 264.855713 60.334276 \nL 265.465049 60.677079 \nL 265.769717 60.13613 \nL 266.379054 60.60044 \nL 266.683722 59.807513 \nL 266.988391 60.654589 \nL 267.293059 60.793699 \nL 267.597727 60.669842 \nL 268.207064 60.818276 \nL 268.8164 60.809147 \nL 269.121069 61.003985 \nL 269.425737 60.717768 \nL 269.730405 61.098397 \nL 270.035074 61.063719 \nL 270.64441 61.425508 \nL 270.949079 60.926114 \nL 271.253747 61.361551 \nL 271.558415 61.095525 \nL 271.863084 60.042819 \nL 272.167752 60.305872 \nL 272.47242 60.000379 \nL 272.777088 60.375298 \nL 273.386425 59.87438 \nL 273.691093 60.490702 \nL 273.995762 59.873759 \nL 274.30043 60.23016 \nL 274.605098 60.345151 \nL 275.214435 61.046344 \nL 275.823771 60.788538 \nL 276.12844 61.204535 \nL 276.433108 61.16336 \nL 277.042445 61.882566 \nL 277.347113 62.003582 \nL 277.95645 62.623795 \nL 278.261118 63.119525 \nL 278.565786 62.924543 \nL 279.175123 65.261427 \nL 279.479791 65.732541 \nL 279.784459 67.67872 \nL 280.089128 70.519779 \nL 280.393796 70.873698 \nL 281.003133 76.435511 \nL 281.307801 80.28779 \nL 281.612469 82.308357 \nL 281.917138 77.678853 \nL 282.221806 79.275921 \nL 283.440479 96.574801 \nL 283.745147 95.485328 \nL 284.659152 109.087475 \nL 284.963821 104.886856 \nL 285.268489 111.378891 \nL 285.573157 99.803516 \nL 286.182494 122.743239 \nL 286.487162 109.878468 \nL 286.79183 114.523827 \nL 287.705835 141.515331 \nL 288.010504 138.895485 \nL 288.315172 144.871037 \nL 288.61984 146.335971 \nL 288.924509 150.550277 \nL 289.533845 162.454988 \nL 290.143182 152.212097 \nL 290.44785 152.368253 \nL 290.752518 163.391378 \nL 291.057187 167.67689 \nL 291.361855 166.282675 \nL 291.666523 169.999519 \nL 291.971192 176.175809 \nL 292.27586 177.678657 \nL 292.885197 185.553746 \nL 293.494533 162.267691 \nL 294.408538 189.015003 \nL 294.713206 190.751117 \nL 295.017875 188.296251 \nL 295.322543 191.655543 \nL 295.627211 189.225361 \nL 295.93188 191.501292 \nL 296.236548 199.1179 \nL 296.541216 197.263626 \nL 296.845885 198.094652 \nL 297.150553 201.47942 \nL 297.455221 203.148079 \nL 297.759889 206.131938 \nL 298.064558 196.760099 \nL 298.369226 202.866526 \nL 298.673894 204.051092 \nL 298.978563 208.562453 \nL 299.283231 204.102586 \nL 299.587899 195.666133 \nL 299.892568 193.174456 \nL 300.501904 202.386136 \nL 300.806572 206.829982 \nL 301.111241 204.988453 \nL 301.720577 208.283982 \nL 302.025246 207.118263 \nL 302.329914 208.732536 \nL 302.634582 202.800461 \nL 302.939251 186.919739 \nL 303.243919 192.721861 \nL 303.548587 203.766325 \nL 303.853256 207.455613 \nL 304.157924 208.745879 \nL 304.462592 211.315534 \nL 304.76726 210.250798 \nL 305.681265 214.294043 \nL 306.290602 212.62289 \nL 306.59527 203.754816 \nL 306.899939 205.867092 \nL 307.204607 191.443834 \nL 307.509275 191.239332 \nL 307.813943 198.683763 \nL 308.118612 208.872839 \nL 308.42328 209.962985 \nL 308.727948 208.44562 \nL 309.032617 202.752208 \nL 309.337285 205.945719 \nL 309.641953 207.535899 \nL 309.946622 204.089809 \nL 310.25129 211.278778 \nL 310.555958 212.220788 \nL 310.860627 212.258994 \nL 311.165295 213.336364 \nL 311.469963 213.91011 \nL 311.774631 213.491957 \nL 312.0793 214.627203 \nL 312.383968 214.195093 \nL 312.688636 216.338209 \nL 312.993305 215.311014 \nL 313.297973 217.520547 \nL 313.602641 214.678396 \nL 314.211978 214.633569 \nL 314.516646 217.184932 \nL 314.821314 218.649767 \nL 315.125983 218.777031 \nL 315.430651 217.702126 \nL 315.735319 208.90378 \nL 316.039988 212.737526 \nL 316.344656 213.138843 \nL 316.649324 215.917479 \nL 316.953993 217.344985 \nL 317.563329 216.763132 \nL 317.867998 215.991704 \nL 318.172666 212.88016 \nL 318.477334 216.260315 \nL 318.782002 215.918351 \nL 319.391339 220.879595 \nL 319.696007 221.600664 \nL 320.000676 221.336136 \nL 320.305344 216.522932 \nL 320.610012 217.325767 \nL 320.914681 212.835225 \nL 321.219349 218.515951 \nL 321.524017 218.977928 \nL 321.828686 221.05076 \nL 322.133354 220.749159 \nL 322.438022 221.044747 \nL 322.74269 221.134161 \nL 323.047359 222.195804 \nL 323.352027 222.619176 \nL 323.656695 220.241694 \nL 324.266032 218.142537 \nL 324.5707 214.579067 \nL 324.875369 215.7274 \nL 325.180037 220.222697 \nL 325.484705 220.638663 \nL 325.789373 219.150686 \nL 326.39871 225.28457 \nL 326.703378 226.170271 \nL 327.008047 225.535777 \nL 327.312715 219.052566 \nL 327.617383 220.225894 \nL 327.922052 223.583659 \nL 328.22672 222.257004 \nL 328.531388 223.015552 \nL 328.836057 225.148362 \nL 329.140725 224.759857 \nL 329.445393 224.988865 \nL 329.750061 224.704407 \nL 330.05473 226.432463 \nL 330.359398 226.825644 \nL 330.664066 227.91408 \nL 330.968735 228.344705 \nL 331.273403 227.623732 \nL 331.578071 228.66924 \nL 331.88274 228.615965 \nL 332.187408 228.79327 \nL 332.492076 229.256437 \nL 332.796744 229.391081 \nL 333.710749 229.507531 \nL 334.320086 229.647327 \nL 336.452764 229.764489 \nL 342.850799 229.857179 \nL 346.506818 229.874489 \nL 346.506818 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 239.758125 \nL 26.925 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 361.725 239.758125 \nL 361.725 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 239.758125 \nL 361.725 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 22.318125 \nL 361.725 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_14\">\n    <!-- Loss -->\n    <g transform=\"translate(181.164375 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa6b3f65dff\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "with torch.no_grad():\n",
    "    X_tensor = torch.tensor(X_test, device='cuda:0').float()\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.long, device='cuda:0')\n",
    "            # 推論\n",
    "    y_pred= np.array(model(X_tensor).cpu()).argmax(axis=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.44339622641509435"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "y_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "import pickle\n",
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "model_path = \"../models/context/\"\n",
    "model_name = \"topic.pickle\"\n",
    "modelM = DataManager(model_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "modelM.save_data(model_name, model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success save : ../models/context/topic.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}