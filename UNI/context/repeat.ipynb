{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatools.analyzer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章ごとに n-gram を考えてみる\n",
    "def get_ngram_set(doc, N=3):\n",
    "    if isinstance(doc, str):\n",
    "        doc = nlp(doc)\n",
    "    surfaces = [token.text for token in doc]\n",
    "    ngram_set = set()\n",
    "    filled = [\"FOS\", *surfaces, \"EOS\"]\n",
    "    # print(filled)\n",
    "    for i in range(len(filled)-N+1):\n",
    "        f = \"_\".join(filled[i:i+N])\n",
    "        ngram_set.add(f)\n",
    "    return ngram_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repeat_rate(target:set, history:list, border=0.7):\n",
    "    t_list = list(target)\n",
    "    for prev_set in history:\n",
    "        size = len(prev_set)\n",
    "        hit = 0\n",
    "        for t in t_list:\n",
    "            if t in prev_set:\n",
    "                hit+=1\n",
    "        if hit/size >= border:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = []\n",
    "for conv in convs:\n",
    "    ngram_sets = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        y_pred.append(0)\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) <= 3:\n",
    "                # print(sent)\n",
    "                continue\n",
    "            ngram_set = get_ngram_set(sent, N=3)\n",
    "            # これまでのセットで重複が大きいものがあるかチェック！\n",
    "            if check_repeat_rate(target=ngram_set, history=ngram_sets, border=0.8):\n",
    "                # print(ut, ut.errors)\n",
    "                y_pred[-1] = 1\n",
    "            ngram_sets.append(ngram_set)\n",
    "        # break\n",
    "    # break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Repetition\"\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        # \n",
    "        if ut.is_error_included(error):\n",
    "            # print(ut.errors)\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[2129   15]\n",
      " [  26   30]]\n",
      "accuracy =  0.9813636363636363\n",
      "precision =  0.6666666666666666\n",
      "recall =  0.5357142857142857\n",
      "f1 score =  0.594059405940594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print('precision = ', precision_score(y_true=y, y_pred=y_pred))\n",
    "print('recall = ', recall_score(y_true=y, y_pred=y_pred))\n",
    "print('f1 score = ', f1_score(y_true=y, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:25<01:43, 25.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086956521739131 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:51<01:17, 25.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6126126126126126 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:09<00:00, 25.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epoch = 5\n",
    "\n",
    "border = 0.7\n",
    "f_score = 0\n",
    "gamma = 0.05\n",
    "results = []\n",
    "for _ in tqdm(range(epoch)):\n",
    "    y_pred = []\n",
    "    for conv in convs:\n",
    "        ngram_sets = []\n",
    "        for ut in conv:\n",
    "            if not ut.is_system():\n",
    "                continue\n",
    "            utt = ut.utt\n",
    "            doc = nlp(utt)\n",
    "            y_pred.append(0)\n",
    "            for sent in doc.sents:\n",
    "                if len(sent) <= 3:\n",
    "                    # print(sent)\n",
    "                    continue\n",
    "                ngram_set = get_ngram_set(sent, N=3)\n",
    "                # これまでのセットで重複が大きいものがあるかチェック！\n",
    "                if check_repeat_rate(target=ngram_set, history=ngram_sets, border=border):\n",
    "                    # print(ut, ut.errors)\n",
    "                    y_pred[-1] = 1\n",
    "                ngram_sets.append(ngram_set)\n",
    "            # break\n",
    "    this_f1 = metrics.f1_score(y, y_pred)\n",
    "    results.append([this_f1, border])\n",
    "    if this_f1 > f_score:\n",
    "        f_score = this_f1\n",
    "        border += gamma\n",
    "        print(f_score, border)\n",
    "    else:\n",
    "        border -= gamma\n",
    "\n",
    "# print(f_score, border)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本来エラーなのに非エラー扱い 1502868266\n",
      "['こんにちは。ちょうど退屈してたんだ。', 'こんにちは。元気ですか？', '元気ですかは元気です', '元気そうでよかった。', '元気ですかは元気ですね', 'いまなにしてたの？', 'もちろんですってのが元気ですかにいいですね', 'そうなんだ。どうして？', 'もちろんですってのが元気ですかにいいらしいですよ']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470624414\n",
      "['こんにちは！熱中症に気をつけて。', 'こんにちは。ありがとう、水分とるように気を付けてます。', '気をとってるんですね。熱中症に気をつけないんですか？', '大丈夫です。お仕事がお休みの日は何してるんですか？', '熱中症に気をつけたいのです', '水分とって、涼しくしてください。', '熱中症を引き起こします', '病院には\\u3000行きたくないです。', '病院は治療を受けましょう', '薬は\\u3000好きじゃありません。', '病院を終えるんですよねぇ', '車で帰りますよ。', '病院は有名ですね', '公園は好きですか？', '行くのがいいですね', '帽子を忘れないでくださいね', '帽子がほしいですね', '麦わら帽子はどうですか？', '帽子は欲しいですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1502868964\n",
      "['こんにちは。声かけてくれるのを待ってたんだ。', 'こんにちは。調子はどうですか？', '何か普段の生活で気をつけていることはありますか。', '健康には特に気を付けています。', '和食、洋食、中華どれが食べたいですか？', 'そうですねぇ。和食ですかね。', '和食ですか。和食でヘルシーですね', 'そうですね。あなたはどれが食べたいですか？', '和食でヘルシーです']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470624033\n",
      "['こんにちは。声かけてくれるのを待ってたんだ。', 'こんにちは。最近興味のあることってありますか？', '興味が湧いてないかもです', 'そうですかー。映画とか気になるのありません？', '映画が楽しみです', '今いろんな映画やっていますもんね', '映画は大丈夫です', '映画でポップコーン食べるのが好きです。', '映画は元気ですね', '私も元気な映画を見るのは好きですよ', '映画が楽しみですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470647225\n",
      "['こんばんは！海へ行きたいね。', '海は気持ちが良いですからね。', '海は楽しいですね', '海に行って何をしたら楽しいですか？', '海は大好きですね', '私も好きです。', 'ありがとう', '海は、ただ眺めてるだけでも良いですよね。', '海は好きですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470647225\n",
      "['こんばんは！海へ行きたいね。', '海は気持ちが良いですからね。', '海は楽しいですね', '海に行って何をしたら楽しいですか？', '海は大好きですね', '私も好きです。', 'ありがとう', '海は、ただ眺めてるだけでも良いですよね。', '海は好きですね', 'どのような海が好きですか？', '気持ちがえいですけどねぇ', '潮風に吹かれていると気持ちが良いですね。', '潮風が心地良いですね', '海に行く時は、帽子を忘れないでください。', '潮風は肌に感じます', '肌に感じる潮風と、潮の匂いが大好きです。', '潮風を感じられるのでしょうかね', '潮風を感じて、そのまま空を飛べる感じがします。', '肌に感じるんでしょうか']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470661075\n",
      "['こんばんはー。気軽に声かけてね。', '分かりました。', 'よろしい', '最近の調子はどうですか？', 'そういえば思い出したんですけど、遊園地にある大きな池は、防災も兼ねてるらしいですよ。', 'そういえば、聞いたことがあります。防災は大切ですよね。', '外で見ようですよ', '何か見るのですか？', '池は寒いです', '夏の時期は気持ちが良いかもしれません。', '池に住むのです', 'それは面白そうです。', '池はいいですね', 'ボートの使えるところもありますね。', '活性が高いですね', 'どのような活性ですか？', '活性は高いです']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470661075\n",
      "['こんばんはー。気軽に声かけてね。', '分かりました。', 'よろしい', '最近の調子はどうですか？', 'そういえば思い出したんですけど、遊園地にある大きな池は、防災も兼ねてるらしいですよ。', 'そういえば、聞いたことがあります。防災は大切ですよね。', '外で見ようですよ', '何か見るのですか？', '池は寒いです', '夏の時期は気持ちが良いかもしれません。', '池に住むのです', 'それは面白そうです。', '池はいいですね', 'ボートの使えるところもありますね。', '活性が高いですね', 'どのような活性ですか？', '活性は高いです', '活発なのはいいことです。', '活性が全体的に低いです', 'そういえば、家の近くでお祭りがありました。', '活性は高いですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1502868713\n",
      "['こんにちは。ちょうど退屈してたんだ。', 'こんにちは、今日も暑いですね。', '話は変わりますけど、１２月１日は映画の日だって、知ってました？', 'すみませ〜ん、知りません。', '得意料理はなんですか？', '餃子です。あなたの得意料理はなんですか？', '料理は不得手です', 'ところで好きな映画はなんですか？', '料理は下手です']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1502869481\n",
      "['こんにちは。夏といえばスイカだね。', 'スイカは好きですか？', 'スイカはうまいですね', 'はい美味しいですね。', 'スイカは食べるのがおいしいですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1502869481\n",
      "['こんにちは。夏といえばスイカだね。', 'スイカは好きですか？', 'スイカはうまいですね', 'はい美味しいですね。', 'スイカは食べるのがおいしいですね', '夏といえばスイカ以外に何を思い浮かべますか？', '家で食べます', '一人暮らしですか？', '一人暮らしは食生活が偏るのです', '自炊してますか？', '自炊は安上がりです', '安上がりですけど面倒ですよね？', '自炊はお金がかからないんですよねぇ']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1502869481\n",
      "['こんにちは。夏といえばスイカだね。', 'スイカは好きですか？', 'スイカはうまいですね', 'はい美味しいですね。', 'スイカは食べるのがおいしいですね', '夏といえばスイカ以外に何を思い浮かべますか？', '家で食べます', '一人暮らしですか？', '一人暮らしは食生活が偏るのです', '自炊してますか？', '自炊は安上がりです', '安上がりですけど面倒ですよね？', '自炊はお金がかからないんですよねぇ', '得意料理は何ですか？', 'お金がかかるのです', '明日の予定は？', 'お金はお財布に入れるかもしれない', '毎月貯金してますか？', '手を貯金に出しますねぇ', '明日の予定は？', '手を貯金に出します']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470623938\n",
      "['こんにちはー。ちょうど退屈してたんだ。', 'こんにちは。朝ごはんは何を食べた？', '朝ごはんが楽しみです', '君の好きな食べ物は？', '朝ごはんは一人で食べるのです', '私は家族で食べるよ。', 'おにぎりがいいですね', '自分で料理はする？', '料理は必要です', '料理するのは楽しいよね。', '料理は大切です', '昨日は何を食べた？', '料理に欠かせてないかも', '最近はゲームにはまっているんだ。', 'ゲームにはまってるんですね。料理は不得意です', '野球が好きだから、野球ゲームをやっているよ。', '野球は好きですか。料理は素晴らしいですね', '君の好きなスポーツは何？', '料理は素晴らしいですね', 'そうだね。今日はオリンピック観戦するよ。', '料理は大切ですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470661323\n",
      "['こんばんは。声かけてくれるのを待ってたんだ。', 'こんばんは。', 'こん', '今日は暑かったですね。', 'そういえば思い出したんですけど、遊園地にある大きな池は、防災も兼ねてるらしいですよ。', 'そうなんですか。それは知りませんでした。', '外で見ようですよ', '遊園地は好きですか', 'そうですね、子供の頃行きましたね。', 'ジェットコースターは苦手です', 'ジェットコースターに乗った直後のカタカタって昇っていく音がたまらなく好きなんだ。', 'ジェットコースターが落ちる時が怖いです', 'カタカタを使いますよねー', 'どんな乗り物が好きですか', '乗り物は自転車が好きですね', '私もよく乗ります', '乗り物は好きですね', 'サイクリングは楽しいです', 'サイクリングは楽しいんですか？？乗り物は大好きですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470623656\n",
      "['こんにちはー。夏といえばスイカだね。', 'こんにちは！そうですねー、かき氷もいいですよね', 'スイカで有名か？？', '有名ですよ', 'スイカで有名ですね', '糖度が高くて、甘くておいしいです', '糖度は高いんですか？？スイカは大好きですね', '大好きです。海は好きですか？', '海はサイコーですね', '泳ぐのも好きですか？', '泳ぐを覚えるのです', '暑いとき、どうしてますか？', '泳ぐを繰り返すのです', 'そうなんですね、山は好きですか？', '泳ぐを覚えたりとかします', '泳ぎ覚えるの大変ですか？', '泳ぐを覚えないんです', '最近、雨ばっかりですねー', '泳ぐを繰り返してるのですか？']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1502868703\n",
      "['こんにちは。夏といえばスイカだね。', 'スイカは美味しいですね', 'スイカで有名ですね', 'かき氷も美味しいですよね', 'スイカは面白いですね', '映画は好きですか?', '映画は、たまに観ますよ。', 'どんなジャンルが好きですか?', '邦画が好きですね', '私は洋画のほうが好きです', '洋画のほうは好きですか。邦画は好きですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1502868703\n",
      "['こんにちは。夏といえばスイカだね。', 'スイカは美味しいですね', 'スイカで有名ですね', 'かき氷も美味しいですよね', 'スイカは面白いですね', '映画は好きですか?', '映画は、たまに観ますよ。', 'どんなジャンルが好きですか?', '邦画が好きですね', '私は洋画のほうが好きです', '洋画のほうは好きですか。邦画は好きですね', '最近何か映画を見ましたか?', '邦画を好んだことありますね', '一人で見るほうが好きですか?', 'コメディが好きですね', '私も好きですよ', 'ありがとう', 'これからどちらに行く予定ですか?', 'どちらに行ってるんですね。コメディは好きですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470623865\n",
      "['こんにちは！熱中症に気をつけて。', '今日も暑いですからね', '予防が大切ですね', '何か熱中症の対策をしていますか？', '日頃から予防を心掛けるかもしれない', '私はいつも麦茶を用意していますよ。', '麦茶を用意するんですね。予防を怠ります', '麦茶以外にもミルクも良いそうです。', '予防が肝心ですね', '何事も用意しておいて損は有りませんからね。', '受けるのが効果的ですね', '寝不足にも注意が必要ですね。', '寝不足のときは、頭のてっぺんを軽く押すといいんだって。', '頭のてっぺんですか。一人でやっていると、何かのギャグみたいですが。', '寝不足のときは、頭のてっぺんを軽く押すといいんだって。', '私は、コメカミを押すと良いと聞きました。', '寝不足のときは、頭のてっぺんを軽く押すといいんだって。', '今度、試してみますね。', '寝不足で頭が重いですか', '今は大丈夫です。', '寝不足で頭が痛いですか']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470645949\n",
      "['こんばんは。ちょうど退屈してたんだ。', 'こんばんは。最近の調子はどうですか？', 'お仕事何かしてますか？', 'していますよ。', 'IDおしえてー', 'IDはまだ決めていません。', 'IDはいいですね', 'IDはいろいろなサービスで必要となっていますね。', 'IDに行きますよねえ', 'どこですか？', 'idに行きます']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470645949\n",
      "['こんばんは。ちょうど退屈してたんだ。', 'こんばんは。最近の調子はどうですか？', 'お仕事何かしてますか？', 'していますよ。', 'IDおしえてー', 'IDはまだ決めていません。', 'IDはいいですね', 'IDはいろいろなサービスで必要となっていますね。', 'IDに行きますよねえ', 'どこですか？', 'idに行きます', 'よく行かれるんですか？', 'iDに行きます']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470622856\n",
      "['こんにちはー。気軽に声かけてね。', 'こんにちは。', 'こんにちは。', 'そちらの天気はどんな感じですか。', '天気は恋しいです', '恋しい天気とはどんな天気なんでしょう。', '天気は怪しそうですよ', '怪しいんですか。雨が降りそうなのですか？', '出かけない日に限って、晴天になるよね。', 'そうですね。なぜかそうなりますね。', '出かけない日に限って、晴天になるよね。', '本当にそうですね。', '出かけない日に限って、晴天になるよね。', 'はい。何ででしょうか。', '出かけない日に限って、晴天になるよね。', 'どうしてだか分かりますか？', '天気は晴れが多いです', 'そうなのですか。こちらも最近は晴れの日が多いです。', '晴れの日は多いです']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1550628619\n",
      "['こんにちは。気軽に声かけてね。', 'よろしくお願いします。', 'おねがいします', '今日はいいお天気ですね。', 'なにおねがいするの？', '友達に遊びたいとお願いをします。あなたは今日は何をしたいですか？', '何がしたいのかポイントを絞ってから仕事をすると確実にこなせますよ。', '仕事をする上で大切ですね。あなたは何の仕事をしていますか？', '通勤がなければ、仕事も楽になるんだけどね。', '通勤は大変ですね。電車通勤ですか？', '通勤ラッシュが嫌で、始発に乗る人も多いんだって。', 'スッキリ目覚める秘訣が知りたいです。', 'そうなんですか。。。\\u3000そうだ、お酒はけっこう飲みますか？', 'いいえ、お酒は苦手です。あなたはどんなお酒が好きですか？', 'ご想像にお任せします。', 'お酒に合うおつまみは美味しいですよね。', 'おつまみは美味しいですね', '特に、しょっぱいものは食が進みます。', 'おつまみはおいしいですね']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1470622826\n",
      "['こんにちは。夏といえばスイカだね。', 'そうですね。でもスイカはあまり好きではないです。', 'スイカで有名か？？', '違います。', 'ご家族といっしょにお住まいですか?', 'いいえ。一人暮らしです。', 'ペットを飼うのです', 'ペットは飼っていません', 'ペットは室内で飼うのです']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1471927793\n",
      "['何かお話しましょうか？', 'こんにちは', 'こんにちは', '趣味は何ですか？', 'JR東日本と東京ディズニーシーなら、どちらに行ってみたいですか？', '東京ディズニーシーに行きたいです。', '東京ディズニーシーのトイ・ストーリー・マニア！はとても面白そうなアトラクションですね。', 'そうですね。乗ってみたいです。', 'トイ・ストーリー・マニア！は東京ディズニーシーにあるアトラクションです。']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1503291514\n",
      "['こんにちは。最近気になることとか、はまっていることとかありますか？', '歴史的な道に興味があります', 'あ、そうなんですか？例えばどんな感じか聞いてもいいです？', '古い昔の雰囲気が残っている道など、歩いてみたいですね', 'ロールして歩いてますよ。（ちなみに昨日も大須で愛犬とうろついてました。', '歩くのは楽しいですよね', 'まだ外にいけるだけ幸せですよね', 'そうですね', 'では、音楽はどんなのを聴かれていますか？', '最近はあまり音楽を聴かなくなってきました', 'なるほど。では音楽はどんなジャンルをよく聴きますか？']\n",
      "\n",
      "本来エラーなのに非エラー扱い 1471415566\n",
      "['こんにちは。よくチャットはやられるんですか？', 'こんにちは。やりますよ。', 'こんにちは。']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for conv in convs:\n",
    "    conv_list = []\n",
    "    for ut in conv:\n",
    "        conv_list.append(ut.utt)\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        # 本来エラーではないけどエラー扱い\n",
    "        # if y[i]==0 and y_pred[i]==1:\n",
    "        #     print(\"本来エラーではないけどエラー扱い\", ut.did)\n",
    "        #     print(conv_list)\n",
    "        #     print()\n",
    "        if y[i]==1 and y_pred[i]==0:\n",
    "            print(\"本来エラーなのに非エラー扱い\", ut.did)\n",
    "            print(conv_list)\n",
    "            print()\n",
    "        # elif y[i]==1 and y_pred[i]==1:\n",
    "        #     print(\"よく検出した！えらいぞ\", ut.did)\n",
    "        #     print(conv_list)\n",
    "        #     print()\n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def leven_prev_sim(target, history, border=0.7):\n",
    "    for text in history:\n",
    "        if border <  Levenshtein.ratio(text, target):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred2 = []\n",
    "for conv in convs:\n",
    "    # ngram_sets = []\n",
    "    history = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        y_pred2.append(0)\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) <= 3:\n",
    "                # print(sent)\n",
    "                continue\n",
    "            # ngram_set = get_ngram_set(sent, N=3)\n",
    "            target = sent.text\n",
    "            # これまでのセットで重複が大きいものがあるかチェック！\n",
    "            if leven_prev_sim(target, history, border=0.75):\n",
    "                # print(ut, ut.errors)\n",
    "                y_pred2[-1] = 1\n",
    "            history.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[2081   71]\n",
      " [  17   31]]\n",
      "accuracy =  0.96\n",
      "F-measure:  0.41333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred2))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred2))\n",
    "print('F-measure: ', metrics.f1_score(y, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Repetition\"\n",
    "import csv\n",
    "import Levenshtein\n",
    "import random\n",
    "def make_X_y_csv(filename=\"repetition.csv\"):\n",
    "    X = []\n",
    "    y = []\n",
    "    all_data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_ = csv.reader(f)\n",
    "        for d in data_:\n",
    "          all_data.append(d)\n",
    "    \n",
    "    for d in all_data:\n",
    "        y.append(int(d[0]))\n",
    "        hit = 0\n",
    "        leven_rate = Levenshtein.ratio(d[1], d[2])\n",
    "        ngram_set = get_ngram_set(d[2], N=3)\n",
    "        for ngram in get_ngram_set(d[1], N=3):\n",
    "            if ngram in ngram_set:\n",
    "                hit += 1\n",
    "        ngram_rate = hit/len(ngram_set)\n",
    "        X.append([ngram_rate, leven_rate])\n",
    "\n",
    "    u1_l = [d[1]  for d in all_data]\n",
    "    u2_l = [d[2]  for d in all_data]\n",
    "    for u1, u2 in zip( random.choices(u1_l, k=len(u1_l)), random.choices(u2_l, k=len(u2_l)) ):\n",
    "        X.append(make_feature(u1, u2))\n",
    "        y.append(0)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_ = make_X_y_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='sag')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2つを組み合わせてもいいかもしれないな！\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='sag', max_iter=1000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[12  0]\n",
      " [ 1 15]]\n",
      "accuracy =  0.9642857142857143\n",
      "precision =  1.0\n",
      "recall =  0.9375\n",
      "f1 score =  0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred_))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred_))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred_))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred_))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature(u1, u2):\n",
    "    leven_rate = Levenshtein.ratio(u1, u2)\n",
    "    ngram_set = get_ngram_set(u2, N=3)\n",
    "    hit = 0\n",
    "    for ngram in get_ngram_set(u1, N=3):\n",
    "        if ngram in ngram_set:\n",
    "            hit += 1\n",
    "    ngram_rate = hit/len(ngram_set)\n",
    "    return np.asarray([ngram_rate, leven_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = []\n",
    "for conv in convs:\n",
    "    ngram_sets = []\n",
    "    history = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        y_pred3.append(0)\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) <= 3:\n",
    "                # print(sent)\n",
    "                continue\n",
    "            ngram_set = get_ngram_set(sent, N=3)\n",
    "            target = sent.text\n",
    "            # これまでのセットで重複が大きいものがあるかチェック！\n",
    "            for ngram, text in zip(ngram_sets, history):\n",
    "                leven_rate = Levenshtein.ratio(target, text)\n",
    "                hit = 0\n",
    "                for s in ngram_set:\n",
    "                    if s in ngram:\n",
    "                        hit += 1\n",
    "                ngram_rate = hit/len(ngram)\n",
    "                x = np.asarray([ngram_rate, leven_rate])\n",
    "                # if lr.predict(x.reshape(1, -1))[0] == 1:\n",
    "                if ngram_rate>=0.8 or leven_rate>=0.9:\n",
    "                    y_pred3[-1] = 1\n",
    "                    break\n",
    "            history.append(target)\n",
    "            ngram_sets.append(ngram_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[2127   25]\n",
      " [  24   24]]\n",
      "accuracy =  0.9777272727272728\n",
      "precision =  0.4897959183673469\n",
      "recall =  0.5\n",
      "f1 score =  0.4948453608247423\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred3))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred3))\n",
    "print('precision = ', precision_score(y_true=y, y_pred=y_pred3))\n",
    "print('recall = ', recall_score(y_true=y, y_pred=y_pred3))\n",
    "print('f1 score = ', f1_score(y_true=y, y_pred=y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
