{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatools.analyzer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../hand_labeled/\"\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = read_conv(path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章ごとに n-gram を考えてみる\n",
    "def get_ngram_set(doc, N=3):\n",
    "    if isinstance(doc, str):\n",
    "        doc = nlp(doc)\n",
    "    surfaces = [token.text for token in doc]\n",
    "    ngram_set = set()\n",
    "    filled = [\"FOS\", *surfaces, \"EOS\"]\n",
    "    # print(filled)\n",
    "    for i in range(len(filled)-N+1):\n",
    "        f = \"_\".join(filled[i:i+N])\n",
    "        ngram_set.add(f)\n",
    "    return ngram_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repeat_rate(target:set, history:list, border=0.7):\n",
    "    t_list = list(target)\n",
    "    for prev_set in history:\n",
    "        size = len(prev_set)\n",
    "        hit = 0\n",
    "        for t in t_list:\n",
    "            if t in prev_set:\n",
    "                hit+=1\n",
    "        if hit/size >= border:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = []\n",
    "for conv in convs:\n",
    "    ngram_sets = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        y_pred.append(0)\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) <= 3:\n",
    "                # print(sent)\n",
    "                continue\n",
    "            ngram_set = get_ngram_set(sent, N=3)\n",
    "            # これまでのセットで重複が大きいものがあるかチェック！\n",
    "            if check_repeat_rate(target=ngram_set, history=ngram_sets[-5:], border=0.80):\n",
    "                # print(ut, ut.errors)\n",
    "                y_pred[-1] = 1\n",
    "            ngram_sets.append(ngram_set)\n",
    "        # break\n",
    "    # break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Repetition\"\n",
    "y = []\n",
    "for conv in convs:\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        # \n",
    "        if ut.is_error_included(error):\n",
    "            # print(ut.errors)\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[2134   18]\n",
      " [  29   19]]\n",
      "accuracy =  0.9786363636363636\n",
      "F-measure:  0.44705882352941173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred))\n",
    "print('F-measure: ', metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# epoch = 5\n",
    "\n",
    "# border = 0.7\n",
    "# f_score = 0\n",
    "# gamma = 0.05\n",
    "# results = []\n",
    "# for _ in tqdm(range(epoch)):\n",
    "#     y_pred = []\n",
    "#     for conv in convs:\n",
    "#         ngram_sets = []\n",
    "#         for ut in conv:\n",
    "#             if not ut.is_system():\n",
    "#                 continue\n",
    "#             utt = ut.utt\n",
    "#             doc = nlp(utt)\n",
    "#             y_pred.append(0)\n",
    "#             for sent in doc.sents:\n",
    "#                 if len(sent) <= 3:\n",
    "#                     # print(sent)\n",
    "#                     continue\n",
    "#                 ngram_set = get_ngram_set(sent, N=3)\n",
    "#                 # これまでのセットで重複が大きいものがあるかチェック！\n",
    "#                 if check_repeat_rate(target=ngram_set, history=ngram_sets, border=border):\n",
    "#                     # print(ut, ut.errors)\n",
    "#                     y_pred[-1] = 1\n",
    "#                 ngram_sets.append(ngram_set)\n",
    "#             # break\n",
    "#     this_f1 = metrics.f1_score(y, y_pred)\n",
    "#     results.append([this_f1, border])\n",
    "#     if this_f1 > f_score:\n",
    "#         f_score = this_f1\n",
    "#         border += gamma\n",
    "#     else:\n",
    "#         border -= gamma\n",
    "\n",
    "# print(f_score, border)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本来エラーではないけどエラー扱い 1470624021\n",
      "['こんにちは。声かけてくれるのを待ってたんだ。', 'こんにちは。元気ですね', '夕食は食べましたか？', 'はい。天丼を食べました。', 'ラーメンが食べたいです？', 'ラーメンは私も好きで食べたいですね', 'ラーメンはスープが美味しいですね', 'はい。どんなラーメンが好きですか？', 'ラーメンは美味しいですね', '焼きそばも好きです', '焼きそばは好きですか。麺が美味しいですね', 'そうですね。スポーツは好きですか？', '麺が美味しいですね']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1502868470\n",
      "['こんにちは！海へ行きたいね。', '海いいですね', '気持ちがいいですね', '海いいですね！大好きです。', '海は好きですね', 'どこの海がお勧めですか？', '海は楽しいですね', 'でも私は泳げないんです。', '海は大好きですね']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470624301\n",
      "['こんにちは！海へ行きたいね。', 'こんにちは！暑いから海行きたいね！', '海に行かないんですね？', '週末、行く予定だよ。', '海に行ってるんですね。海は一人で行ってます？', '家族と行く予定です。', '家族で買い物に行きます？', '買物はたいてい家族と一緒だね。', '家族で食事に行きますか？', 'たまに家族で外食に行くよ。', '家族でどこかに出かけます？', 'おばあちゃんちに行くつもりです。', '家族で食べに行きますか？']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623090\n",
      "['こんにちは。熱中症に気をつけて。', 'こんにちは。ホントに気をつけないとね。', '熱中症に気をつけないんですか？', '気をつけていますよ。水分を多めに摂っています。', '熱中症に気をつけてるんですね。熱中症に気をつけか？', '明日は猛暑らしいですから', '猛暑は欲しいですね', 'え〜欲しいの？わたしは要らないな', '猛暑は嬉しいですね', '暑いの好きなんですね', '猛暑は欲しいですよねー', '私は冷夏の方が嬉しいかな？', '冷夏が多いらしいですよ', '日本がですか？', '冷夏は野菜が高いです', 'ああ、そうですね。野菜が高くなると困りますもんね', '冷夏が多いです', '冷夏になると夏にまつわるものが売れませんしね', '冷夏で野菜が高いです']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1550628325\n",
      "['こんにちは。気軽に声かけてね。', 'こんにちは。今日はいい天気ですね。', '天気がいいとそれだけで元気が出ちゃうよね。', 'こんにちは。今日はいい天気ですね。', 'あ\\u3000そうなんですか。\\u3000うーんとね、猫を飼ったことはありますか？', '猫は飼っていますよ。あなたはどうですか？', 'ご想像にお任せします。', '猫は飼っていますよ。あなたはどうですか？', 'どうだと思いますか？', 'あなたは犬を飼っていると思います。', '人によく懐くから、ずーっと昔からペットとして飼われ続けているんだよ。', 'ところで、今日の昼ご飯は何を食べるつもりですか？', 'インターネットで調べたところ、もしかすると炊き込みご飯です。', 'お昼から炊き込みご飯を作るなんてえらいですね。', '日本では１９８４年に大学同士で接続されたのが始まりなんだ。', 'インターネットはいまではだれでも使っていますよね。', '日本の国花は桜だけど、法律で決められたものじゃないんだよ。', 'それもインターネットで調べた知識ですか？', '四季を感じるとき、やっぱり日本はいいなあって思うよね。', 'そうですね。あなたはどの季節が好きですか？', 'ご想像にお任せします。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623280\n",
      "['こんにちはー。海へ行きたいね。', '最近だといつ海に行った？', '海に行かないんですね？', 'いや、一週間前に言ったよ', '海は一人で行ってます？', '友達と行った。', '気持ちがいいですね', '天気も晴れていて清々しかったよ。', '天気は気持ちがいいですね']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470623280\n",
      "['こんにちはー。海へ行きたいね。', '最近だといつ海に行った？', '海に行かないんですね？', 'いや、一週間前に言ったよ', '海は一人で行ってます？', '友達と行った。', '気持ちがいいですね', '天気も晴れていて清々しかったよ。', '天気は気持ちがいいですね', '最近は暑すぎるけどね', '天気がホントに良いですね', '夏と冬どっちがいい？', '天気は晴れがいいですね', '寒いのは嫌い？', '晴れで気持ちがいいですね']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1470646042\n",
      "['こんばんは。海へ行きたいね。', '山でキャンプはどう？', 'キャンプに備えるのです', 'キャンプで食べるお肉はおいしいよね', '宮崎で張ります', '宮崎は自然が豊かだからね', '宮崎は温かいですね', 'マンゴーが有名ですよね', 'マンゴーは甘酸っぱいですよねー', 'マンゴー好き？', 'マンゴーは味が濃いですね', '果物は好きなんですか？', '果物は食べるのがいいですね', '毎朝食べていますよ', '果物は朝がいいですね', 'バナナは栄養の吸収が良いから朝食べると良いらしいですよ', '果物は生がいいですね']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471869912\n",
      "['何かお話しましょうか？', 'こんばんは', 'こんばんは', '暇ですか？', '奈良と京都なら、どちらが楽しいでしょうか？', '京都ですね', '京都には、嵐山と祇園と岸和田にあります。', '行きたいですね', '修学旅行で祇園と京都がセットになっていることはよくありますが、岸和田までセットになっているのは初めて聞きました。', '岸和田行ってみたいですね', '祇園限定のGREEN CAFE STYLE・ 茶乃逢のキャンディーであるモンスターズ・インク ライド＆ゴーシーク！なら、京都で有名な八つ橋味も面白いかなと思っていました。', 'とても詳しいですね', '祇園、京都府内で死亡した高齢男性の体内から相次いで青酸化合物が検出された連続変死事件で、岸和田の小籔千豊さんを殺害した容疑で逮捕された妻の中川礼二容疑者という人物がとても気になります。', '気になりますね', '漫才コンビであるGREEN CAFE STYLE・ 茶乃逢の芸人佐野史郎さんは駅員さんのコスプレをして鉄道のアナウンスのものまねをしている時もありますよね。', '知りませんでした', '漫才コンビであるGREEN CAFE STYLE・ 茶乃逢の竹野内豊さんが鉄道員のものまねをしているときに着用している制服がTBSの制服かどうかまではわかりませんが、他にも中川家のキャンペーンイベントでも中川家の駅員姿で登場したことがあるようです。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1503631674\n",
      "['こんにちは！有名人でざわちんと浦沢直樹なら、どちらに興味がありますか？', 'どちらかというとざわちんかな。\\u3000モノマネメイクしてみたくならない？', 'ざわちんさんのものまねメイクに似たようなことを、以前板野友美さんが、「ホンマでっか！？TV」 という本のなかでしていましたが、それはかなりお笑い寄りだったので、一般人が取り入れようという感じではなかったですしね。', 'そうかな？滝川クリステルとかすごい似てるよ。ものまねもいいけどメイク自体楽しいよね', '時々見かけるフジテレビのスペシャルドラマ痛快！明石家電視台シリーズで女優ざわちんさんが演じている板野友美の役もハマっていて面白いと思いました。', '他にはどんなジャンルのドラマみるの？', 'フジテレビの番組であるBISTRO SMAPの世界行ってみたらホントはこんなトコだった！？のコーナーにも海外の有名人が多くゲスト出演されますが、ゲストの素の部分が見えて、とても楽しんでみることができます。', 'どっきりが好きならYouTubeの動画もおすすめだよ。', 'YouTubeでの動画公開は今後も色々なジャンルの人の参入がどんどん増えていきそうですね。', 'そうだね。HIKAKINのどうかは見たことある？', '海外や外国人にフォーカスをあてた番組として、フジテレビのYouTubeや、→Pia-no-jaC←の世界行ってみたらホントはこんなトコだった！？などありますね。', '一昔前はニコニコ動画が人気だったね。ニコニコ生放送も盛り上がってたね', 'ニコニコ動画で動画を公開してみたいと思ったことはありますか？ 。', 'あるけど、私はニコニコ生放送をやっていたよ。', '私としては同じ→Pia-no-jaC←だと、フジテレビのヒカキンさんが、テレビ番組のレギュラーとしてよく出演しているイメージがあります。', 'テレビといったらヒカルが出てた番組みた？', '→Pia-no-jaC←系列のテレビ番組であるニコニコ動画は毎週見逃さないように録画しています。', '熱心だね。2重録画できるレコーダーがおすすめだよ', '→Pia-no-jaC←のスポーツ番組であるニコニコ動画で初めて元陸上競技選手のあばれるくんさんが登場した時の放送をたまたま見ましたが、とてもわくわくしました。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471923455\n",
      "['何かお話しましょうか？', 'こんにちは', 'こんにちは', '暑いですね', '千原ジュニアと三浦知良なら、どちらに関心がありますか？', '三浦知良ですね', 'サッカーの三浦知良選手がテレビで話した、北澤豪選手との話とはどのような話でしたか？。', '見ていないのでわからないです', 'サッカーが好きですので、ミラノでこれだけサッカーがメジャーになったのは佐村河内守選手がカズダンスなど一般でも話題になるようなネタを提供してくれたからだと思っています。', 'そうですね', 'リオネル・メッシ選手とクリスティアーノ・ロナウド選手の二人は、1998年にFIFAワールドカップのメンバーから落選した後、ミラノに行って買い物したり、ご飯を食べにいったりして、そこでクリスティアーノ・ロナウド選手は突発的に金髪に染めて帰国されたという話などです。', '面白い話ですね', 'ミラノ代表がポルトガルワールドカップで優勝できなかったのも、やはりマラドーナ選手の個の力だけでは限界があるというのを示したと私は思っています。', '私もそう思います', '田中マルクス闘莉王選手は、これまで誰よりもミラノのサッカーに貢献してきたのに、ワールドカップに一度も出場した経験がないというのが、とても寂しいですね。', 'それは悲しいですね', 'サッカーが好きですので、ミラノでこれだけサッカーがメジャーになったのはクリスティアーノ・ロナウド選手がカズダンスなど一般でも話題になるようなネタを提供してくれたからだと思っています。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471917180\n",
      "['何かお話しましょうか？', 'こんにちは', 'こんにちは', '昨日は台風凄かったですね', 'JR東日本と愛媛地方の四国でサイクリングをしたことがあります。', 'サイクリング気持ちよさそうですね', 'フタバ図書が販売しているJR東日本限定の台風19号は、四国産しらすを使用したおとなのふりかけ・愛媛みかん、愛媛みかん果汁を使用した「冷え知らず」さんの生姜チャイ、中国産かつおを使用したおとなのふりかけ、しまなみ海道名物のラーメンをふりかけにしたおとなのふりかけ・ちりめんじゃこの詰め合わせ4種類が入っているみたいですね。', 'いろんなふりかけがあるんですね。私は大の納豆好きなんですよ。納豆はお好きですか？', 'ユニクロが販売しているJR東日本限定の「冷え知らず」さんの生姜チャイは、しまなみ海道産しらすを使用した台風19号、四国みかん果汁を使用したおとなのふりかけ・愛媛みかん、愛媛産かつおを使用したおとなのふりかけ、ローソン・フタバ図書GIGA広島駅前店名物のラーメンをふりかけにしたおとなのふりかけ・ちりめんじゃこの詰め合わせ4種類が入っているみたいですね。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471917413\n",
      "['何かお話しましょうか？', '今日は雨が降らないといいですね。', '東京と瀬戸内なら、どちらに行ってみたいですか？', '瀬戸内に行ってみたいです。', '瀬戸内は、Ballad Showのキャンペーンからセックス・アンド・ザ・シティのキャンペーンで観光を強化していて、かなり他府県から評価が上がっているそうです。', 'そうなんですね。観光旅行は好きですか？', 'いかめしをそれほど熱心に見ていたわけではないので、カップケーキのことは良く知らないのですが、瀬戸内よりもカラフルなカップケーキがあることは知っています。', 'カラフルなカップケーキはどこで売っていますか？', '泣ける！広島県をそれほど熱心に見ていたわけではないので、カップケーキのことは良く知らないのですが、瀬戸内よりもカラフルなカップケーキがあることは知っています。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471927106\n",
      "['何かお話しましょうか？', '好きな食べ物は何？', 'TBSで放送されていたずっとあなたが好きだったというドラマといえば、俳優の冬彦さんさんが女優の桂田冬彦さんと交際のきっかけとなったテレビドラマですね。', 'そうなの？ぜんぜん知らなかった', '最近、テレビドラマを見る事がほとんどなくなったので、残念ながら、TBSで放送されていたずっとあなたが好きだったというドラマと、京都雲月で放送されていた噂の現場直行ドキュメン・ガンミ!!というドラマは見たことがありません。', 'じゃあ半沢直樹見たことある？', '私が俳優の小籔千豊さんが出演している作品で好きなものは、TBSで放送されていた半沢直樹というドラマと、京都雲月で放送されていたずっとあなたが好きだったというドラマです。', '小籔千豊は俳優じゃないよ', '俳優の小籔千豊さんといえば、22年前のTBSのドラマのキベの部屋で、宮川大輔の役が印象的でしたね。', 'みんな芸人だよ', '俳優の小籔千豊さんといえば、TBSのドラマで放送された珍獣ハンターイモトの宮川大輔役のイメージが強く、その役に成りきられるイメージが強いです。', 'バラエティー見るの？', '俳優の小籔千豊さんといえば、TBSのドラマで放送されたWOWOWの宮川大輔役のイメージが強く、その役に成りきられるイメージが強いです。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471927106\n",
      "['何かお話しましょうか？', '好きな食べ物は何？', 'TBSで放送されていたずっとあなたが好きだったというドラマといえば、俳優の冬彦さんさんが女優の桂田冬彦さんと交際のきっかけとなったテレビドラマですね。', 'そうなの？ぜんぜん知らなかった', '最近、テレビドラマを見る事がほとんどなくなったので、残念ながら、TBSで放送されていたずっとあなたが好きだったというドラマと、京都雲月で放送されていた噂の現場直行ドキュメン・ガンミ!!というドラマは見たことがありません。', 'じゃあ半沢直樹見たことある？', '私が俳優の小籔千豊さんが出演している作品で好きなものは、TBSで放送されていた半沢直樹というドラマと、京都雲月で放送されていたずっとあなたが好きだったというドラマです。', '小籔千豊は俳優じゃないよ', '俳優の小籔千豊さんといえば、22年前のTBSのドラマのキベの部屋で、宮川大輔の役が印象的でしたね。', 'みんな芸人だよ', '俳優の小籔千豊さんといえば、TBSのドラマで放送された珍獣ハンターイモトの宮川大輔役のイメージが強く、その役に成りきられるイメージが強いです。', 'バラエティー見るの？', '俳優の小籔千豊さんといえば、TBSのドラマで放送されたWOWOWの宮川大輔役のイメージが強く、その役に成りきられるイメージが強いです。', '好きな食べ物は何？', '女優小泉八雲さんが主演された、TBSで放送されたテレビドラマのWOWOWも好きでしたよ。', 'ドラマばっかだね', 'TBSのドラマのWOWOWでは、倉科カナというマザコン男性が、1992年の社会現象となりましたね。', 'それは冬彦さんだよ', '現在テニスの冬彦さん選手は、世界ランキング5位で、私は冬彦さん選手の試合をテレビで見たいのですが、テレビでは有料放送のTBSに契約しないと見られないんでしたよね。あとはインターネットの動画でテニスの試合を見る方法もあるそうですが。', 'それは錦織だよ', 'TBSで放送されていた噂の現場直行ドキュメン・ガンミ!!というドラマといえば、俳優の冬彦さんさんが女優の倉科カナさんと交際のきっかけとなったテレビドラマですね。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471928252\n",
      "['何かお話しましょうか？', 'ぜひ。好きな食べ物の話なんてどうですか？', 'TBSで放送されていたずっとあなたが好きだったというドラマといえば、俳優の冬彦さんさんが女優の桂田冬彦さんと交際のきっかけとなったテレビドラマですね。', 'だいぶ昔のドラマですね。佐野史郎が怖かった！', 'TBSで放送されていたずっとあなたが好きだったというドラマには、佐野史郎さんも出演していましたね。ドラマの中で佐野史郎さんは冬彦さんさんに密かに恋心を抱いている、という設定でした。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471916240\n",
      "['何かお話しましょうか？', '台風のシーズンですね', 'あなたの住んでいるJR東日本には台風19号は配置されていませんが、近くの愛媛には配置されているみたいです。', '今年は台風が多そうですが，あなたの住んでいる地域はどうですか', '焼肉でんが販売しているJR東日本限定の台風19号は、愛媛産しらすを使用したおとなのふりかけ・愛媛みかん、四国みかん果汁を使用した「冷え知らず」さんの生姜チャイ、香川県産かつおを使用したおとなのふりかけ・ちりめんじゃこ、徳島名物のラーメンをふりかけにしたおとなのふりかけ・徳島ラーメン風の詰め合わせ4種類が入っているみたいですね。', '夏は麺類が欲しくなりますね', 'チャーリーズが販売しているJR東日本限定の「冷え知らず」さんの生姜チャイは、愛媛産しらすを使用した台風19号、四国みかん果汁を使用したおとなのふりかけ・愛媛みかん、香川県産かつおを使用したおとなのふりかけ・ちりめんじゃこ、徳島名物のラーメンをふりかけにしたおとなのふりかけ・徳島ラーメン風の詰め合わせ4種類が入っているみたいですね。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471916240\n",
      "['何かお話しましょうか？', '台風のシーズンですね', 'あなたの住んでいるJR東日本には台風19号は配置されていませんが、近くの愛媛には配置されているみたいです。', '今年は台風が多そうですが，あなたの住んでいる地域はどうですか', '焼肉でんが販売しているJR東日本限定の台風19号は、愛媛産しらすを使用したおとなのふりかけ・愛媛みかん、四国みかん果汁を使用した「冷え知らず」さんの生姜チャイ、香川県産かつおを使用したおとなのふりかけ・ちりめんじゃこ、徳島名物のラーメンをふりかけにしたおとなのふりかけ・徳島ラーメン風の詰め合わせ4種類が入っているみたいですね。', '夏は麺類が欲しくなりますね', 'チャーリーズが販売しているJR東日本限定の「冷え知らず」さんの生姜チャイは、愛媛産しらすを使用した台風19号、四国みかん果汁を使用したおとなのふりかけ・愛媛みかん、香川県産かつおを使用したおとなのふりかけ・ちりめんじゃこ、徳島名物のラーメンをふりかけにしたおとなのふりかけ・徳島ラーメン風の詰め合わせ4種類が入っているみたいですね。', '「冷え知らず」さんの生姜チャイ飲んだことある！不味いですよ！好きですか？', 'JR東日本のスポーツオーソリティというメーカーの清涼飲料水を飲んだことがないのですが、どのようなところがおすすめですか？ 。', '私も知りません。アクエリアスウォーターは好きです', 'チャーリーズで売っているJR東日本産のJR西日本というメーカーの清涼飲料水です。', '最近は特保コーラにハマってます。特保飲料は飲みますか', '私がチャーリーズというメーカーの清涼飲料水を買うのは、以前JR東日本へ行ったことがあるので、JR東日本産のものを買いたくなってしまう、ということくらいで選んでいるんです。', 'ご当地サイダーが流行っているようですが，知ってました？', 'インターネットでみつけたJR東日本あたりでのクリスマスに登場するイエス・キリストの写真によると、赤色のコスチュームではありますが、半袖、半ズボンといったまさしく夏の服装でした。', 'それじゃあまるで南半球のサンタですね', '絵本かと思いきや、ドールハウスのようなサンタクロースのお家が出現するとは、飛び出す絵本のサンタのクリスマスハウスを初めて見た人は子供も大人もかなり驚いて嬉しい気持ちになりそうですね。', '面白そう！そんなサプライズ経験はありますか', '私が日本穀物検定協会というメーカーの清涼飲料水を買うのは、以前JR東日本へ行ったことがあるので、JR東日本産のものを買いたくなってしまう、ということくらいで選んでいるんです。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471927839\n",
      "['何かお話しましょうか？', '宮部みゆきなんてどう？', '宮部みゆきは「ぼんくら」シリーズがとても好きです。10月からはテレビドラマ化もされるようなので、ファンとしては少し楽しみでもあり不安でもあります。', '今年の10月から？', '宮部みゆきさんのタマシイレボリューションシリーズでは、体の弱い廻船問屋の若だんなが主人公なのですが、あやかしと呼ばれる妖怪達と話せる能力を持っています。', 'へえ〜。村上春樹は好き？', '村上春樹さん原作のレベル7を宮部みゆき監督が撮影した映画は残念に思いました。', 'そんな映画があったんだ。映画は好き？', '村上春樹のミステリー小説が好きなのですが、その中ではぼんくらがとても感動した覚えがあります。', '感動すると泣いたりする？', 'スタジオジブリも世代交代のようなので、1億人の大質問！？笑ってコラえて！の続編は難しいのかもしれないですね。', '笑ってこらえてってジブリだったんだ。トトロでしょ？', 'ジブリの作品は子供から大人まで楽しめますね。ごぶごぶが乗るメーヴェにとても憧れました。', 'リオの閉会式良かったよね', 'ジブリの作品は子供から大人まで楽しめますね。風立ちぬが乗るメーヴェにとても憧れました。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471917265\n",
      "['何かお話しましょうか？', 'そうですね、今日も暑いですね。', 'ざわちんと田中マルクス闘莉王なら、どちらに関心がありますか？', 'ざわちんですね。', 'ざわちんさんの出演されたハングリー!はどのような内容でしたか？。', 'たしかフランス料理のお店の話でしたね。', 'Get Your Guideでは、食べ物のおいしさが伝わるように描くため、フランス人とボルドー人のシェフたちからアドバイスを受けて、撮影方法などにもこだわっているそうです。', 'すごい、よく知っていますね！', 'マクドナルドのGet Your Guideでは、小麦粉と水とドライイーストと塩のみで作るボルドーパンがとても美味しいです。', 'フランス料理っぽくておいしそうですね。', 'テレビ朝日のLet It Goで作ったフランスパンはそんなに固くないと思います。', '固くないほうが食べやすくていいですよね。', '2015年1月からテレビ朝日系列で放送される、マツコの知らない世界で新聞記者役を演じるそうなのですが、これで24年連続テレビドラマで主役を続けているということになるそうです。', 'マツコさんは好きなので見たい番組です。', 'マツコ・デラックスが出ている番組だと、他には、アムゼルケラーをたまに観ますね。', 'どんな内容ですか。', 'マツコ・デラックスは人気ですね。他には、ナインティナインのマツコデラックスと共演しているモヤモヤさまぁ〜ずという番組もあります。', 'そうでしたっけ。バラエティ番組は見ていて楽しいから好きなんですよ。', 'ホンマでっか！？TVは、マツコ・デラックスさんと、マツコデラックスさんの掛け合いが面白いテレビ番組ですね。', 'あれ本当に面白いですよね！私もよく見ますよ。', '私がマツコ・デラックスさんが出演するテレビ番組で一番好きなのは、望月製紙の番組であるホンマでっか！？TVです。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1550565978\n",
      "['こんにちは。あなたはいつも家では何をしてますか？', 'こんにちは。いつも家では仕事をしています。', '政治家になりたいです．', 'どんな政治家ですか？', 'ほんとに．失言が多いもんね．', '政治家の失言は多いですね', 'ほんとに．失言が多いもんね．']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471415606\n",
      "['こんにちは。最近気になることとか、はまっていることとかありますか？', 'オリンピックの結果です。', 'そうですね。結果だけ聞くよりもどうなるかワクワクしながら観る方が面白いと思います。', 'はい、卓球の放送は見たいです。', 'フィギアは人気がありますね。あなたはスポーツはしておられないんですよね？', 'はい', '木の素材とか、大量生産してるとか、手作りとか・・そういうので全然値段が変わってきますからね。', 'そうですね。フィギアは好きですか？', '見ましたよ\\u3000荒川が完璧な演技でこのまま行けばメダル取れそうですね！安藤美姫が地元なのでおうえんして。', 'それはフィギュアでスケートですね', 'やはり女子に期待ですか？', '今度は韓国ですね。', 'はい。何度かあります。近いし、安いし、食べ物はうまいし本当に良いところですよ。', '韓国料理で何か好きなものはありますか？', '結構好き嫌いなくなんでも食べますよ。', '私はキムチが好きです。', 'やはり辛いのですか。以前見た本場、韓国のキムチ作りを取り上げた番組で、韓国のおばちゃんは白菜を洗濯機で洗う荒技を披露していましたが、考えてみれば日本の電動餅つき器もかなり異質なマシーンですね。', 'そうですね。料理の機械は色々ありますね。', 'そうですね。洋食系ではカレーライスや、オムライス、スパゲッティ、ハンバーグとかが中心で、和食だと、煮魚や煮物が中心です。']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471416039\n",
      "['こんにちは。好きな動物は何ですか？', 'ネコが好きです。', 'かわいいですよね', 'そうですね、ネコの鳴き声が特に好きですね。', 'かわいいですよね']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471414705\n",
      "['こんにちは。好きな動物は何ですか？', 'ネコが好きです', 'かわいいですよね', 'あなたの好きな動物は？', 'ペットは飼ってませんが動物は好きです。', 'どんな動物が好きなんですか？', 'ペットは飼ってませんが動物は好きです。', 'よく動物園に行ったりするんですか？', '水族館だね。', '魚が好きなんですか', 'ブラジル系。なんででしょうね。', 'ブラジル系って、例えば？', '海水浴です。魚も見えましたよ', '今年は海水浴に行きましたか？', '何故かブラジル系のひとが多くて本当に海外に来たみたいでした。', '何だか不思議ですね', '何がですか？', 'ブラジル系の人が多かったことがです', '海水浴です。魚も見えましたよ']\n",
      "\n",
      "本来エラーではないけどエラー扱い 1471415547\n",
      "['こんにちは。よくチャットはやられるんですか？', 'あまりしません', 'その人の好きなジャンルにもよってですね。', 'それはありますね。', 'では、あなたはいつも家では何をしてますか？', 'パソコンでインターネットを見ています。', 'きれいな花を見つけたら、写真も撮るんですか？', '花はあまり好きではありません。', 'へ〜、いいですね。自分のやってみたいとは思うのですが、なかなか見つかりません。', 'いいですか？そうは思いません。', 'でもアニメと漫画は全然ストーリーが違いますけどね。', '違いますね。漫画のほうが好きです。', 'おかげで気づいたら漫画の本だけでもすでに部屋に１０００冊以上（２０００行ってるかな？）になってしまっています。', 'それはすごい！', 'どんな会話してるんでしょうね。', '漫画の会話ですよ。', 'ですよね。本当に同感です。こんな体験めったにできないので、なかなかいいのではないかと思いました。', '確かにそう思います。', 'そうですね。産地偽装はよくない行為ですね。', '産地偽装はダメです。', 'そうですね。産地偽装はよくない行為ですね。']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for conv in convs:\n",
    "    conv_list = []\n",
    "    for ut in conv:\n",
    "        conv_list.append(ut.utt)\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        # 本来エラーではないけどエラー扱い\n",
    "        if y[i]==0 and y_pred3[i]==1:\n",
    "            print(\"本来エラーではないけどエラー扱い\", ut.did)\n",
    "            print(conv_list)\n",
    "            print()\n",
    "        # if y[i]==1 and y_pred[i]==0:\n",
    "        #     print(\"本来エラーなのに非エラー扱い\", ut.did)\n",
    "        #     print(conv_list)\n",
    "        #     print()\n",
    "        # elif y[i]==1 and y_pred[i]==1:\n",
    "        #     print(\"よく検出した！えらいぞ\", ut.did)\n",
    "        #     print(conv_list)\n",
    "        #     print()\n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def leven_prev_sim(target, history, border=0.7):\n",
    "    for text in history:\n",
    "        if border <  Levenshtein.ratio(text, target):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred2 = []\n",
    "for conv in convs:\n",
    "    # ngram_sets = []\n",
    "    history = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        y_pred2.append(0)\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) <= 3:\n",
    "                # print(sent)\n",
    "                continue\n",
    "            # ngram_set = get_ngram_set(sent, N=3)\n",
    "            target = sent.text\n",
    "            # これまでのセットで重複が大きいものがあるかチェック！\n",
    "            if leven_prev_sim(target, history, border=0.75):\n",
    "                # print(ut, ut.errors)\n",
    "                y_pred2[-1] = 1\n",
    "            history.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[2081   71]\n",
      " [  17   31]]\n",
      "accuracy =  0.96\n",
      "F-measure:  0.41333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred2))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred2))\n",
    "print('F-measure: ', metrics.f1_score(y, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = \"Repetition\"\n",
    "import csv\n",
    "import Levenshtein\n",
    "import random\n",
    "def make_X_y_csv(filename=\"repetition.csv\"):\n",
    "    X = []\n",
    "    y = []\n",
    "    all_data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        data_ = csv.reader(f)\n",
    "        for d in data_:\n",
    "          all_data.append(d)\n",
    "    \n",
    "    for d in all_data:\n",
    "        y.append(int(d[0]))\n",
    "        hit = 0\n",
    "        leven_rate = Levenshtein.ratio(d[1], d[2])\n",
    "        ngram_set = get_ngram_set(d[2], N=3)\n",
    "        for ngram in get_ngram_set(d[1], N=3):\n",
    "            if ngram in ngram_set:\n",
    "                hit += 1\n",
    "        ngram_rate = hit/len(ngram_set)\n",
    "        X.append([ngram_rate, leven_rate])\n",
    "\n",
    "    u1_l = [d[1]  for d in all_data]\n",
    "    u2_l = [d[2]  for d in all_data]\n",
    "    for u1, u2 in zip( random.choices(u1_l, k=len(u1_l)), random.choices(u2_l, k=len(u2_l)) ):\n",
    "        X.append(make_feature(u1, u2))\n",
    "        y.append(0)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_ = make_X_y_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='sag')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2つを組み合わせてもいいかもしれないな！\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='sag', max_iter=1000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[12  0]\n",
      " [ 1 15]]\n",
      "accuracy =  0.9642857142857143\n",
      "precision =  1.0\n",
      "recall =  0.9375\n",
      "f1 score =  0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred_))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred_))\n",
    "print('precision = ', precision_score(y_true=y_test, y_pred=y_pred_))\n",
    "print('recall = ', recall_score(y_true=y_test, y_pred=y_pred_))\n",
    "print('f1 score = ', f1_score(y_true=y_test, y_pred=y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature(u1, u2):\n",
    "    leven_rate = Levenshtein.ratio(u1, u2)\n",
    "    ngram_set = get_ngram_set(u2, N=3)\n",
    "    hit = 0\n",
    "    for ngram in get_ngram_set(u1, N=3):\n",
    "        if ngram in ngram_set:\n",
    "            hit += 1\n",
    "    ngram_rate = hit/len(ngram_set)\n",
    "    return np.asarray([ngram_rate, leven_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = []\n",
    "for conv in convs:\n",
    "    ngram_sets = []\n",
    "    history = []\n",
    "    for ut in conv:\n",
    "        if not ut.is_system():\n",
    "            continue\n",
    "        utt = ut.utt\n",
    "        doc = nlp(utt)\n",
    "        y_pred3.append(0)\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) <= 3:\n",
    "                # print(sent)\n",
    "                continue\n",
    "            ngram_set = get_ngram_set(sent, N=3)\n",
    "            target = sent.text\n",
    "            # これまでのセットで重複が大きいものがあるかチェック！\n",
    "            for ngram, text in zip(ngram_sets, history):\n",
    "                leven_rate = Levenshtein.ratio(target, text)\n",
    "                hit = 0\n",
    "                for s in ngram_set:\n",
    "                    if s in ngram:\n",
    "                        hit += 1\n",
    "                ngram_rate = hit/len(ngram)\n",
    "                x = np.asarray([ngram_rate, leven_rate])\n",
    "                # if lr.predict(x.reshape(1, -1))[0] == 1:\n",
    "                if ngram_rate>=0.8 or leven_rate>=0.9:\n",
    "                    y_pred3[-1] = 1\n",
    "                    break\n",
    "            history.append(target)\n",
    "            ngram_sets.append(ngram_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = \n",
      " [[2127   25]\n",
      " [  24   24]]\n",
      "accuracy =  0.9777272727272728\n",
      "precision =  0.4897959183673469\n",
      "recall =  0.5\n",
      "f1 score =  0.4948453608247423\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix = \\n', confusion_matrix(y_true=y, y_pred=y_pred3))\n",
    "print('accuracy = ', accuracy_score(y_true=y, y_pred=y_pred3))\n",
    "print('precision = ', precision_score(y_true=y, y_pred=y_pred3))\n",
    "print('recall = ', recall_score(y_true=y, y_pred=y_pred3))\n",
    "print('f1 score = ', f1_score(y_true=y, y_pred=y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
