{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import select\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaConfig\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "\n",
    "from pyknp import Juman\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessor:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        self.model_path = \"/home/yamada/Downloads/training_bert_japanese\"\n",
    "        self.sen_model = SentenceTransformer(self.model_path, show_progress_bar=False)\n",
    "\n",
    "        # 半角全角英数字\n",
    "        # self.DELETE_PATTERN_1 = re.compile(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+')\n",
    "        # 記号\n",
    "        self.DELETE_PATTERN_2 = re.compile(\n",
    "            r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+')\n",
    "        \n",
    "        self.emb_size = self.get_sentence_vec(\"emb\").shape[0]\n",
    "        print(self.emb_size)\n",
    "\n",
    "    def get_sentence_vec(self, sen) -> np.array:\n",
    "        # sen_ = self.DELETE_PATTERN_1.sub(sen)\n",
    "        sen_ = self.DELETE_PATTERN_2.sub(\"\", sen)\n",
    "        sentence_vec = self.nlp(sen_).vector\n",
    "        # sentence_vec = self.sen_model.encode(sen)[0]\n",
    "        return sentence_vec\n",
    "    \n",
    "    def read_json_with_NoErr(self, path:str, datalist:list) -> pd.DataFrame:\n",
    "        cols = ['did', 'tid', 'usr', 'sys', 'ec']\n",
    "        df = pd.DataFrame(index=[], columns=cols)\n",
    "\n",
    "        for p in datalist:\n",
    "            datapath = Path(path + p + '/')\n",
    "            for file in datapath.glob(\"*.json\"):\n",
    "                # print(file)\n",
    "                with open(file, \"r\") as f:\n",
    "                    json_data = json.load(f)\n",
    "                    did = json_data[\"dialogue-id\"]\n",
    "                    for t in json_data[\"turns\"]:\n",
    "                        if t[\"turn-index\"] == 0:\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"U\":\n",
    "                            usr = t[\"utterance\"]\n",
    "                            continue\n",
    "                        if t[\"speaker\"] == \"S\" :\n",
    "                            tid = t[\"turn-index\"]\n",
    "                            sys = t[\"utterance\"]\n",
    "                            if t[\"error_category\"]:\n",
    "                                ec = t[\"error_category\"]\n",
    "                            else:\n",
    "                                ec = [\"No-Err\"]\n",
    "                            df = df.append(pd.DataFrame([did, tid, usr, sys, ec], index = cols).T)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "    \n",
    "    def make_error_dict(self, error_types):\n",
    "        error_dict = {}\n",
    "        for e in error_types:\n",
    "            error_dict[e] = len(error_dict)\n",
    "        return error_dict\n",
    "    \n",
    "    def extract_X_y(self, df:pd.DataFrame, error_types, prev_num) -> np.array:\n",
    "        # nlp = spacy.load('ja_ginza')\n",
    "        \n",
    "        did = df.did[0]\n",
    "        n = prev_num\n",
    "        # print(did)\n",
    "        # 全体\n",
    "        X_data = []\n",
    "        y_data = []\n",
    "        # 各 did \n",
    "        sequence_did = []\n",
    "        y_did = []\n",
    "        # エラーの辞書定義\n",
    "        error_dict = self.make_error_dict(error_types)\n",
    "\n",
    "        # 初期の調整 padding\n",
    "        for i in range(n-1):\n",
    "            sequence_did.append(\n",
    "                np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "            )\n",
    "\n",
    "        # didごとに返却する？\n",
    "        # エラーが発生したら、開始からエラーまでの文脈を入力とする(N=5の固定長でも可能)\n",
    "        # 先にこのベクトル列を作成し，Tensorに変換して， List に保持\n",
    "        for d, u, s, e in zip(df.did, df.usr, df.sys, df.ec):\n",
    "            if did != d:\n",
    "                did = d\n",
    "                sequence_did = []\n",
    "                y_did = []\n",
    "                # 初期の調整 padding\n",
    "                for i in range(n-1):\n",
    "                    sequence_did.append(\n",
    "                            np.concatenate( [np.zeros(self.emb_size), np.zeros(self.emb_size)])\n",
    "                        )\n",
    "                # break\n",
    "\n",
    "            # sequence_did.append([u, s])\n",
    "            sequence_did.append(\n",
    "                    np.concatenate(\n",
    "                        [self.get_sentence_vec(u), self.get_sentence_vec(s)]\n",
    "                    )\n",
    "                # [u, s]\n",
    "            )\n",
    "            if e[0] == \"No-Err\":\n",
    "                continue\n",
    "            else:\n",
    "                y_each_error_label = np.zeros(len(error_types))\n",
    "                for e_ in e:\n",
    "                    y_each_error_label[error_dict[e_]] = 1\n",
    "                X_data.append(sequence_did[-n:])\n",
    "                # y_did = np.array(y_each_error_label)\n",
    "                y_data.append(y_each_error_label)\n",
    "        return np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, data_path) -> None:\n",
    "        import os\n",
    "        import pickle\n",
    "        self.data_path = data_path\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        self.dir = os.listdir(data_path)\n",
    "\n",
    "    def is_exist(self, name):\n",
    "        return (name in self.dir)\n",
    "    \n",
    "    def save_data(self, name, obj):\n",
    "        with open(self.data_path+name, \"wb\") as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print(\"success save : {0}{1}\".format(self.data_path, name))\n",
    "\n",
    "    def load_data(self, name):\n",
    "        with open(self.data_path+name, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(\"success load : {0}{1}\".format(self.data_path, name))\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pre = preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "path = './error_category_classification/dbdc5_ja_dev_labeled/'\n",
    "datalist = ['DCM', 'DIT', 'IRS']\n",
    "    # List of error types\n",
    "# error_types = ['Ignore question', 'Unclear intention', \n",
    "#             'Wrong information', 'Topic transition error', \n",
    "#             'Lack of information', 'Repetition', \n",
    "#             'Semantic error', 'Self-contradiction', \n",
    "#             'Contradiction', 'Grammatical error', \n",
    "#             'Ignore offer', 'Ignore proposal', \n",
    "#             'Lack of sociality', 'Lack of common sense',\n",
    "#             'Uninterpretable', 'Ignore greeting', \n",
    "#             'No-Err'\n",
    "#             ]\n",
    "error_types = ['Unclear intention', 'Wrong information',\n",
    " 'Ignore question', 'Topic transition error', \n",
    " 'Lack of information', 'Repetition', \n",
    " 'Contradiction', 'Self-contradiction',\n",
    "  'Lack of common sense', 'Semantic error',\n",
    "   'Grammatical error', 'Ignore proposal', \n",
    "   'Ignore offer', 'Lack of sociality', \n",
    "   'Uninterpretable', 'Ignore greeting', \n",
    "   'No-Err']\n",
    "df = pre.read_json_with_NoErr(path, datalist)\n",
    "    # df = pre.read_json(path, datalist)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}