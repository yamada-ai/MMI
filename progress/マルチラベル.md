# A Literature Survey on Algorithms for Multi-label Learning
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.364.5612&rep=rep1&type=pdf

## Abstract
- マルチラベル分類
    - 複数のクラスに属することが出来る
    - クラスラベルの集合を予測可能
        
- マルチラベル分類の主なタスク
    - テキスト分類，意味画像ラベル付け，遺伝子機能分類
    
- 異なる評価指標を議論，既存のアルゴリズムの比較分析

1. ## Introduction
    - マルチラベル分類の目標は、各インスタンスがLの1つ以上のクラスに属するインスタンスの集合から学習すること
        - テキスト分類 : 科学と医学両方
        - 遺伝子機能 : 複数ある
        - 画像 : 複数の項目が映っているならマルチラベル
    - マルチラベル分類は一般性があり，学習が困難

    - Multi-label Learning and Fuzzy Logic Based Learning
        - マルチラベル分類とファジーロジックに基づく分類を区別することは重要

        - ファジー論理
            - 特徴空間における曖昧さを扱うことが多く、複数のクラスを区別するために、分類の前に追加ブロック（例えば、特徴の重みを求める）として使用
        - マルチラベル分類
            - 1つ以上のクラスでインスタンスをラベリングすること

    - Notations
        - 省略

1. ## Multi-label Learning
    1. Simple Problem Transformation Methods
        - copy
            copy して単一分類問題として実施
        - dubbed copy-weight
            - copyし、その時の事例の重みを1/事例数にする
    

----


6. ## Problem Variations
    1. 複数のラベルを用いた学習．不連続の場合
        人によって付けられるラベルにバラツキがある場合
        これはマルチラベル分類とは無関係
    
    1. マルチタスク学習
        - 一般の機械学習では，
            - 大きな問題を小さく独立な部分問題の集合に分解し，それらを別々に学習し，結合
            - 多くの情報や独立問題間の依存性を無視
        - マルチタスクでは，様々な特徴を統合可能
            - タスクの関連性を考慮
    
    1. マルチインスタンス・マルチラベル学習(MIML)
        - 複数のインスタンス(=バッグ)に対して複数のラベルが割り当てられる
        - スコープの違いとまではいかないが，複数のインスタンス(=文脈，履歴)と置き換えることは出来る？
            - しかし，本質が若干異なる(気がする)
        
        参考
        - Learnability of Multi-Instance Multi-Label Learning




- 学習方法の一覧
https://jetbead.hatenablog.com/entry/20140731/1406739777


----
# Learnability of Multi-Instance Multi-Label Learning
https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/CSciB11.pdf

    マルチインスタンス・マルチラベル学習（MIML）[1, 2]は、新しい機械学習の枠組みである。従来の教師あり学習では、一つのデータ対象は一つのインスタンスで表現され、一つのクラスラベルと関連付けられるが、MIMLでは一つのデータ対象は複数のインスタンスで表現され、複数のクラスラベルが関連付けられる（図1）。このような枠組みは、複数の意味を持つ複雑なデータオブジェクトを扱う場合に特に有効である。例えば、画像注釈では、画像は多くのパッチを含み、それぞれがインスタンスで表現され、同時に複数の注釈用語が割り当てられる。また、テキスト分類では、文書は複数のセクションを含み、それぞれがインスタンスで表現され、同時に複数のカテゴリに分類される。

    本論文では、仮説クラスφn(H)の学習可能性を示すことにより、マルチインスタンスシングルラベル仮説クラスφn(H)から構成されるMIML仮説クラスφn(H) lがPAC学習可能であることを証明します。この論文では、理論的なMIMLの研究を始めたばかりであり、多くの問題があり、更なる探求が必要であることに注意されたい。第一に、この研究は、マルチインスタンスシングルラベル仮説クラスから構築されたMIML仮説クラスに焦点を当てているが、マルチインスタンスシングルラベル仮説に依存しない多くのMIMLアルゴリズムが存在する。第二に、本論文で考慮したラベル相関は非常に単純であるが、実際のタスクにおけるラベル相関は、しばしば、より複雑である。これらの問題は、将来的に研究する価値がある。


---
スコープ
# Security Risks of Machine Learning Systems and Taxonomy Based on the Failure Mode Approach
https://www.imes.boj.or.jp/research/papers/english/21-E-03.pdf


セキュリティの機械学習モデルが故障する要因
1. 意図的か否か
1. 脆弱性の場所
1. 完全性，機密性，可用性


---
# Learning Taxonomy Adaptation in Large-scale Classification
https://www.jmlr.org/papers/volume17/14-207/14-207.pdf

- 関連研究

        ビッグデータ時代において、何万もの対象カテゴリを含む大規模な分類が重要視されている。これまで、テキストや画像の分類において、多数のカテゴリを対象とした分類手法が数多く提案されてきた。これらのアプローチは、カテゴリ間の意味的な関係を利用する点で異なっている。同様の流れで、近年、Large-scale Hierarchical Text Classification (LSHTC) (Partalas et al., 2015) や Large Scale Visual Recognition Challenge (ILSVRC)2 (Russakovsky et al., 2014) などのオープンチャレンジが開催されている。テキスト分類を目的として対象クラス間の階層性を利用することについては、Koller and Sahami (1997) や Dumais and Chen (2000) が先行研究している。これらの技術は、分類法を用いて、トップダウンのパチンコマシン方式で各ノードで独立した分類器を学習する。ルートからリーフへのパスに沿ったナイーブベイズ分類器のためのパラメータスムージングは，McCallumら（1998）によって探求されました．Liuら(2005)は，Yahoo! ディレクトリの10万以上のカテゴリを対象とした大規模な階層型SVMを適用した最初の研究の1つである．最近では、大規模な階層的テキスト分類のための他の技術も提案されている。Bennett and Nguyen (2009)は、検証で学習したRefined Expertsを適用することでエラー伝播を防止する手法を提案している。この手法では，下位分類器の出力を利用してボトムアップの情報伝播を行い，上位分類器の分類精度を向上させる．また，木構造型分類器の誤差伝播を抑制するために，ビームサーチ（Norvig, 1992）のように根から葉への経路を複数探索するアプローチもある．この点については，Fleuret and Geman (2001); Sun et al. (2013) がそのようなアプローチを提案している．しかし、この方法では、特に多数のターゲットカテゴリが存在する場合、予測の計算量が増大するため、数万件のターゲットカテゴリに対してうまくスケールしない可能性がある。Xue et al. (2008)のDeep Classificationでは、階層プルーニングを行い、より小さなターゲットクラスのサブセットを最初に識別することを提案している。そして、最初のステップで特定されたターゲットクラスのサブセットに対してナイーブベイズ分類器を再トレーニングすることで、テストインスタンスの予測を行う。最大マージンに基づくアプローチの損失関数の設計に分類法を用いることは，Cai and Hofmann (2004); Dekel et al. (2004) によって提案されており，誤分類に対するペナルティの程度は階層木における真のクラスと予測されるクラスの間の距離によって決まる．また，Dekel (2009) による最近のアプローチでは，損失関数設計をクラス不均衡やタクソノミ構造の恣意性問題に対してロバストにすることが提案されている．しかし、これらのアプローチは、カテゴリ数が数百に限定されたデータセットに適用されたものである。大規模階層分類のベイズモデリングはGopalら(2012)によって提案されており，親子ノード間の階層的依存関係は，子ノードの事前分布を親のパラメータ値で中心化することによってモデル化されている．大規模分類のための再帰的正則化に基づく戦略はGopal and Yang (2013)によって提案されている。上記2つの研究で提示されたアプローチは、カテゴリ数が数万の範囲にある問題を解決しようとするものである。これらの研究において、著者らは、ノードの親子ペアの重みベクトルが互いに近接しているべきであるという直観を採用している。これは，ベイズアプローチでは事前分布の形で，(Gopal et al., 2012)，再帰的正則化アプローチでは正則化の形で強制される(Gopal and Yang, 2013)．しかし、これらの論文で使用された大規模データセットのほとんどで、提案アプローチの精度性能（MicroF1）は、Liblinearなどのすぐに使えるパッケージが利用可能なフラットな分類スキームに近いものです。我々の研究に関連するもう一つの研究は、クラスラベルの階層木における木距離メトリックに関して、階層的分類アルゴリズムの整合性を研究するNarasimhanら(2015)のものである。

# Large Scale Hierarchical Text Classification
https://www.kaggle.com/c/lshtc

- 非常に大規模なデータセットを用いた階層的なテキスト分類のコンペティション(kaggle)

- 新たな文書を階層内のカテゴリに自動分類
- データセットはマルチラベル，マルチクラス，階層構造

# Hierarchically classifying documents using very few words
http://robotics.stanford.edu/users/sahami/papers-dir/ml97-hier.pdf
- abstract
    
    - 階層化された文書の急増に伴い，新たな文書をその階層内に自動分類するツールが必要とされてている
    - 既存の分類器では，階層構造を無視し，トピックを個別のクラスをとして扱う
        - 一般の多クラス分類
        - 膨大なトピックを分類する為の膨大な特徴量
        - 単純な分類器しか使えない(当時)
    - 階層的なトピック構造を利用し，分類タスクを分割
        - 分類ツリーの各ノードに1ずつ
        - 分割することで計算上の問題やロバスト性の問題を回避
- 結果
    - ナイーブベイズ，特徴量間の相互作用を許容した拡張ナイーブベイズ(KDB)を用いて，階層構造または非階層構造を採用したそれぞれを実験
    - 特徴量を選択した場合は，階層構造を利用した手法が有利っぽい

---
# Consistent Multiclass Algorithms for Complex Performance Measures
http://proceedings.mlr.press/v37/narasimhanb15.pdf
- abstract
    - 混同行列の任意の関数で定義される，複雑な性能指標を持つ多クラス学習に対する新たなアルゴリズム

- よくわからんので一旦パス

# Deep Classification in Large-scale Text Hierarchies
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.805&rep=rep1&type=pdf

- abstract
    - Web文書を大規模な分類体系におけるカテゴリに分類するための新しい深層分類アプローチを提案
    - 2つのステージから構成
        1. 検索ステージ
            - カテゴリ検索アルゴリズムにより，カテゴリ候補を検索
        1. 分類ステージ
            - 候補に基づいて分類
    
    - Open Directory Projectにおいて，トップダウンSVMと比較して性能が向上